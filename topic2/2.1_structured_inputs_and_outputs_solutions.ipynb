{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mzilberman40/LLM-Engineering-Essentials/blob/main/topic2/2.1_structured_inputs_and_outputs_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "# 2.1. Structured Inputs and Outputs"
      ],
      "metadata": {
        "id": "XXuQF3YXe0C_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practice task solutions**"
      ],
      "metadata": {
        "id": "4bPMLxcse0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -qU"
      ],
      "metadata": {
        "id": "vmxtcQ2de0DB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b094a2-0edc-48b6-f40e-c65265a2e5ed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/734.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.4/734.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m727.0/734.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.3/734.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "os.environ['NEBIUS_API_KEY'] = userdata.get(\"nebius_api_key\")\n",
        "\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "llama_model = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "\n",
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)\n",
        "\n",
        "def answer_with_llm(prompt: str,\n",
        "                    system_prompt=\"You are a helpful assistant\",\n",
        "                    max_tokens=512,\n",
        "                    client=nebius_client,\n",
        "                    model=llama_model,\n",
        "                    prettify=True,\n",
        "                    temperature=None) -> str:\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            }\n",
        "        )\n",
        "\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    )\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    if prettify:\n",
        "        return prettify_string(completion.choices[0].message.content)\n",
        "    else:\n",
        "        return completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "a1AxEa78e0DB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1. LLM Information extraction\n",
        "\n",
        "The goal of this task is to create a system, which extracts data about events from free text into a predictable format."
      ],
      "metadata": {
        "id": "7ZlJaY22e0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's imagine that you work for a marketing agency, and you need to gather analytics about the passing events dedicated to AI and Machine Learning. For that, you need to process press releases and extract:\n",
        "- Event name,\n",
        "- Event date,\n",
        "- Number of participants,\n",
        "- Number of speakers,\n",
        "- Attendance price.\n",
        "\n",
        "Of course, you can do it manually, but it's much more fun to use Generative AI! So, your task will be to write a function that does this with only one request to OpenAI API.\n",
        "\n",
        "Below there is an example of a press release (generated by ChatGPT, of course, so that both the event and the personae are fictional). All of them are in the press_releases.zip archive in the hometask week 1 folder.\n",
        "\n",
        "<blockquote>\n",
        "<p>PRESS RELEASE\n",
        "\n",
        "InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence</p>\n",
        "\n",
        "City of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\n",
        "\n",
        "Esteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\n",
        "\n",
        "This year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\n",
        "\n",
        "Among other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\n",
        "\n",
        "The event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\n",
        "\n",
        "For media inquiries, please contact:\n",
        "Jane Cipher\n",
        "Director of Communications, InnovAI Summit\n",
        "Email: jane.cipher@innovai.org\n",
        "Phone: +123-4567-8910</p>\n",
        "</blockquote>\n",
        "\n",
        "More specifically, you should write a function\n",
        "\n",
        "```python\n",
        "parse_press_release(pr: str) -> dict\n",
        "```\n",
        "\n",
        "where the output should be in the format\n",
        "\n",
        "```python\n",
        "{\n",
        "  name: 'InnovAI Summit 2023',\n",
        "  date: '08.11.2023',\n",
        "  n_participants: 3500,\n",
        "  n_speakers: 4,\n",
        "  price:\n",
        "}\n",
        "```\n",
        "\n",
        "If any of the four characteristics is not mentioned in the text, put `None` in the respective field.\n",
        "\n",
        "At the end, calculate the statistics of right answers and analyse what kind of mistakes you \"model\" makes the most."
      ],
      "metadata": {
        "id": "cEvtiZy6e0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hints and suggestions:**\n",
        "- It's gonna be more convenient to experiment in Nebius AI Studio's playground https://studio.nebius.com/playground.\n",
        "- You need to be very accurate with what you want from the model.\n",
        "- It will help if you specify in the prompt that the output should be in JSON format, this way you will spend less time parsing the output. But be careful. Though some models are easily prompted to output a JSON, please check the output format. It may contain excessive formatting, for example:\n",
        "<pre><code>```json\n",
        "{\"name\": \"InnovAI Summit 2023\", ...}\n",
        "```</pre></code>\n",
        "Actually, examining LLM outputs and their format is a must when working with them\n",
        "\n",
        "- Please be careful with the details. For example, Jane Cipher in the text above is not a speaker and shouldn't be counter as such (how to get rid of a contact person?). Also pay attention to the date format,\n",
        "- If the model is too wilful with the output format, don't hesitate to show some examples. Decreasing the temperature of predictions can help reduce the creativity of the answer, which is what we want for such task.\n",
        "- Debugging an LLM-powered application may become a tough business. When you think that you've polished it, an LLM can still surprise you. So, we don't expect 100% accuracy in this task, but we expect that you do your best to achieve high quality results."
      ],
      "metadata": {
        "id": "eySv4YHWe0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus points**:\n",
        "Try writing the solution using:\n",
        "- Structured JSON Output\n",
        "- Guiding JSON Output using Structures"
      ],
      "metadata": {
        "id": "vbJYFRc6e0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)\n"
      ],
      "metadata": {
        "id": "6Kx4rZLPVfmP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_with_llm(\n",
        "    messages: list,\n",
        "    client,\n",
        "    model,\n",
        "    max_tokens=512,\n",
        "    prettify=True,\n",
        "    temperature=None,\n",
        "    extra_body: dict = None,\n",
        ") -> str:\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        extra_body=extra_body,\n",
        "    )\n",
        "\n",
        "    if prettify:\n",
        "        return prettify_string(completion.choices[0].message.content)\n",
        "    else:\n",
        "        return completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "cqOaMmmJVSzg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "press_release = \"\"\"PRESS RELEASE\n",
        "\n",
        "InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence\n",
        "\n",
        "City of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\n",
        "\n",
        "Esteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\n",
        "\n",
        "This year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\n",
        "\n",
        "Among other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\n",
        "\n",
        "The event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\n",
        "\n",
        "For media inquiries, please contact: Jane Cipher Director of Communications, InnovAI Summit Email: jane.cipher@innovai.org Phone: +123-4567-8910\"\"\""
      ],
      "metadata": {
        "id": "zJspUelOe0DL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "import json\n",
        "import re"
      ],
      "metadata": {
        "id": "T4DYj9o2Wfka"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LLM_MODEL = \"meta-llama/Llama-3.3-70B-Instruct\"\n"
      ],
      "metadata": {
        "id": "hiLsu_KTWlCC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an information extraction assistant.\n",
        "\n",
        "Below is a press release describing a public event. Extract and return a JSON object with the following fields:\n",
        "\n",
        "- \"name\": the full official name of the event.\n",
        "- \"date\": the date of the event in the format \"DD.MM.YYYY\", or \"DD.MM.YYYY-DD.MM.YYYY\" if it lasted several days. If not specified, return \"None\".\n",
        "- \"n_participants\": the number of attendees or audience members. Do not include organizers, staff, or service personnel. If not specified, return \"None\".\n",
        "- \"n_speakers\": the number of people who gave talks, presentations, participated in panels, or spoke publicly during the event. Do not include contact persons, moderators, or organizers unless they also spoke publicly. If not specified, return \"None\".\n",
        "- \"price\": the cost to attend the event, formatted as \"EUR 100\", \"USD 1000\", or \"GBP 100\". Do not use currency symbols. If the event was free or no price is mentioned, return \"None\".\n",
        "\n",
        "Return only the JSON object. Do not add explanations. Use \"None\" (as a string) if any field is missing.\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "oiuRlSnTWquR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EventProfile(BaseModel):\n",
        "    name: str = Field(..., description=\"Full event name\")\n",
        "\n",
        "    date: str = Field(..., description=\"DD.MM.YYYY or DD.MM.YYYY-DD.MM.YYYY or 'None'\")\n",
        "    n_participants: Union[int, str] = Field(..., description=\"Number of participants or 'None'\")\n",
        "    n_speakers: Union[int, str] = Field(..., description=\"Number of speakers or 'None'\")\n",
        "    price: str = Field(..., description=\"Price in 'EUR 100' etc., or 'None'\")\n",
        "\n",
        "    @field_validator('date')\n",
        "    @classmethod\n",
        "    def validate_date_format(cls, v: str) -> str:\n",
        "        if v == \"None\":\n",
        "            return v\n",
        "        single = r'\\d{2}\\.\\d{2}\\.\\d{4}'\n",
        "        date_range = f'{single}-{single}'\n",
        "        if re.fullmatch(single, v) or re.fullmatch(date_range, v):\n",
        "            return v\n",
        "        raise ValueError(\"Date must be 'DD.MM.YYYY', 'DD.MM.YYYY-DD.MM.YYYY', or 'None'\")\n",
        "\n",
        "    @field_validator('n_participants', 'n_speakers')\n",
        "    @classmethod\n",
        "    def validate_number_or_none(cls, v: Union[int, str]) -> Union[int, str]:\n",
        "        if v == \"None\":\n",
        "            return v\n",
        "        # Accept stringified integers like \"3500\"\n",
        "        if isinstance(v, str) and v.isdigit():\n",
        "            return int(v)\n",
        "        if isinstance(v, int):\n",
        "            return v\n",
        "        raise ValueError(\"Must be an integer or 'None'\")\n",
        "\n",
        "    @field_validator('price')\n",
        "    @classmethod\n",
        "    def validate_price_format(cls, v: str) -> str:\n",
        "        if v == \"None\":\n",
        "            return v\n",
        "        if re.fullmatch(r'^(EUR|USD|GBP) \\d+$', v):\n",
        "            return v\n",
        "        raise ValueError(\"Price must be 'EUR 100', 'USD 1000', 'GBP 100', or 'None'\")"
      ],
      "metadata": {
        "id": "fXP5oO41e0DL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For JSON schema\n",
        "schema = EventProfile.model_json_schema()\n",
        "print(json.dumps(schema, indent=2))"
      ],
      "metadata": {
        "id": "JnzmZmkjWyLR",
        "outputId": "0822367e-4068-421f-e37a-d905d2b17f29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"properties\": {\n",
            "    \"name\": {\n",
            "      \"description\": \"Full event name\",\n",
            "      \"title\": \"Name\",\n",
            "      \"type\": \"string\"\n",
            "    },\n",
            "    \"date\": {\n",
            "      \"description\": \"DD.MM.YYYY or DD.MM.YYYY-DD.MM.YYYY or 'None'\",\n",
            "      \"title\": \"Date\",\n",
            "      \"type\": \"string\"\n",
            "    },\n",
            "    \"n_participants\": {\n",
            "      \"anyOf\": [\n",
            "        {\n",
            "          \"type\": \"integer\"\n",
            "        },\n",
            "        {\n",
            "          \"type\": \"string\"\n",
            "        }\n",
            "      ],\n",
            "      \"description\": \"Number of participants or 'None'\",\n",
            "      \"title\": \"N Participants\"\n",
            "    },\n",
            "    \"n_speakers\": {\n",
            "      \"anyOf\": [\n",
            "        {\n",
            "          \"type\": \"integer\"\n",
            "        },\n",
            "        {\n",
            "          \"type\": \"string\"\n",
            "        }\n",
            "      ],\n",
            "      \"description\": \"Number of speakers or 'None'\",\n",
            "      \"title\": \"N Speakers\"\n",
            "    },\n",
            "    \"price\": {\n",
            "      \"description\": \"Price in 'EUR 100' etc., or 'None'\",\n",
            "      \"title\": \"Price\",\n",
            "      \"type\": \"string\"\n",
            "    }\n",
            "  },\n",
            "  \"required\": [\n",
            "    \"name\",\n",
            "    \"date\",\n",
            "    \"n_participants\",\n",
            "    \"n_speakers\",\n",
            "    \"price\"\n",
            "  ],\n",
            "  \"title\": \"EventProfile\",\n",
            "  \"type\": \"object\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_press_release(pr: str) -> dict:\n",
        "  completion = nebius_client.chat.completions.create(\n",
        "    model=LLM_MODEL,\n",
        "    temperature=1,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": pr}\n",
        "    ],\n",
        "    extra_body={\n",
        "        \"guided_json\": EventProfile.model_json_schema()\n",
        "    }\n",
        "  )\n",
        "  validated = EventProfile.model_validate_json(completion.choices[0].message.content)\n",
        "  return validated.model_dump()\n"
      ],
      "metadata": {
        "id": "6hfrn3oVWybc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parse_press_release(press_release)"
      ],
      "metadata": {
        "id": "o2dOvdb3VrAj",
        "outputId": "7f916722-21ac-4ef3-abdc-41d2f712fb82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'InnovAI Summit 2023', 'date': '05.11.2023', 'n_participants': 3500, 'n_speakers': 5, 'price': 'None'}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_triple_backtick_blocks(text):\n",
        "    \"\"\"\n",
        "    Extracts all text enclosed between triple backticks (```).\n",
        "    Returns a list of code/text blocks.\n",
        "    \"\"\"\n",
        "    return re.findall(r\"```(.*?)```\", text, re.DOTALL)\n",
        "\n",
        "def parse_press_release(pr: str) -> dict:\n",
        "    answer = answer_with_llm(\n",
        "            f\"Here's a press release\\n{pr}\\n\\nExtract from it the following json:\"\\\n",
        "            \"If any information needed for JSON is not available, write \\\"None\\\" instead (with quotes).\\n\"\\\n",
        "            '{\"name\": NAME_OF_EVENT, \"date\": DATE_OF_EVENT, \"n_participants\": NUM_PARTICIPANTS, \"n_speakers\": NUM_SPEAKERS, \"price\": PRICE}'\\\n",
        "            \"NAME_OF_EVENT should be the name of event advertised,\\n\"\\\n",
        "            \"DATE_OF_EVENT hould be the date of event mentioned in format DD.MM.YYYY or DD.MM.YYYY-DD.MM.YYYY if the event lasted for several days,\\n\"\\\n",
        "            \"NUM_PARTICIPANTS should be the estimated amount of participants of said event in a format like 200 or 1000 or 10000, do not write it like 2,000,\\n\"\\\n",
        "            \"NUM_SPEAKERS is a number, corresponding to amount of names of speakers and hosts mentioned\\n\"\\\n",
        "            \"PRICE should be the price of event in the format EUR 100 or USD 1000 or GBP 100 depending on currency. Do not write currency symbol, instead write an abbreviation.\\n\"\\\n",
        "            \"If any information needed for JSON is not available, write a json string \\\"None\\\" instead (with quotes).\"\n",
        "    )\n",
        "    return answer\n",
        "    # try:\n",
        "    #     if \"```\" in answer:\n",
        "    #         answer = extract_triple_backtick_blocks(answer)[0]\n",
        "    #     return json.loads(answer)\n",
        "    # except Exception as e:\n",
        "    #     print(answer)\n",
        "    #     raise"
      ],
      "metadata": {
        "id": "BQbhZTwPe0DL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = parse_press_release(press_release)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "jS9DlC4LrDCt",
        "outputId": "d2c0bbf1-db79-4f61-c2df-e669acff5b72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'InnovAI Summit 2023', 'date': '05.11.2023', 'n_participants': 3500, 'n_speakers': 5, 'price': 'None'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r2y-UvlNsRkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extract_triple_backtick_blocks(text)"
      ],
      "metadata": {
        "id": "-MiJJhOHrnoW",
        "outputId": "a3ca2847-4681-4f9a-9b9b-0cbd162d7769",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n{\\n\"name\": \"InnovAI Summit 2023\",\\n\"date\": \"05.11.2023\",\\n\"n_participants\": 3500,\\n\"n_speakers\": 5,\\n\"price\": \"None\"\\n}\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Solution**"
      ],
      "metadata": {
        "id": "fIwYkdRJe0DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parse_press_release(press_release)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b44501-e9a2-473f-961b-e95366e7f35a",
        "id": "yMzVnReye0DL"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'InnovAI Summit 2023', 'date': '05.11.2023', 'n_participants': 3500, 'n_speakers': 5, 'price': 'None'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "\n",
        "We've prepared a small dataset for you to test your prompt on. Provided you've written your function, try running the following code. At the end you also have an opportunity to look at the results in a table side-by-side in with_results.csv. Your goal is to get at least 60% of fields right.."
      ],
      "metadata": {
        "id": "rXJF_b3de0DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gdown\n",
        "!gdown -O press_release_extraction.csv https://docs.google.com/spreadsheets/d/15IGdc3MV8864lxrLxsug0Ij480p76T1EAwBM7WGT_OI/export?format=csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2868ad6e-c09f-4e7e-f0d0-d9f5c16afa7f",
        "id": "dTVTe284e0DL"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.6.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/parse_url.py:48: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=None\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://docs.google.com/spreadsheets/d/15IGdc3MV8864lxrLxsug0Ij480p76T1EAwBM7WGT_OI/export?format=csv\n",
            "To: /content/press_release_extraction.csv\n",
            "16.0kB [00:00, 23.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "pr_df = pandas.read_csv(\"press_release_extraction.csv\")\n",
        "pr_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "2ef37618-8f6f-4a94-ee5f-ed11aa0ba0f4",
        "id": "OJjcT2pSe0DL"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             pr_text  \\\n",
              "0  InnovAI Summit 2023: A Glimpse into the Future...   \n",
              "1  Press Dispatch: 'Artificial Mariners: Navigati...   \n",
              "2  FOR IMMEDIATE RELEASE\\n\\nAI Innovators Convene...   \n",
              "3  Press Release: Cutting-Edge Innovations Debute...   \n",
              "4  Press Release: Innovative Minds Gather at \"AI ...   \n",
              "\n",
              "                                           pr_parsed  \n",
              "0  {\\n  \"name\": \"InnovAI Summit 2023\",\\n  \"date\":...  \n",
              "1  {\"name\": \"Artificial Mariners: Navigatin' the ...  \n",
              "2  {\"name\": \"Annual Machine Learning Symposium 20...  \n",
              "3  {\"name\": \"AI Advancements Summit 2023\",\\n \"dat...  \n",
              "4  {\"name\": \"AI Horizon 2023\",\\n \"date\": \"15.10.2...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dcdf5a5a-6ce5-4d19-b7af-b3663b44ceaa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pr_text</th>\n",
              "      <th>pr_parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>InnovAI Summit 2023: A Glimpse into the Future...</td>\n",
              "      <td>{\\n  \"name\": \"InnovAI Summit 2023\",\\n  \"date\":...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Press Dispatch: 'Artificial Mariners: Navigati...</td>\n",
              "      <td>{\"name\": \"Artificial Mariners: Navigatin' the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FOR IMMEDIATE RELEASE\\n\\nAI Innovators Convene...</td>\n",
              "      <td>{\"name\": \"Annual Machine Learning Symposium 20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Press Release: Cutting-Edge Innovations Debute...</td>\n",
              "      <td>{\"name\": \"AI Advancements Summit 2023\",\\n \"dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Press Release: Innovative Minds Gather at \"AI ...</td>\n",
              "      <td>{\"name\": \"AI Horizon 2023\",\\n \"date\": \"15.10.2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dcdf5a5a-6ce5-4d19-b7af-b3663b44ceaa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dcdf5a5a-6ce5-4d19-b7af-b3663b44ceaa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dcdf5a5a-6ce5-4d19-b7af-b3663b44ceaa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2e5ca5fb-344d-4813-9a4a-b866feea7a6f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2e5ca5fb-344d-4813-9a4a-b866feea7a6f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2e5ca5fb-344d-4813-9a4a-b866feea7a6f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pr_df",
              "summary": "{\n  \"name\": \"pr_df\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"pr_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence\\n\\nCity of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\\n\\nEsteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\\n\\nThis year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\\n\\nAmong other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\\n\\nThe event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\\n\\nFor media inquiries, please contact: Jane Cipher Director of Communications, InnovAI Summit Email: jane.cipher@innovai.org Phone: +123-4567-8910\",\n          \"Press Dispatch: 'Artificial Mariners: Navigatin' the AI Seas' - The Grand AI and Machine Learnin' Symposium of 2023\\n\\nOctober 12, 2023 - Tortuga Bay, The Spanish Main - Avast ye! Just a fortnight past, the shores of Tortuga Bay were graced with the grandest gatherin' of minds and marauders from across the seven seas. The event known far and wide as \\\"Artificial Mariners: Navigatin' the AI Seas\\\" did cast its anchor on the 8th and 9th of October, bringin' together a motley crew of over 2,000 sea dogs, scholars, and ship captains keen on decipherin' the mysteries of Artificial Intelligence and Machine Learnin'.\\n\\nWith the Jolly Roger flyin' high, the symposium boasted of tales and tools shared by the likes of the fearsome Dr. \\\"Blackwater\\\" Aria Kessler, known in the New World and Old for her dark arts in makin' machines mimic the mind. There were whispers amongst the crew about Dr. Jun \\\"Kraken\\\" Zhao, who spoke of harnessin' the monstrous power of calculations with nary a need for extra rum, savin' energy like a true sea scoundrel.\\n\\nNo gatherin' of this sort would be complete without explorin' the depths of ethical plunderin', led by the sharp-witted Dr. Sofia \\\"Siren\\\" \\u00c1lvarez. Her talk drew maps for navigatin' the fine line 'twixt progress and plunder, makin' sure the power of our newfound intelligence be used for the good of all, not just the few.\\n\\nBut shiver me timbers, it weren\\u2019t all just jabber! Tom\\u00e1s \\\"One-Eye\\\" Rivera, a cunning pirate with a penchant for codes and ciphers, put together a hands-on spectacle. This live code-crackin' session saw shipmates and buccaneers alike put their heads together, tacklin' problems that'd make even the saltiest of sea dogs sweat.\\n\\nAs the sun set on the final day, the renowned and somewhat mystical Dr. Emilia \\\"Seafarer\\\" van der Meer took to the stage, her eyes alight with visions of uncharted waters. Her words wove tales of futures where our trusty shipmates, the AI, be integral to weatherin' the storms ahead, safeguardin' not just our gold, but our lands and livelihoods.\\n\\n\\\"'Twas a rally like no other, fillin' our hearts with fire and our minds with dreams of treasures that lay beyond the horizons of man and machine,\\\" confessed an old tar as he prepared to disembark.\\n\\nAs the symposium closed its doors, the air was thick with plans and plots, the promise of alliances, and a shared quest to conquer the vast, unpredictable seas of Artificial Intelligence.\\n\\nFor those wishin' to re-live the adventure or who couldn\\u2019t sail with us this time around, visit [Symposium\\u2019s Mysterious Cove].\\n\\nContact for parley:\\nName: [Your trusty informant]\\nTitle: [Harbormaster of Information]\\nBird-mail: [Email]\\nMessage in a bottle: [Phone Number]\",\n          \"Press Release: \\\"AI for Equity Summit\\\" Champions Inclusivity in Technology\\n\\nOctober 18, 2023 \\u2014 The transformative \\\"AI for Equity Summit\\\" convened on October 15, 2023, marking an historic congregation of technological prowess dedicated to inclusivity and equitable advancements in artificial intelligence. The event welcomed over 3,000 attendees, each contributing a registration fee of $250, signifying their commitment to nurturing diversity in AI.\\n\\nSix illustrious speakers graced the summit's virtual stage, including Dr. Ayesha Khurram, renowned for her work in ethical AI, tech visionary Omar Svensson, Dr. Lola Adebayo, an advocate for minorities in STEM, Dr. Ji-hoon Park, known for his innovative algorithms against biases, Maria Navarro, a crusader for women in tech, and coding prodigy, Zane Kirschner, who is making significant strides in accessible AI for persons with disabilities.\\n\\nThe summit wasn't just about discussions; it was about making tangible strides. The highlight was the launch of the \\\"AI Equity Initiative,\\\" a fund that collected over $1.2 million from participant contributions, aimed at supporting tech education in underserved regions.\\n\\nWorkshops emphasized actionable strategies for dismantling systemic barriers in the tech industry. Dr. Adebayo's session on 'Intersectionality in AI Development' and Kirschner's workshop titled 'Coding Without Barriers' particularly stood out for their depth and engagement.\\n\\nMoreover, the \\\"AI for Equity Summit\\\" was proud to allocate 20% of all registration fees toward scholarships for students from underrepresented backgrounds to pursue AI studies.\\n\\nIn the wake of the event, a communiqu\\u00e9 was released, expressing a unified pledge among the participants and speakers to proactively include underrepresented groups in both the development of AI and the conversations around it.\\n\\nAttendees and interested parties are encouraged to stay informed through the summit's official channels for ongoing initiatives and future events.\\n\\nEnd of Release\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pr_parsed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"{\\n  \\\"name\\\": \\\"InnovAI Summit 2023\\\",\\n  \\\"date\\\": \\\"05.11.2023\\\",\\n  \\\"n_participants\\\": 3500,\\n  \\\"n_speakers\\\": 4,\\n  \\\"price\\\": \\\"None\\\"\\n}\",\n          \"{\\\"name\\\": \\\"Artificial Mariners: Navigatin' the AI Seas\\\",\\n \\\"date\\\": \\\"08.10.2023-09.10.2023\\\",\\n \\\"n_participants\\\": 2000,\\n \\\"n_speakers\\\": 5,\\n \\\"price\\\":\\\"None\\\"}\",\n          \"{\\\"name\\\": \\\"AI for Equity Summit\\\",\\n \\\"date\\\": \\\"15.10.2023\\\",\\n \\\"n_participants\\\":  3000,\\n \\\"n_speakers\\\": 6,\\n \\\"price\\\": \\\"USD 250\\\"}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pr_df.pr_parsed[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "828ce4d6-43b3-4a75-bc96-152c48a6edf1",
        "id": "sDGJ1vKxe0DL"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n  \"name\": \"InnovAI Summit 2023\",\\n  \"date\": \"05.11.2023\",\\n  \"n_participants\": 3500,\\n  \"n_speakers\": 4,\\n  \"price\": \"None\"\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "parsed_list = []\n",
        "fields = {\n",
        "    \"name\": str,\n",
        "    \"date\": str,\n",
        "    \"n_speakers\": int,\n",
        "    \"n_participants\": int,\n",
        "    \"price\": str\n",
        "}\n",
        "correct_fields = 0\n",
        "for row in pr_df.itertuples():\n",
        "    parsed_release = parse_press_release(row.pr_text)\n",
        "    parsed_list.append(json.dumps(parsed_release, indent=4))\n",
        "    golden = json.loads(row.pr_parsed)\n",
        "    for field, field_type in fields.items():\n",
        "        golden_field = golden[field]\n",
        "        parsed_field = parsed_release.get(field)\n",
        "        try:\n",
        "            parsed_field = field_type(parsed_field)\n",
        "        except (ValueError, TypeError):\n",
        "            pass\n",
        "        if golden_field == parsed_field:\n",
        "            correct_fields += 1\n",
        "        else:\n",
        "            print(f\"For {golden['name']} {field} {parsed_release.get(field)} doesn't seem the same as {golden[field]}\")\n",
        "\n",
        "print(f\"Correctly extracted {correct_fields} out of {5*len(pr_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce748dfa-4e47-4a5c-fd7e-d29118aabd4f",
        "id": "RSvVEFCWe0DL"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For InnovAI Summit 2023 n_speakers 5 doesn't seem the same as 4\n",
            "Correctly extracted 34 out of 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus points\n",
        "- Try and compare different ways of establishing the correct answer formatting\n",
        "- Try and compare different LLMs"
      ],
      "metadata": {
        "id": "lcGzo04oe0DL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2. Character localiztion\n",
        "\n",
        "Cool thing about structured output, is that it's very easy to make a translated version of a specific dataset, taking into account all the context and outputing in a format, which is super easy to parse. Let's try this on MMLU.\n",
        "\n",
        "**Task:** Write a function which inputs a sample from MMLU and outputs a translated version, using structured outputs.\n",
        "\n",
        "Tip: make sure that the correct answer didn't change."
      ],
      "metadata": {
        "id": "EDQRdBcLe0DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets"
      ],
      "metadata": {
        "id": "wsZ7lgLXe0DM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "289990f1-c470-4b73-e96c-bdb03b5a1d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m430.1/491.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class MMLUSample(BaseModel):\n",
        "    ...\n",
        "\n",
        "def translate_mmlu_sample(sample: MMLUSample, target_language: str) -> MMLUSample:\n",
        "    ..."
      ],
      "metadata": {
        "id": "-iMfgJG7e0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class MMLUSample(BaseModel):\n",
        "    question: str\n",
        "    A: str\n",
        "    B: str\n",
        "    C: str\n",
        "    D: str\n",
        "    correct_answer: str\n",
        "\n",
        "def translate_mmlu_sample(sample: MMLUSample, target_language: str) -> MMLUSample:\n",
        "    completion = nebius_client.chat.completions.create(\n",
        "        model=llama_model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Translate this MMLU sample into {target_language}\" \\\n",
        "                f\"Question: {sample.question}\\n\" \\\n",
        "                f\"A: {sample.A}\\n\" \\\n",
        "                f\"B: {sample.B}\\n\" \\\n",
        "                f\"C: {sample.C}\\n\" \\\n",
        "                f\"D: {sample.D}\\n\" \\\n",
        "                f\"Correct answer: {sample.correct_answer}\\n\" \\\n",
        "                f\"Translated sample:\"\n",
        "            }\n",
        "        ],\n",
        "        extra_body={\n",
        "            \"guided_json\": MMLUSample.model_json_schema()\n",
        "        },\n",
        "    )\n",
        "\n",
        "    translated = MMLUSample.model_validate_json(completion.choices[0].message.content)\n",
        "    if translated.correct_answer != sample.correct_answer:\n",
        "        translated.correct_answer = sample.correct_answer\n",
        "    return translated"
      ],
      "metadata": {
        "id": "eieWNf6Ze0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmlu_sample = MMLUSample(\n",
        "    question = \"Which of the following statements about Ethernets is typically FALSE?\",\n",
        "    A = \"Ethernets use circuit switching to send messages.\",\n",
        "    B = \"Ethernets use buses with multiple masters.\",\n",
        "    C = \"Ethernet protocols use a collision-detection method to ensure that messages are transmitted properly.\",\n",
        "    D = \"Networks connected by Ethernets are limited in length to a few hundred meters.\",\n",
        "    correct_answer = \"A\"\n",
        ")\n",
        "\n",
        "translate_mmlu_sample(mmlu_sample, target_language=\"German\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b81eabf-7a69-417b-fd03-3621ab2b6bf7",
        "id": "6EqyzfJxe0DM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MMLUSample(question='Welche der folgenden Aussagen über Ethernets ist typischerweise FALSCH?', A='Ethernets verwenden Schaltauswahl, um Nachrichten zu senden.', B='Ethernets verwenden Busse mit mehreren Master-Geräten.', C='Ethernet-Protokolle verwenden ein Kollisions-Erkennungsverfahren, um sicherzustellen, dass Nachrichten ordnungsgemäß übertragen werden.', D='Netzwerke, die durch Ethernets verbunden sind, sind in ihrer Länge auf einige hundert Meter begrenzt.', correct_answer='A')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's remember the code we've written for MMLU evaluator and add a little twist:\n",
        "\n",
        "We'll have both topic and language in which we want to evaluate the model."
      ],
      "metadata": {
        "id": "uwoVgXi7e0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "KIB_2RFYe0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Modify the following MMLUEvaluator code so that it can also translate the input question and evaluate the performance in a different language."
      ],
      "metadata": {
        "id": "TIqzv_AJe0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "class MMLUEvaluator:\n",
        "    def __init__(self, system_prompt: str = None, prompt: str = None,\n",
        "                 topic: str = \"high_school_mathematics\"):\n",
        "        \"\"\"\n",
        "        Initialize the MMLU evaluator.\n",
        "\n",
        "        Args:\n",
        "            system_prompt: Optional system prompt for the model\n",
        "            prompt: Custom prompt for the model\n",
        "            topic: Which topic to choose\n",
        "        \"\"\"\n",
        "\n",
        "        self.topic = topic\n",
        "        self.topic_prettified = topic.replace(\"_\", \" \")\n",
        "        self.system_prompt = system_prompt or f\"You are an expert in {self.topic_prettified}.\"\n",
        "\n",
        "        self.prompt = \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
        "You need to ponder the question and justify the choice of one of the options A, B, C, or D.\n",
        "At the end, do write the chosen answer option A, B, C, D after #ANSWER:\n",
        "Now, take a deep breath and work out this problem step by step. If you do well, I'll tip you 200$.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "ANSWER OPTIONS:\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "\"\"\"\n",
        "\n",
        "        self.questions, self.choices, self.answers = self.load_mmlu_data(topic=self.topic)\n",
        "\n",
        "    def load_mmlu_data(self, topic: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load MMLU test data on a given topic.\n",
        "\n",
        "        Args:\n",
        "            topic: Which topic to choose\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with questions and answers\n",
        "        \"\"\"\n",
        "\n",
        "        dataset = load_dataset(\"cais/mmlu\", topic, split=\"test\")\n",
        "\n",
        "        dataset = dataset\n",
        "        dataset = pd.DataFrame(dataset)\n",
        "\n",
        "        # Load questions and choices separately\n",
        "        questions = dataset[\"question\"]\n",
        "        choices = pd.DataFrame(\n",
        "            data=dataset[\"choices\"].tolist(), columns=[\"A\", \"B\", \"C\", \"D\"]\n",
        "        )\n",
        "        # In the dataset, true answer labels are in 0-3 format;\n",
        "        # We convert it to A-D\n",
        "        answers = dataset[\"answer\"].map(lambda ans: {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[ans])\n",
        "\n",
        "        return questions, choices, answers\n",
        "\n",
        "    def extract_answer(self, solution: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract the letter answer from model's response.\n",
        "\n",
        "        Args:\n",
        "            response: Raw model response\n",
        "\n",
        "        Returns:\n",
        "            Extracted answer letter (A, B, C, D, or Failed to parse)\n",
        "        \"\"\"\n",
        "        # Look for a single letter answer in the response\n",
        "        try:\n",
        "            answer = solution.split('#ANSWER:')[1].strip()\n",
        "        except:\n",
        "            answer = \"Failed to parse\"\n",
        "        return answer\n",
        "\n",
        "    def evaluate_single_question(self, question: str, choices: Dict[str, str],\n",
        "                                 correct_answer: str,\n",
        "                                 client, model) -> Tuple[bool, str]:\n",
        "        \"\"\"\n",
        "        Evaluate a single question.\n",
        "\n",
        "        Args:\n",
        "            question: Formatted question string\n",
        "            correct_answer: Correct answer letter\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (is_correct, extracted_answer, model_response)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            model_response = answer_with_llm(\n",
        "                prompt=self.prompt.format(\n",
        "                    client=client, model=model,\n",
        "                    topic_prettified=self.topic_prettified,\n",
        "                    question=question,\n",
        "                    A=choices['A'], B=choices['B'], C=choices['C'], D=choices['D']\n",
        "                ),\n",
        "                system_prompt=self.system_prompt,\n",
        "                prettify=False\n",
        "            )\n",
        "            answer = self.extract_answer(model_response)\n",
        "            is_correct = (answer.upper() == correct_answer.upper())\n",
        "            return is_correct, answer, model_response\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating question: {e}\")\n",
        "            return False, None, None\n",
        "\n",
        "    def run_evaluation(self, client=nebius_client, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                       n_questions=50) -> Dict:\n",
        "        \"\"\"\n",
        "        Run evaluation of a given model on the first n_questions.\n",
        "\n",
        "        Args:\n",
        "            client: Which client to use (OpenAI or Nebius)\n",
        "            model: Which model to use\n",
        "            n_questions: How many first questions to take\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with evaluation metrics\n",
        "        \"\"\"\n",
        "        evaluation_log = []\n",
        "        correct_count = 0\n",
        "\n",
        "        if n_questions:\n",
        "            n_questions = min(n_questions, len(self.questions))\n",
        "        else:\n",
        "            n_questions = len(self.questions)\n",
        "\n",
        "        for i in tqdm(range(n_questions)):\n",
        "            is_correct, answer, model_response = self.evaluate_single_question(\n",
        "                question=self.questions[i],\n",
        "                choices=self.choices.iloc[i],\n",
        "                correct_answer=self.answers[i],\n",
        "                client=client,\n",
        "                model=model,\n",
        "            )\n",
        "\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "\n",
        "            evaluation_log.append({\n",
        "                'answer': answer,\n",
        "                'model_response': model_response,\n",
        "                'is_correct': is_correct\n",
        "            })\n",
        "\n",
        "        accuracy = correct_count / n_questions\n",
        "        evaluation_results = {\n",
        "            'accuracy': accuracy,\n",
        "            'evaluation_log': evaluation_log\n",
        "        }\n",
        "\n",
        "        return evaluation_results\n"
      ],
      "metadata": {
        "id": "KmbtCJnNe0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "mSxq5NsQe0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "class MMLUEvaluator:\n",
        "    def __init__(self, system_prompt: str = None, prompt: str = None,\n",
        "                 topic: str = \"high_school_mathematics\",\n",
        "                 language: str = \"English\"):\n",
        "        \"\"\"\n",
        "        Initialize the MMLU evaluator.\n",
        "\n",
        "        Args:\n",
        "            system_prompt: Optional system prompt for the model\n",
        "            prompt: Custom prompt for the model\n",
        "            topic: Which topic to choose\n",
        "        \"\"\"\n",
        "\n",
        "        self.topic = topic\n",
        "        self.language = language\n",
        "        self.topic_prettified = topic.replace(\"_\", \" \")\n",
        "        self.system_prompt = system_prompt or f\"You are an expert in {self.topic_prettified}.\"\n",
        "\n",
        "        self.prompt = \"\"\"You are given a question in {topic_prettified} with four answer options labeled by A, B, C, and D.\n",
        "You need to ponder the question and justify the choice of one of the options A, B, C, or D.\n",
        "At the end, do write the chosen answer option A, B, C, D after #ANSWER:\n",
        "Now, take a deep breath and work out this problem step by step. If you do well, I'll tip you 200$.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "ANSWER OPTIONS:\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "\"\"\"\n",
        "\n",
        "        self.questions, self.choices, self.answers = self.load_mmlu_data(topic=self.topic)\n",
        "\n",
        "    def load_mmlu_data(self, topic: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load MMLU test data on a given topic.\n",
        "\n",
        "        Args:\n",
        "            topic: Which topic to choose\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with questions and answers\n",
        "        \"\"\"\n",
        "\n",
        "        dataset = load_dataset(\"cais/mmlu\", topic, split=\"test\")\n",
        "\n",
        "        dataset = dataset\n",
        "        dataset = pd.DataFrame(dataset)\n",
        "\n",
        "        # Load questions and choices separately\n",
        "        questions = dataset[\"question\"]\n",
        "        choices = pd.DataFrame(\n",
        "            data=dataset[\"choices\"].tolist(), columns=[\"A\", \"B\", \"C\", \"D\"]\n",
        "        )\n",
        "        # In the dataset, true answer labels are in 0-3 format;\n",
        "        # We convert it to A-D\n",
        "        answers = dataset[\"answer\"].map(lambda ans: {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[ans])\n",
        "\n",
        "        return questions, choices, answers\n",
        "\n",
        "    def extract_answer(self, solution: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract the letter answer from model's response.\n",
        "\n",
        "        Args:\n",
        "            response: Raw model response\n",
        "\n",
        "        Returns:\n",
        "            Extracted answer letter (A, B, C, D, or Failed to parse)\n",
        "        \"\"\"\n",
        "        # Look for a single letter answer in the response\n",
        "        try:\n",
        "            answer = solution.split('#ANSWER:')[1].strip()\n",
        "        except:\n",
        "            answer = \"Failed to parse\"\n",
        "        return answer\n",
        "\n",
        "    def evaluate_single_question(self, question: str, choices: Dict[str, str],\n",
        "                                 correct_answer: str,\n",
        "                                 client, model) -> Tuple[bool, str]:\n",
        "        \"\"\"\n",
        "        Evaluate a single question.\n",
        "\n",
        "        Args:\n",
        "            question: Formatted question string\n",
        "            correct_answer: Correct answer letter\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (is_correct, extracted_answer, model_response)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if self.language != \"English\":\n",
        "                sample = MMLUSample(\n",
        "                    question=question,\n",
        "                    A=choices['A'], B=choices['B'], C=choices['C'], D=choices['D'],\n",
        "                    correct_answer=correct_answer\n",
        "                )\n",
        "                translated = translate_mmlu_sample(sample, target_language=self.language)\n",
        "                question = translated.question\n",
        "                choices = {\"A\": translated.A, \"B\": translated.B, \"C\": translated.C, \"D\": translated.D}\n",
        "                correct_answer = translated.correct_answer\n",
        "            model_response = answer_with_llm(\n",
        "                prompt=self.prompt.format(\n",
        "                    client=client, model=model,\n",
        "                    topic_prettified=self.topic_prettified,\n",
        "                    question=question,\n",
        "                    A=choices['A'], B=choices['B'], C=choices['C'], D=choices['D']\n",
        "                ),\n",
        "                system_prompt=self.system_prompt,\n",
        "                prettify=False\n",
        "            )\n",
        "            answer = self.extract_answer(model_response)\n",
        "            is_correct = (answer.upper() == correct_answer.upper())\n",
        "            return is_correct, answer, model_response\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating question: {e}\")\n",
        "            return False, None, None\n",
        "\n",
        "    def run_evaluation(self, client=nebius_client, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                       n_questions=50) -> Dict:\n",
        "        \"\"\"\n",
        "        Run evaluation of a given model on the first n_questions.\n",
        "\n",
        "        Args:\n",
        "            client: Which client to use (OpenAI or Nebius)\n",
        "            model: Which model to use\n",
        "            n_questions: How many first questions to take\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with evaluation metrics\n",
        "        \"\"\"\n",
        "        evaluation_log = []\n",
        "        correct_count = 0\n",
        "\n",
        "        if n_questions:\n",
        "            n_questions = min(n_questions, len(self.questions))\n",
        "        else:\n",
        "            n_questions = len(self.questions)\n",
        "\n",
        "        for i in tqdm(range(n_questions)):\n",
        "            is_correct, answer, model_response = self.evaluate_single_question(\n",
        "                question=self.questions[i],\n",
        "                choices=self.choices.iloc[i],\n",
        "                correct_answer=self.answers[i],\n",
        "                client=client,\n",
        "                model=model,\n",
        "            )\n",
        "\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "\n",
        "            evaluation_log.append({\n",
        "                'answer': answer,\n",
        "                'model_response': model_response,\n",
        "                'is_correct': is_correct\n",
        "            })\n",
        "\n",
        "        accuracy = correct_count / n_questions\n",
        "        evaluation_results = {\n",
        "            'accuracy': accuracy,\n",
        "            'evaluation_log': evaluation_log\n",
        "        }\n",
        "\n",
        "        return evaluation_results\n"
      ],
      "metadata": {
        "id": "aIOBbz4ee0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "78Eqh6jhe0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MMLUEvaluator(topic=\"medical_genetics\", language=\"English\")\n",
        "\n",
        "results = evaluator.run_evaluation(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                         n_questions=50)\n",
        "print(f'\\nAccuracy: {results[\"accuracy\"]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412,
          "referenced_widgets": [
            "6e8744a8e13d400fa3048d8babdcb0e3",
            "3fe4de73df694bb8b200796bbf769c66",
            "4468945e65aa4908940ebd08ccbb9d31",
            "b75e60ac58de474886a28a38b3d8498e",
            "d4274624c0484a03815acd275333c3ba",
            "117977a9d92a483aa00b99cf3326024a",
            "163630a09ac14eee889a38201ad7deed",
            "0edc92810a2f406793a00a55ea4af7c8",
            "825ba5c4cdaf4a468df315c26e705cb8",
            "e7b357024a5f4342a9039df0fd7cbea1",
            "1e61edc804cb4ffc97f0c1d1e8b48d6b",
            "8813abd74f6a45c8b3bb9d3c2df18afe",
            "e4629b3e91ad4344826996d0993fa769",
            "a0464d55a5944770b0762059dfd7d340",
            "3c74db8dea8e402db200df914216e9a4",
            "321f1c1bc3b646838e28024408ff1cbf",
            "686e222eaa46468386b8db782c0b4934",
            "4584cfb243f44e28a025cb3e344829bb",
            "77fe41ba468f4924a66354477952bbc4",
            "3ecc502579554bdf9b1ef03e94215309",
            "e76399cf1a4c4abfae4c3f98c65abccd",
            "79536bda0d624afe82cdc625827e58ff",
            "024c5b665383415aafa2d516c03cd15d",
            "26334b4a5ea34e59823f2874b24698cc",
            "febea34f6bab49e1a164b0d2cd817b25",
            "d0692f71a2604001adbd7fa32cdfb3cd",
            "6df1122294234329b1dca26be11f8788",
            "c6258810df634cb0a9739000afddc856",
            "793aa71bbdc14601a204a6847488c3cb",
            "5de9c0ba1a4f4b9c86a78727c522142a",
            "a5e5a93c646d4908abfdf251b5900195",
            "38a27fbe52764d44b9a6b677b0fbb31e",
            "6a77b6449feb44378a3339826612df12",
            "72d20a0acacd4d18a23a566f2e441125",
            "d1f9e945a83d480786f1a616bcb2e58e",
            "b28fcc9f477e494f8e496e54b9a663ba",
            "0455dc3ac33242ddb3a19096772d0eca",
            "318d114eaa9d437f963fce085f074d32",
            "63c7a805c625440ea039fddd6b155ddc",
            "42a333cc5a5d477eb2161ae9edeb1e7e",
            "fad831cd7bd04afd8611b63235dd23e0",
            "6465bd0202c7432e96ce65050aec4331",
            "f08ea9afc6cf4eb9b6c919bede250287",
            "d865d08d00e64750a7f8808640821b10",
            "53081f9667154d2dabcbbfefab4ffdb5",
            "ff8d6156e0db419eb91991bcb699271a",
            "f558226643fa48d3b65ac7987eb722ef",
            "0a592ed2acd343f88c0a9b482f105243",
            "50d10268f1a54b32b0b08467bd773351",
            "83df68dfc86b4fc2978a090f25a8ec28",
            "628f498cf63a4a66bfa6fd258ea6f12e",
            "3c2825ef3d1d490f9d5b073996501e35",
            "f2a7bc33240f4e96a5935227ac4e970c",
            "3a67f88f94ac4f39a1e5c382e70d445a",
            "b8f918647a20422c98f2eefb11a0d9cc",
            "44f51b5a2e6445aca6a2d91cf354b722",
            "4cf8bc058dee46b0bdbb147f4bc0aa40",
            "0de578f187b7462b9e2e53643e8e8ad1",
            "60c82cd6c786452ea4f8e3b88d45343b",
            "6be556fce2ad49ffa84e0232b41b7dfb",
            "029d0d4d84654f5db2ce1e44070e8274",
            "7fb429d83d6542308fa29498a3a2ba42",
            "583a05a18a504876ad710c1e5ad11095",
            "624c42c9168346d98ef580cca01db8f8",
            "caa08bd03ccf45e3be68b324ea1a8a64",
            "af10b6fbbf0c4103aea0ca41977304c2",
            "05d4c4e80aaa449ea6e0c36ef0b6221c",
            "a7fed59ac4cc493d98f40fbd941edef8",
            "f257b5d3af364e80a608fa37db0e5a05",
            "98c1fc529aa746e69750f0697ccf14ad",
            "1c6117879c1742f485fd7406e1de200f",
            "5c34baf5cdc34593a5c22445f8c98900",
            "06607a157ab541b0aa0ae07f8df7c197",
            "25b77cb369174bbf9dc79725f0bdc8ac",
            "e71de291ac9a4b468d0f95307f42188d",
            "3e43f8b1649442f79e962810045e77d6",
            "9c4431a87e0b41b98756cc62c5fd35bd",
            "d66befc43fc141db8a175f7e3f257b2d",
            "379c814028c94cff8a02a3b4495d983e",
            "ef05a72edfe44fda947372b8c228ed2a",
            "cf43fb14aa624817a56bb8716d212899",
            "cdf1340bc0624252a84e205ad127c99c",
            "8bfe0f992f80463aa358db0c043ef251",
            "1460c195be4c4dccaa8b8be0a5ac999c",
            "03f3ce6ac4b341afa6c69b8f8338dc67",
            "8193348327744e91b785fefd770d737a",
            "7115579cd65d409b809e961a2faad971",
            "939c7645f16640d78194bcd7edc282ec"
          ]
        },
        "outputId": "8f6ddf0b-f88c-4698-fb3b-de4ce24f564c",
        "id": "DpQ9Lp6_e0DM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/53.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e8744a8e13d400fa3048d8babdcb0e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dataset_infos.json:   0%|          | 0.00/138k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8813abd74f6a45c8b3bb9d3c2df18afe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/16.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "024c5b665383415aafa2d516c03cd15d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/5.63k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72d20a0acacd4d18a23a566f2e441125"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dev-00000-of-00001.parquet:   0%|          | 0.00/3.77k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53081f9667154d2dabcbbfefab4ffdb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44f51b5a2e6445aca6a2d91cf354b722"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/11 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05d4c4e80aaa449ea6e0c36ef0b6221c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d66befc43fc141db8a175f7e3f257b2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [08:23<00:00, 10.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_de = MMLUEvaluator(topic=\"medical_genetics\", language=\"German\")\n",
        "\n",
        "results_de = evaluator_de.run_evaluation(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                         n_questions=10)\n",
        "print(f'\\nAccuracy: {results_de[\"accuracy\"]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97361557-7d52-4e34-dbce-273f337aef70",
        "id": "-_jrw-z0e0DN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [02:16<00:00, 13.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FIW2nMg8e0DN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}