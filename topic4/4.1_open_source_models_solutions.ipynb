{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic4/4.1_open_source_models_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "The course is in development now, with more materials coming soon."
      ],
      "metadata": {
        "id": "qQh7ewhdqZFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice tasks - rewriting chat bots and agents with open source models\n",
        "\n",
        "In Weeks 1-3 we've created some boilerplate code for creating chat bots and agents. In the tasks below, you'll update it to use open source LLMs instead of APIs."
      ],
      "metadata": {
        "id": "oXOIzuceAlNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1. Updating RAGAgent\n",
        "\n",
        "In this task, you'll need to take the `RAGAgent` class you created in notebook 3.1 (or borrow it from the solution notebook) and replace Nebius AI Studio API calls by open source LLM calls.\n",
        "\n",
        "You can use the code from the \"Tool usage\" section of this notebook. Just don't forget that you'll need a custom parser to extract tool calls from the model's answers. Also, make sure to correctly process the results in case if there seems to be no tool calls in the outputs.\n",
        "\n",
        "In this task, you can assume that your agent gets only one prompt at a time, so you don't need batch inference.\n",
        "\n",
        "If time and GPU resources allow, compare how the agent will work for Llama 3.2 models of different size. Generally, for larger sizes Llamas should be more adept at generating valid tool calls and not generating anything else."
      ],
      "metadata": {
        "id": "bukrlaIM8iMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <YOUR CODE HER>"
      ],
      "metadata": {
        "id": "hhhJw4owRvVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution**"
      ],
      "metadata": {
        "id": "xn2-HyDxRrzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tavily-python"
      ],
      "metadata": {
        "id": "lOxeP8Be_3WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "with open(\"tavily_api_key\", \"r\") as file:\n",
        "    tavily_api_key = file.read().strip()\n",
        "os.environ[\"TAVILY_API_KEY\"] = tavily_api_key\n",
        "\n",
        "with open(\"hf_access_token\", \"r\") as file:\n",
        "    hf_access_token = file.read().strip()\n",
        "os.environ[\"HF_ACCESS_TOKEN\"] = hf_access_token"
      ],
      "metadata": {
        "id": "ViWFxy60_9lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, deque\n",
        "from typing import Dict, Any, List, Optional, Callable\n",
        "import json\n",
        "import traceback\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "class RAGAgent:\n",
        "    def __init__(self,\n",
        "                 model_name: str,\n",
        "                 hf_access_token: str,\n",
        "                 search_client,\n",
        "                 history_size: int = 10,\n",
        "                 get_system_message: Callable[[], Optional[Dict[str, str]]] = None,\n",
        "                 search_depth: str = \"advanced\",\n",
        "                 max_search_results: int = 5\n",
        "                 ):\n",
        "        \"\"\"Initialize the chat agent with RAG tool using local LLM.\n",
        "\n",
        "        Args:\n",
        "            model_name: The Hugging Face model name (e.g., \"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "            hf_access_token: Hugging Face access token\n",
        "            search_client: Search client instance (for example, Tavily)\n",
        "            history_size: Number of messages to keep in history per user\n",
        "            get_system_message: Function to retrieve the system message\n",
        "            search_depth: Depth of web search ('basic' or 'advanced')\n",
        "            max_search_results: Maximum number of search results to retrieve\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.hf_access_token = hf_access_token\n",
        "        self.search_client = search_client\n",
        "        self.history_size = history_size\n",
        "\n",
        "        # Initialize tokenizer and model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_access_token)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            return_dict=True,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            token=hf_access_token\n",
        "        )\n",
        "\n",
        "        # Set pad token if not already set\n",
        "        if self.tokenizer.pad_token_id is None:\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "        if self.model.config.pad_token_id is None:\n",
        "            self.model.config.pad_token_id = self.model.config.eos_token_id\n",
        "\n",
        "        # If no system message function is provided, use the default one\n",
        "        self.get_system_message = get_system_message if get_system_message else self._default_system_message\n",
        "\n",
        "        self.search_depth = search_depth\n",
        "        self.max_search_results = max_search_results\n",
        "\n",
        "        # Initialize chat history storage\n",
        "        self.chat_histories = defaultdict(lambda: deque(maxlen=history_size))\n",
        "\n",
        "        # Function definitions for the LLM\n",
        "        self.function_definitions = \"\"\"\n",
        "        [\n",
        "            {\n",
        "                \"name\": \"retrieve_information\",\n",
        "                \"description\": \"You ALWAYS use this function if you don't have enough information to answer user's query. For example, the user asks about something which is after your knowlege cutoff. In this case, you will use this function to query and get additional context to provide a complete and accurate answer.\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"dict\",\n",
        "                    \"required\": [\"query\"],\n",
        "                    \"properties\": {\n",
        "                        \"query\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The search query to use for retrieving information. This should be a refined version of the user's question optimized for web search.\"\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "        \"\"\"\n",
        "\n",
        "        # Map available tool functions\n",
        "        self.available_tools = {\n",
        "            \"retrieve_information\": self.retrieve_information\n",
        "        }\n",
        "\n",
        "    def _default_system_message(self) -> str:\n",
        "        \"\"\"Default system message if none is provided.\"\"\"\n",
        "        return f\"\"\"You are a helpful assistant with access to various functions. When responding to a question:\n",
        "1. If the question can be answered directly without using functions, provide a clear and helpful response.\n",
        "2. If the question requires using one of the functions available for you to provide a complete answer:\n",
        "   - Answer by ONLY outputting the necessary function call using the format: [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
        "   - DO NOT output anything except for the function call\n",
        "Here is a list of functions in JSON format that you can use when appropriate:\\n\\n{self.function_definitions}\\n\"\"\"\n",
        "\n",
        "    def extract_query(self, completion):\n",
        "        \"\"\"\n",
        "        Extract query from a retrieve_information tool call in a single completion.\n",
        "\n",
        "        Args:\n",
        "            completion (str): Text response from the LLM\n",
        "\n",
        "        Returns:\n",
        "            str or None: The extracted query string, or None if no query is found\n",
        "        \"\"\"\n",
        "        # Look for query pattern in either format:\n",
        "        # Format 1: retrieve_information(query=\"...\")\n",
        "        # Format 2: retrieve_information, query=\"...\"\n",
        "        pattern = r'retrieve_information(?:\\(query=|,\\s*query=)\"([^\"]*)\"'\n",
        "\n",
        "        # Find match\n",
        "        match = re.search(pattern, completion)\n",
        "\n",
        "        if match:\n",
        "            # Extract the query value\n",
        "            return match.group(1)\n",
        "        else:\n",
        "            # No query found\n",
        "            return None\n",
        "\n",
        "    def retrieve_information(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Perform a web search using the search client and format the results.\n",
        "\n",
        "        Args:\n",
        "            query: The search query\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing formatted search results and metadata\n",
        "        \"\"\"\n",
        "        try:\n",
        "            search_results = self.search_client.search(\n",
        "                query=query,\n",
        "                search_depth=self.search_depth,\n",
        "                max_results=self.max_search_results\n",
        "            )\n",
        "\n",
        "            formatted_results = []\n",
        "            for result in search_results.get('results', []):\n",
        "                content = result.get('content', '').strip()\n",
        "                url = result.get('url', '')\n",
        "                if content:\n",
        "                    formatted_results.append(f\"Content: {content}\\nSource: {url}\\n\")\n",
        "\n",
        "            # Join all results with proper formatting\n",
        "            context = \"\\n\".join(formatted_results)\n",
        "\n",
        "            return {\n",
        "                \"context\": context,\n",
        "                \"query\": query,\n",
        "                \"num_results\": len(formatted_results),\n",
        "                \"success\": True,\n",
        "                \"message\": f\"Retrieved {len(formatted_results)} results for query: '{query}'\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in retrieve_information: {str(e)}\")\n",
        "            return {\n",
        "                \"context\": \"\",\n",
        "                \"query\": query,\n",
        "                \"num_results\": 0,\n",
        "                \"success\": False,\n",
        "                \"message\": f\"Failed to retrieve information: {str(e)}\"\n",
        "            }\n",
        "\n",
        "    def chat(self, user_message: str, user_id: str, debug: bool = False) -> str:\n",
        "        \"\"\"Process a user message and return the agent's response.\n",
        "\n",
        "        Args:\n",
        "            user_message: The message from the user\n",
        "            user_id: Unique identifier for the user\n",
        "            debug: Whether to print debug information\n",
        "\n",
        "        Returns:\n",
        "            str: The agent's response\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Construct conversation history\n",
        "            conversation = []\n",
        "\n",
        "            # Add system message\n",
        "            system_message = self.get_system_message()\n",
        "            conversation.append({\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_message\n",
        "            })\n",
        "\n",
        "            # Add chat history\n",
        "            for msg in self.chat_histories[user_id]:\n",
        "                conversation.append(msg)\n",
        "\n",
        "            # Add the new user message\n",
        "            user_message_dict = {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_message\n",
        "            }\n",
        "            conversation.append(user_message_dict)\n",
        "\n",
        "            # Save user message to history\n",
        "            self.chat_histories[user_id].append(user_message_dict)\n",
        "\n",
        "            # Format the prompt\n",
        "            prompt = self.tokenizer.apply_chat_template(\n",
        "                conversation=conversation,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=True\n",
        "            )\n",
        "\n",
        "            if debug:\n",
        "                print(f\"#Formatted prompt:\\n{prompt[:500]}...\\n\")\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                padding_side=\"left\",\n",
        "            ).to(self.model.device)\n",
        "\n",
        "            # Generate\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256,\n",
        "                do_sample=True,\n",
        "                temperature=0.6,\n",
        "            )\n",
        "\n",
        "            # Extract the model's output (excluding the prompt tokens)\n",
        "            output_token_ids = outputs[:, inputs.input_ids.shape[1]:]\n",
        "            completion = self.tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "            if debug:\n",
        "                print(f\"#Initial model output:\\n{completion}\\n\")\n",
        "\n",
        "            # Check if the model wants to use a tool\n",
        "            query = self.extract_query(completion)\n",
        "\n",
        "            # If a tool call is found\n",
        "            if query:\n",
        "                if debug:\n",
        "                    print(f\"#Tool call detected with query: {query}\\n\")\n",
        "\n",
        "                # Process the search request\n",
        "                search_result = self.retrieve_information(query)\n",
        "\n",
        "                # Format the search result as context for the model\n",
        "                context = f\"<context>\\n{search_result['context']}\\n</context>\"\n",
        "\n",
        "                if debug:\n",
        "                    print(f\"#Retrieved context (preview):\\n{context[:200]}...\\n\")\n",
        "\n",
        "                # Create a new conversation with the retrieved information\n",
        "                second_conversation = conversation.copy()\n",
        "\n",
        "                # Add the tool response as an assistant message\n",
        "                second_conversation.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": f\"I need to search for more information about this. [retrieve_information(query=\\\"{query}\\\")]\"\n",
        "                })\n",
        "\n",
        "                # Add the search results\n",
        "                second_conversation.append({\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": f\"Search results for: {query}\\n{context}\"\n",
        "                })\n",
        "\n",
        "                # Format the prompt for the final answer\n",
        "                second_prompt = self.tokenizer.apply_chat_template(\n",
        "                    conversation=second_conversation,\n",
        "                    tokenize=False,\n",
        "                    add_generation_prompt=True\n",
        "                )\n",
        "\n",
        "                # Tokenize\n",
        "                second_inputs = self.tokenizer(\n",
        "                    second_prompt,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    padding_side=\"left\",\n",
        "                ).to(self.model.device)\n",
        "\n",
        "                # Generate the final answer\n",
        "                second_outputs = self.model.generate(\n",
        "                    **second_inputs,\n",
        "                    max_new_tokens=512,  # Allow for a longer response with the context\n",
        "                    do_sample=True,\n",
        "                    temperature=0.7,\n",
        "                )\n",
        "\n",
        "                # Extract the model's final output\n",
        "                final_output_token_ids = second_outputs[:, second_inputs.input_ids.shape[1]:]\n",
        "                final_response = self.tokenizer.batch_decode(final_output_token_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "                if debug:\n",
        "                    print(f\"#Final response with context:\\n{final_response[:200]}...\\n\")\n",
        "\n",
        "                # Save the assistant's final response to chat history\n",
        "                self.chat_histories[user_id].append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": final_response\n",
        "                })\n",
        "\n",
        "                return final_response\n",
        "\n",
        "            else:\n",
        "                # The model provided a direct answer without needing tools\n",
        "                if debug:\n",
        "                    print(f\"#Direct answer without tools:\\n{completion}\\n\")\n",
        "\n",
        "                # Save the assistant's response to chat history\n",
        "                self.chat_histories[user_id].append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": completion\n",
        "                })\n",
        "\n",
        "                return completion\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error in chat: {str(e)}\"\n",
        "            print(error_msg)\n",
        "            print(traceback.format_exc())\n",
        "            return error_msg\n",
        "\n",
        "    def get_chat_history(self, user_id: str) -> list:\n",
        "        \"\"\"Retrieve the chat history for a specific user.\n",
        "\n",
        "        Args:\n",
        "            user_id: Unique identifier for the user\n",
        "\n",
        "        Returns:\n",
        "            list: List of message dictionaries\n",
        "        \"\"\"\n",
        "        return list(self.chat_histories[user_id])"
      ],
      "metadata": {
        "id": "jDfAADuo-ZmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tavily import TavilyClient\n",
        "\n",
        "# Initialize search client\n",
        "tavily_client = TavilyClient(api_key=tavily_api_key)\n",
        "\n",
        "# Initialize the RAG agent\n",
        "rag_agent = RAGAgent(\n",
        "    model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    hf_access_token=hf_access_token,\n",
        "    search_client=tavily_client,\n",
        "    search_depth=\"advanced\",\n",
        "    max_search_results=5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "77c454922f9244e48ba1ec157fcee056",
            "17c4ddea8722425fa6809db343898bca",
            "9f05a88b22b646f8954eb81a0394c54d",
            "0ce64d37eff44de1be913074da56892d",
            "a41bb9ddd4f44f8e85ec20abd704088e",
            "7c388dcb8b1d4e348f522f6f80758fd4",
            "bc4dbe2ea7c2426794d7e6e02488d486",
            "0bd6f11212af4ee6957c286f8709ebdb",
            "3f79bc81f4ee47068693fa67c82deed4",
            "e2b355efbe3f44adbe818db2b7841075",
            "453187bd53be40ac9b899938dfcf6027"
          ]
        },
        "id": "2t1Jn_VhBWko",
        "outputId": "e3ec1762-78c7-4114-f9dd-ab3db9562b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77c454922f9244e48ba1ec157fcee056"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now run the agent outputting lots of debugging information:"
      ],
      "metadata": {
        "id": "yvg5f4VOBtEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "# Example usage\n",
        "user_id = str(uuid.uuid4())\n",
        "\n",
        "response = rag_agent.chat(\"Nice day, isn't it?\", user_id, debug=True)\n",
        "print(\"Response:\", response)\n",
        "\n",
        "response = rag_agent.chat(\"Who won gold in break dance at the 2024 olympics?\", user_id, debug=True)\n",
        "print(\"Response:\", response)\n",
        "\n",
        "response = rag_agent.chat(\"Who played Thaddeus Ross in Captain America: Brave New World?\", user_id, debug=True)\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BieJWInz_f1V",
        "outputId": "2934b84e-08b2-4825-cf48-63f637f277bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Formatted prompt:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 15 May 2025\n",
            "\n",
            "You are a helpful assistant with access to various functions. When responding to a question:\n",
            "1. If the question can be answered directly without using functions, provide a clear and helpful response.\n",
            "2. If the question requires using one of the functions available for you to provide a complete answer:\n",
            "   - Answer by ONLY outputting the necessary function call using the form...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Initial model output:\n",
            "It's hard to say whether it's a nice day or not, as I'm a large language model, I don't have have access to real-time information about the current weather or location. However, I can tell you that I'm here to help answer any questions you may have, so let's make the most of our conversation! What's on your mind?\n",
            "\n",
            "#Direct answer without tools:\n",
            "It's hard to say whether it's a nice day or not, as I'm a large language model, I don't have have access to real-time information about the current weather or location. However, I can tell you that I'm here to help answer any questions you may have, so let's make the most of our conversation! What's on your mind?\n",
            "\n",
            "Response: It's hard to say whether it's a nice day or not, as I'm a large language model, I don't have have access to real-time information about the current weather or location. However, I can tell you that I'm here to help answer any questions you may have, so let's make the most of our conversation! What's on your mind?\n",
            "#Formatted prompt:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 15 May 2025\n",
            "\n",
            "You are a helpful assistant with access to various functions. When responding to a question:\n",
            "1. If the question can be answered directly without using functions, provide a clear and helpful response.\n",
            "2. If the question requires using one of the functions available for you to provide a complete answer:\n",
            "   - Answer by ONLY outputting the necessary function call using the form...\n",
            "\n",
            "#Initial model output:\n",
            "[retrieve_information(query=\"2024 Olympics break dance gold medal winner\")]\n",
            "\n",
            "#Tool call detected with query: 2024 Olympics break dance gold medal winner\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Retrieved context (preview):\n",
            "<context>\n",
            "Content: Featured Video\n",
            "\n",
            "Ranking Every Week 1 Game 🏈\n",
            "\n",
            "Olympic Breakdancing 2024 Results: Women's Breaking Medal Winners and Highlights\n",
            "\n",
            "Breakdancing made its debut at the 2024 Paris Olympics...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Final response with context:\n",
            "The gold medal winners in breakdancing at the 2024 Olympics were:\n",
            "\n",
            "* Women's: Ami Yuasa of Japan\n",
            "* Men's: Phil Wizard (Philip Kim) of Canada...\n",
            "\n",
            "Response: The gold medal winners in breakdancing at the 2024 Olympics were:\n",
            "\n",
            "* Women's: Ami Yuasa of Japan\n",
            "* Men's: Phil Wizard (Philip Kim) of Canada\n",
            "#Formatted prompt:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 15 May 2025\n",
            "\n",
            "You are a helpful assistant with access to various functions. When responding to a question:\n",
            "1. If the question can be answered directly without using functions, provide a clear and helpful response.\n",
            "2. If the question requires using one of the functions available for you to provide a complete answer:\n",
            "   - Answer by ONLY outputting the necessary function call using the form...\n",
            "\n",
            "#Initial model output:\n",
            "[retrieve_information, query=\"Thaddeus Ross Captain America: Brave New World\"]\n",
            "\n",
            "#Tool call detected with query: Thaddeus Ross Captain America: Brave New World\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Retrieved context (preview):\n",
            "<context>\n",
            "Content: Thaddeus Ross is a fictional character originally portrayed by William Hurt and subsequently by Harrison Ford in the Marvel Cinematic Universe (MCU) film franchise, based on the Mar...\n",
            "\n",
            "#Final response with context:\n",
            "The actor who played Thaddeus Ross in Captain America: Brave New World is Harrison Ford....\n",
            "\n",
            "Response: The actor who played Thaddeus Ross in Captain America: Brave New World is Harrison Ford.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_agent.get_chat_history(user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJk135cfB013",
        "outputId": "02420af8-23bc-4f5b-ead1-951d1028d7db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user', 'content': \"Nice day, isn't it?\"},\n",
              " {'role': 'assistant',\n",
              "  'content': \"It's hard to say whether it's a nice day or not, as I'm a large language model, I don't have have access to real-time information about the current weather or location. However, I can tell you that I'm here to help answer any questions you may have, so let's make the most of our conversation! What's on your mind?\"},\n",
              " {'role': 'user',\n",
              "  'content': 'Who won gold in break dance at the 2024 olympics?'},\n",
              " {'role': 'assistant',\n",
              "  'content': \"The gold medal winners in breakdancing at the 2024 Olympics were:\\n\\n* Women's: Ami Yuasa of Japan\\n* Men's: Phil Wizard (Philip Kim) of Canada\"},\n",
              " {'role': 'user',\n",
              "  'content': 'Who played Thaddeus Ross in Captain America: Brave New World?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'The actor who played Thaddeus Ross in Captain America: Brave New World is Harrison Ford.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2. Updating the NPC Factory\n",
        "\n",
        "In this task, you'll need to update the NPC Factory from notebook 1.7 so that it could use a self-depolyed LLM.\n",
        "\n",
        "**Note**. Depending on an LLM you choose, and on GPU you use, you might encounter Out of memory error for a large context size. So, you might want to check the max context size tolerated by your GPU and to cut past dialog histories based on that value.\n",
        "\n",
        "**If you're in for an advanced challege:** In a real-world situation, you'd want to consider batch processing, especially if your service becomes popular and it receives many queries every minute. But then, many questions might arise, like:\n",
        "\n",
        "* What is the timeout after which we send even a non-complete batch to the LLM?\n",
        "* How to balance back size with conversation lengths?\n",
        "* Should we group conversations into batches depending on the conversations length?\n",
        "\n",
        "You're not supposed to fully answer this question here, but we encourage you to experiment with batch sizes and conversation lengths to understand what you chosen GPU is capable of.\n",
        "\n",
        "For a further treatment of the batch size vs GPU vs LLM, see the **Inference metrics** long read and notebook."
      ],
      "metadata": {
        "id": "qLwqbWkE7VC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A basic solution**"
      ],
      "metadata": {
        "id": "XYF9lK5s7pHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"hf_access_token\", \"r\") as file:\n",
        "    hf_access_token = file.read().strip()\n",
        "\n",
        "'''\n",
        "# Or use a colab secret:\n",
        "\n",
        "!pip install --upgrade huggingface_hub\n",
        "\n",
        "from google.colab import userdata\n",
        "hf_access_token = userdata.get('HF_TOKEN')\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "XtVdgZZ4Tp-n",
        "outputId": "260e4342-48d3-4978-bbc8-c69667601bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Or use a colab secret:\\n\\n!pip install --upgrade huggingface_hub\\n\\nfrom google.colab import userdata\\nhf_access_token = userdata.get('HF_TOKEN')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, deque\n",
        "from typing import Dict, Any, Optional\n",
        "import datetime\n",
        "import string\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import re\n",
        "\n",
        "@dataclass\n",
        "class NPCConfig:\n",
        "    world_description: str\n",
        "    character_description: str\n",
        "    history_size: int = 10\n",
        "    has_scratchpad: bool = False\n",
        "\n",
        "class NPCFactoryError(Exception):\n",
        "    \"\"\"Base exception class for NPC Factory errors.\"\"\"\n",
        "    pass\n",
        "\n",
        "class NPCNotFoundError(NPCFactoryError):\n",
        "    \"\"\"Raised when trying to interact with a non-existent NPC.\"\"\"\n",
        "    def __init__(self, npc_id: str):\n",
        "        self.npc_id = npc_id\n",
        "        super().__init__(f\"NPC with ID '{npc_id}' not found\")\n",
        "\n",
        "class SimpleChatNPC:\n",
        "    def __init__(self, tokenizer, model, config: NPCConfig):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.chat_histories = defaultdict(lambda: deque(maxlen=config.history_size))\n",
        "\n",
        "    def get_system_message(self) -> Dict[str, str]:\n",
        "        \"\"\"Returns the system message that defines the NPC's behavior.\"\"\"\n",
        "        character_description = self.config.character_description\n",
        "\n",
        "        if self.config.has_scratchpad:\n",
        "            character_description += \"\"\"\n",
        "You can use scratchpad for thinking before you answer: whatever you output between #SCRATCHPAD and #ANSWER won't be shown to anyone.\n",
        "You start your output with #SCRATCHPAD and after you've done thinking, you #ANSWER\"\"\"\n",
        "\n",
        "        return {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"\"\"WORLD SETTING: {self.config.world_description}\n",
        "###\n",
        "{character_description}\"\"\"\n",
        "        }\n",
        "\n",
        "    def chat(self, user_message: str, user_id: str, debug: bool = False) -> str:\n",
        "        \"\"\"Process a user message and return the NPC's response.\"\"\"\n",
        "        try:\n",
        "            # Construct conversation history\n",
        "            conversation = []\n",
        "\n",
        "            # Add system message\n",
        "            conversation.append(self.get_system_message())\n",
        "\n",
        "            # Add conversation history\n",
        "            history = list(self.chat_histories[user_id])\n",
        "            if history:\n",
        "                conversation.extend(history)\n",
        "\n",
        "            # Add new user message\n",
        "            user_message_dict = {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_message\n",
        "            }\n",
        "            conversation.append(user_message_dict)\n",
        "\n",
        "            # Format the prompt using the chat template\n",
        "            prompt = self.tokenizer.apply_chat_template(\n",
        "                conversation=conversation,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=True\n",
        "            )\n",
        "\n",
        "            if debug:\n",
        "                print(f\"#Formatted prompt:\\n{prompt[:500]}...\\n\")\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                padding_side=\"left\",\n",
        "            ).to(self.model.device)\n",
        "\n",
        "            # Generate response\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=512,\n",
        "                do_sample=True,\n",
        "                temperature=0.6,\n",
        "            )\n",
        "\n",
        "            # Extract the model's output (excluding the prompt tokens)\n",
        "            output_token_ids = outputs[:, inputs.input_ids.shape[1]:]\n",
        "            response = self.tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "            if debug:\n",
        "                print(f\"#Model response:\\n{response}\\n\")\n",
        "\n",
        "            # Handle scratchpad if enabled\n",
        "            response_clean = response\n",
        "            if self.config.has_scratchpad:\n",
        "                scratchpad_match = re.search(r\"#SCRATCHPAD(:?)(.*?)#ANSWER(:?)\", response, re.DOTALL)\n",
        "                if scratchpad_match:\n",
        "                    response_clean = response[scratchpad_match.end():].strip()\n",
        "\n",
        "            # Store user message and response in history\n",
        "            self.chat_histories[user_id].append(user_message_dict)\n",
        "            self.chat_histories[user_id].append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": response  # Store the full response including scratchpad\n",
        "            })\n",
        "\n",
        "            # Return the message to the user without scratchpad\n",
        "            return response_clean\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "class NPCFactory:\n",
        "    def __init__(self, model_name: str, hf_access_token: str):\n",
        "        \"\"\"Initialize the NPC Factory with an open-source LLM.\n",
        "\n",
        "        Args:\n",
        "            model_name: The Hugging Face model name (e.g., \"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "            hf_access_token: Hugging Face access token\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.hf_access_token = hf_access_token\n",
        "        self.npcs: Dict[str, SimpleChatNPC] = {}\n",
        "        self.user_ids: Dict[str, str] = {}  # username -> user_id mapping\n",
        "\n",
        "        # Initialize tokenizer and model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_access_token)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            return_dict=True,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            token=hf_access_token\n",
        "        )\n",
        "\n",
        "        # Set pad token if not already set\n",
        "        if self.tokenizer.pad_token_id is None:\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "        if self.model.config.pad_token_id is None:\n",
        "            self.model.config.pad_token_id = self.model.config.eos_token_id\n",
        "\n",
        "    def generate_id(self) -> str:\n",
        "        \"\"\"Generate a random unique identifier.\"\"\"\n",
        "        return ''.join(random.choice(string.ascii_letters) for _ in range(8))\n",
        "\n",
        "    def register_user(self, username: str) -> str:\n",
        "        \"\"\"Register a new user and return their unique ID.\n",
        "        If username already exists, appends a numerical suffix.\"\"\"\n",
        "        base_username = username\n",
        "        suffix = 1\n",
        "\n",
        "        # Keep trying with incremented suffixes until we find an unused name\n",
        "        while username in self.user_ids:\n",
        "            username = f\"{base_username}_{suffix}\"\n",
        "            suffix += 1\n",
        "\n",
        "        user_id = self.generate_id()\n",
        "        self.user_ids[username] = user_id\n",
        "        return user_id\n",
        "\n",
        "    def register_npc(self, world_description: str, character_description: str,\n",
        "                     history_size: int = 10, has_scratchpad: bool = False) -> str:\n",
        "        \"\"\"Create and register a new NPC, returning its unique ID.\"\"\"\n",
        "        npc_id = self.generate_id()\n",
        "\n",
        "        config = NPCConfig(\n",
        "            world_description=world_description,\n",
        "            character_description=character_description,\n",
        "            history_size=history_size,\n",
        "            has_scratchpad=has_scratchpad\n",
        "        )\n",
        "\n",
        "        # Pass the shared tokenizer and model to the NPC\n",
        "        self.npcs[npc_id] = SimpleChatNPC(self.tokenizer, self.model, config)\n",
        "        return npc_id\n",
        "\n",
        "    def chat_with_npc(self, npc_id: str, user_id: str, message: str, debug: bool = False) -> str:\n",
        "        \"\"\"Send a message to a specific NPC from a specific user.\n",
        "\n",
        "        Args:\n",
        "            npc_id: The unique identifier of the NPC\n",
        "            user_id: The unique identifier of the user\n",
        "            message: The message to send\n",
        "            debug: Whether to print debug information\n",
        "\n",
        "        Returns:\n",
        "            The NPC's response\n",
        "\n",
        "        Raises:\n",
        "            NPCNotFoundError: If the specified NPC doesn't exist\n",
        "        \"\"\"\n",
        "        if npc_id not in self.npcs:\n",
        "            raise NPCNotFoundError(npc_id)\n",
        "\n",
        "        npc = self.npcs[npc_id]\n",
        "        return npc.chat(message, user_id, debug=debug)\n",
        "\n",
        "    def get_npc_chat_history(self, npc_id: str, user_id: str) -> list:\n",
        "        \"\"\"Retrieve chat history between a specific user and NPC.\n",
        "\n",
        "        Args:\n",
        "            npc_id: The unique identifier of the NPC\n",
        "            user_id: The unique identifier of the user\n",
        "\n",
        "        Returns:\n",
        "            List of message dictionaries containing the chat history\n",
        "\n",
        "        Raises:\n",
        "            NPCNotFoundError: If the specified NPC doesn't exist\n",
        "        \"\"\"\n",
        "        if npc_id not in self.npcs:\n",
        "            raise NPCNotFoundError(npc_id)\n",
        "\n",
        "        return list(self.npcs[npc_id].chat_histories[user_id])"
      ],
      "metadata": {
        "id": "_pNENBkkT5lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a factory\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "# Use Qwen instead if you didn't get access to Llama 3.2\n",
        "# model_name = \"Qwen2.5-3B-Instruct\"\n",
        "\n",
        "npc_factory = NPCFactory(model_name=model_name, hf_access_token=hf_access_token)"
      ],
      "metadata": {
        "id": "hajZ4pSI7ku5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461,
          "referenced_widgets": [
            "aee4097799914d818e96fb291ace4177",
            "e64fc218869248da9b2cfbfb22046193",
            "ced75a26df744668b7c42056710d5eeb",
            "917e768e967f4c15bd343fb8c77b9c0d",
            "256bcd68b73f46769a7a579572261c1a",
            "68fec76bed2641808e1763fef0cc4515",
            "60c6ce9ad741442693d9412e1c8cdd16",
            "844e6eb043754f639d8f1d9513571a35",
            "7202f6183f4e4e108a6ac77e2ea6a302",
            "6201e103a1034977a241b954be597336",
            "2393190808eb470eb85586541ebbf962",
            "b1d7e32d8a0d46b992ae13a8e512c13e",
            "6b1cf66e21624d2594ec429b5b078f58",
            "6a51e355d6ba4652afe77517b40a050e",
            "3d885a3c5bab4230a0b9121ce6441e0d",
            "d0637b2da5304e449f014e49811cd0a6",
            "4f7caf241f3f4a5080580ccd2d26a401",
            "80d7d99c3404430aa18f47d0602d07cd",
            "79f75df8666240d39f89fcdc7cb89601",
            "ced77c894d754251a213a9527a02307f",
            "281fd39fc09243e09cfcb56c688a9c7c",
            "635beff0a2704598bb254e6cb483ffa3",
            "730bfad8991c4205982cacbeb8da0bf5",
            "be2623ace041496ea1c26e768c8ff042",
            "e69139b7a1fb4244a42a9266ef8a36e0",
            "4451676661834964b4d05d014a959e7c",
            "35279abba4a340a5b35a0257af517b61",
            "d2f505a3079c4f709f189612c0669bb0",
            "2080f1dbe0504243a716419b859ac830",
            "805c96139bf64dc390517cb744f1b2c5",
            "10f44a5b7b924a228394259731c7bdf9",
            "5768726347844809b9d4ff55221a780c",
            "b7739248c83640ee90285ca390e053ca",
            "3bff8c178f014eccb8d5ea987bd9e89a",
            "93663a2dd30a42d48b34ffc83bd73d3e",
            "07d906d1100c4023b66468eb05723780",
            "3cd27f013682467eb6be9d68842b0577",
            "c6be43ce86c748f6b33aeb270f378177",
            "8abb42fc00b54d75a9c5b7c80859e412",
            "656470154d8a4549bf65ea277b3c7550",
            "45b87796587f4d4d969ebd57c94deb62",
            "98ca9d79e5634391addd84abcea3e1f7",
            "220a488a4dcd4b9ab98fe6e9f06074fd",
            "18caeddc6ee5423ebb719f436a32d8e7",
            "b377ee5eb2ab484e8095b6f51d37e365",
            "b4cea74be4d0459bb16694ec2b8bb48d",
            "3e51ac10eb194e5aa749439317ca41f2",
            "380cc6b0b9d4416db69b660762ac4561",
            "afbbeb25c86249158fa91a627c4413e7",
            "3ef8e448c8dc437292adb34298031382",
            "2eef8df7b0bc4d4c90b71e662e1481c8",
            "3db827cde26d475881cbe8ed6aab84c1",
            "a4a227e0a6a646adbfa705a9f1339df3",
            "7b31b420bbb746378106e3f326c4fb97",
            "40f1dde86c544b0f9fb52529ab67be63",
            "90b8293c468d46bc96d632e1dbdf79a0",
            "e2df762bffbf4601ba238d0c79ae2760",
            "4058703a96c14bd486be05c15fb3c93c",
            "d94d5fd3db4c429ea41384d95c9b8572",
            "750dc13ce1f74d3185708bb719a5c6fa",
            "82bca02b418e4cfb82f241400cd9879c",
            "356799be96dd46a4a1c7a4c83d7c23b3",
            "43e929c51aad47dca002aea9f54fca49",
            "4112d33426684358a6dcb6e60762eb79",
            "cdf3e83f14c64573afe6c599181b25ed",
            "d3e8106a156b4d7b9cfc8f4cb05298b9",
            "d9854c1a66e04bc8b16d14101c0e9611",
            "356349223d41405eb4a96e8b20d938b1",
            "3af4dac1fd2442ee8b1141d7cb61758e",
            "be5b5f22d7c94cfda8bfbe39c121c8f8",
            "2aabff96f263496cb8f603404a9f2bc5",
            "6d4a164e71904727bb7e3ec57dcafa2b",
            "fce7d56c916549e1b184d801f099dddb",
            "df394ab40c8c4bea892d3c1be71034f9",
            "46f5b27ab91141e89d6d6d7ebf6e6d28",
            "5e0cb9e298ac493baf81ad62fda4bfcc",
            "d581df247b4241bba257b56f56b63589",
            "319a09266bf0409386cc03ed124aaeea",
            "6e7573e5a123459d82ac34f4ebb190e9",
            "452ba59cb4e44cd58c364509b4240558",
            "ea33a77dd09540c2a37ef78ac8dcd24f",
            "a361e93f5a864a1d89253d920e67e52f",
            "6df85a35ad064fa5ac7b982254607bcc",
            "f63129b7adac4a288cf21f5891c508a2",
            "a26e26518f1240a1b0b05a59ad6c45ee",
            "04e61967e24146ed802f03cbebd4b86f",
            "45d3c7990aa841639af7744e37ad7fcd",
            "5fffd54c03f44795b554d23def992e17",
            "bb9b9360d2be4ff49f212de0b81cbec5",
            "85c2d53485984db7b9c65a4399d9504b",
            "337d4019d6c349788f12ec3c6f6f068b",
            "368b15e9033a4f1bb74a251d8e3a9185",
            "864a31e8414f4d1f82721f4c77011b7b",
            "2bf2dbe2202f4c878092ac041cd6bfd9",
            "b84d9fa6ea7244d08a23c979a303933c",
            "e1ca14e260b44f5cb1312cccd046c5c0",
            "c1823885f55b440fa614ec0703fc3e31",
            "0c42c81f9ff24e07a29ea5e6e8409748",
            "72f25bf9dcf84138a1141e3e408450c1",
            "3687111b966f4217b3d5cbea659c8aea",
            "cec79d2d028c4d2e87b1acb551babaf4",
            "4264dd0c1559488f84b5d17575026e57",
            "bb776ddf8a6546eaa555b23fe09de92c",
            "ad2bd15bb6a44b8a9ed56b9796432089",
            "d5f0fd5f3ff64a84bc3911281cc42240",
            "a2691e2ec47d4c52912b31df59262dde",
            "3db527f786c145fbb75c8f8766f1b3aa",
            "b687062aaced460aa0ad015a94b42a5b",
            "b073d1940fe24e4481e44365f2d3554c",
            "eb4fa756373247b2a9d721e2b44cfccb"
          ]
        },
        "outputId": "cf880e25-88fd-4047-dcac-c3513795dbee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aee4097799914d818e96fb291ace4177"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1d7e32d8a0d46b992ae13a8e512c13e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "730bfad8991c4205982cacbeb8da0bf5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bff8c178f014eccb8d5ea987bd9e89a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b377ee5eb2ab484e8095b6f51d37e365"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90b8293c468d46bc96d632e1dbdf79a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9854c1a66e04bc8b16d14101c0e9611"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "319a09266bf0409386cc03ed124aaeea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb9b9360d2be4ff49f212de0b81cbec5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3687111b966f4217b3d5cbea659c8aea"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Register a user\n",
        "user_id = npc_factory.register_user(\"Alice\")\n",
        "\n",
        "# Create an NPC\n",
        "npc_id = npc_factory.register_npc(\n",
        "    world_description=\"Medieval London, XIII century\",\n",
        "    character_description=\"A knight at Edward I's court\",\n",
        "    has_scratchpad=False\n",
        ")"
      ],
      "metadata": {
        "id": "PQDP0E2v8Onc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)"
      ],
      "metadata": {
        "id": "_X9_LEhR-Mmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = npc_factory.chat_with_npc(npc_id, user_id,\n",
        "                                     \"\"\"Good day, sir knight!\"\"\"\n",
        "                                     )\n",
        "print(prettify_string(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuHyGK-T8n0X",
        "outputId": "f4650841-c6b4-4782-d371-d64d15ae4d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good morrow to thee, good fellow! I am Sir Edward de Montfort, a loyal knight\n",
            "of the realm and a trusted advisor to His Majesty, King Edward I. 'Tis a grand\n",
            "day to be alive, don't thou think? The sun shineth brightly upon our fair city\n",
            "of London, and the sounds of hammering and sawing fill the air as our great\n",
            "king's builders work tirelessly to construct grand new buildings and\n",
            "fortifications.\n",
            "\n",
            "I must say, I am in high spirits today, for I have just returned from a\n",
            "successful tournament in the city, where I fought valiantly alongside my fellow\n",
            "knights and won the favor of the king himself. 'Twas a grand spectacle, with\n",
            "jousting, archery, and even a bit of sword fighting. I doth hope thou hast seen\n",
            "the tournament, good fellow?\n",
            "\n",
            "Now, tell me, what brings thee to our fair city? Art thou a merchant, a\n",
            "traveler, or perhaps a noble seeking an audience with the king?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H1rQ9W8ZU7It"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
