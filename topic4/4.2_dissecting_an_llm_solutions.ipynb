{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic4/4.2_dissecting_an_llm_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "The course is in development now, with more materials coming soon."
      ],
      "metadata": {
        "id": "qQh7ewhdqZFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2. Dissecting an LLM"
      ],
      "metadata": {
        "id": "N_ikBbEuG91-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice task solutions"
      ],
      "metadata": {
        "id": "6Nz0HdNShxi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1. Assessing the artchitecture of **gemma-3-1b-it**\n",
        "\n",
        "In this task, you'll need to study the architecture of [**gemma-3-1b-it**](https://huggingface.co/google/gemma-3-1b-it) by Google:\n",
        "\n",
        "- How many layers, attention heads, and query heads per key/value head does it have?\n",
        "- What is the internal structure of the FFN block?\n",
        "- How many parameters does the LLM have in self-attention vs in FFN blocks?\n",
        "- Which type of attention masking (multiplicative or additive) does it use?"
      ],
      "metadata": {
        "id": "IW0y5qBtPVBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2. Key-value cache\n",
        "\n",
        "Each time an LLM generates a new token $y_t$, it needs to calculate, at each self-attention layer,\n",
        "\n",
        "$$(q_tk_1^T)v_1 + (q_tk_2^T)v_2 \\ldots + (q_tk_2^T)v_2$$\n",
        "\n",
        "and for that, you need all the keys $k_i = x_iW_K$ and values $v_i = x_iW_V$, where $x_i$ are inputs for the self-attention layer. Theoretically, this would require recalculating all the $k_i$ and $v_i$ *for every newly generated token* which is a terrible waste of compute.\n",
        "\n",
        "So, in most situations **keys and values are cached**, and the data structure in which they are stored is known as **KV-cache**.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1X4kCfcpAXNAcHrVsGVpsOk6roHL0AOnT\" width=600 />\n",
        "</center>\n",
        "\n",
        "Of course, caching has its own downsides: the LLM's appetites for additional memory increase linearly as the sequence length grows."
      ],
      "metadata": {
        "id": "HBgznOq1hzDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Two stages of LLM inference\n",
        "\n",
        "Introduction of KV-cache makes more clear the necessity of distinguising between the following two stages of LLM inference:\n",
        "\n",
        "1. At the **Cache pre-fill** stage, the prompt is processed and the KV-cache for prompt tokens is populated.\n",
        "2. At the **Autoregressive generation** stage, new tokens are generated, one by one."
      ],
      "metadata": {
        "id": "1jvbVEnEOLU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your task\n",
        "\n",
        "In this task, you'll need to update the implementation of Scaled dot product attention layer given below, adding KN-cache support to it. Make sure that both key and value vectors ($k = xW_K$ and $v = xW_V$) are only calculated once for each token.\n",
        "\n",
        "Assume that during LLM inference, the transformer's `forward` method is called by an external `generate` function, which\n",
        "\n",
        "* At the **cache pre-fill stage** runs `forward` on the whole prompt\n",
        "* At the **autoregressive generation stage** only sends the current sequence's last token to `forward`.\n",
        "\n",
        "You can have in mind the following generation process:\n",
        "\n",
        "```python\n",
        "def generate(model, prompt_tokens, max_new_tokens=50, eos_token_id=None):\n",
        "    \"\"\"\n",
        "    Minimalistic generation showing prefill and generation phases.\n",
        "    \n",
        "    Args:\n",
        "        model: Transformer model with KV-cache support\n",
        "        prompt_tokens: Input prompt tensor (batch_size, prompt_len, d_model)\n",
        "        max_new_tokens: Maximum number of tokens to generate\n",
        "        eos_token_id: End-of-sequence token ID\n",
        "    \"\"\"\n",
        "    # Phase 1: Cache pre-fill - processes entire prompt at once\n",
        "    model.clear_cache()\n",
        "    output = model(prompt_tokens, is_causal=True, use_cache=True)\n",
        "    \n",
        "    # Get last token's output as starting point for generation\n",
        "    next_token_logits = output[:, -1, :]  # (batch_size, d_model)\n",
        "    \n",
        "    generated_tokens = []\n",
        "    \n",
        "    # Phase 2: Autoregressive generation - processes one token at a time\n",
        "    for i in range(max_new_tokens):\n",
        "        # Sample next token (simplified - just using argmax)\n",
        "        next_token_id = next_token_logits.argmax(dim=-1, keepdim=True)\n",
        "        \n",
        "        # Check for EOS\n",
        "        # In this implementation, batch generation stops when the longest continuation in a batch is generated\n",
        "        if eos_token_id is not None and (next_token_id == eos_token_id).all():\n",
        "            break\n",
        "        \n",
        "        # Process single new token using cached K,V\n",
        "        output = model(next_token_id, use_cache=True)\n",
        "        \n",
        "        # Get logits for next token prediction\n",
        "        next_token_logits = output[:, -1, :]\n",
        "        \n",
        "        generated_tokens.append(next_token_id)\n",
        "    \n",
        "    return torch.cat(generated_tokens, dim=1)  # (batch_size, num_generated)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dC7rANHY7iLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention without KV-cache.\n",
        "\n",
        "    Based on PyTorch's scaled_dot_product_attention implementation,\n",
        "    wrapped in a class with linear projections for Q, K, V.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, dropout_p=0.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: Dimension of the model (must be divisible by n_heads)\n",
        "            n_heads: Number of attention heads\n",
        "            dropout_p: Dropout probability for attention weights\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads  # Dimension per head\n",
        "        self.dropout_p = dropout_p\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def _scaled_dot_product_attention(self, query, key, value, attn_mask=None,\n",
        "                                      is_causal=False, scale=None):\n",
        "        \"\"\"\n",
        "        Compute scaled dot-product attention.\n",
        "        Based on PyTorch's implementation.\n",
        "\n",
        "        Args:\n",
        "            query: (batch_size, n_heads, L, d_k)\n",
        "            key: (batch_size, n_heads, S, d_k)\n",
        "            value: (batch_size, n_heads, S, d_k)\n",
        "            attn_mask: Optional mask (batch_size, 1, L, S) or (1, 1, L, S)\n",
        "            is_causal: If True, applies causal masking\n",
        "            scale: Optional scaling factor (defaults to 1/sqrt(d_k))\n",
        "\n",
        "        Returns:\n",
        "            attention_output: (batch_size, n_heads, L, d_k)\n",
        "            attention_weights: (batch_size, n_heads, L, S)\n",
        "        \"\"\"\n",
        "        L, S = query.size(-2), key.size(-2)\n",
        "        scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
        "\n",
        "        # Initialize attention bias\n",
        "        attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n",
        "\n",
        "        # Apply causal mask if requested\n",
        "        if is_causal:\n",
        "            temp_mask = torch.ones(L, S, dtype=torch.bool, device=query.device).tril(diagonal=0)\n",
        "            attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
        "            attn_bias = attn_bias.to(query.dtype)\n",
        "\n",
        "        # Apply custom attention mask if provided\n",
        "        if attn_mask is not None:\n",
        "            if attn_mask.dtype == torch.bool:\n",
        "                attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
        "            else:\n",
        "                attn_bias = attn_mask + attn_bias\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
        "        attn_weight += attn_bias\n",
        "        attn_weight = torch.softmax(attn_weight, dim=-1)\n",
        "\n",
        "        # Apply dropout if in training mode\n",
        "        if self.dropout_p > 0 and self.training:\n",
        "            attn_weight = F.dropout(attn_weight, p=self.dropout_p)\n",
        "\n",
        "        # Compute attention output\n",
        "        attention_output = attn_weight @ value\n",
        "\n",
        "        return attention_output, attn_weight\n",
        "\n",
        "    def forward(self, x, attn_mask=None, is_causal=False):\n",
        "        \"\"\"\n",
        "        Forward pass for scaled dot-product attention.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "            attn_mask: Optional attention mask. Can be:\n",
        "                       - Boolean tensor where True = attend, False = mask\n",
        "                       - Float tensor with additive attention scores\n",
        "                       Shape: (batch_size, 1, seq_len, seq_len) or (1, 1, seq_len, seq_len)\n",
        "            is_causal: If True, applies causal masking (attn_mask must be None)\n",
        "\n",
        "        Returns:\n",
        "            output: Attention output of shape (batch_size, seq_len, d_model)\n",
        "            attention_weights: Attention weights of shape (batch_size, n_heads, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Linear transformations\n",
        "        Q = self.W_q(x)  # (batch_size, seq_len, d_model)\n",
        "        K = self.W_k(x)  # (batch_size, seq_len, d_model)\n",
        "        V = self.W_v(x)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        # Shape: (batch_size, n_heads, seq_len, d_k)\n",
        "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        context, attention_weights = self._scaled_dot_product_attention(\n",
        "            Q, K, V, attn_mask=attn_mask, is_causal=is_causal\n",
        "        )\n",
        "\n",
        "        # Concatenate heads and apply output projection\n",
        "        # First transpose: (batch_size, seq_len, n_heads, d_k)\n",
        "        # Then reshape: (batch_size, seq_len, d_model)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.W_o(context)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "Bfyz0_2MRQvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the following code to test your implementation:"
      ],
      "metadata": {
        "id": "ZBIpvzmFoet-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.testing\n",
        "import time\n",
        "\n",
        "\n",
        "def test_kv_cache_correctness(attention_module, d_model=512, n_heads=8, seq_len=20, batch_size=2):\n",
        "    \"\"\"Test that cached and non-cached modes produce identical results.\"\"\"\n",
        "    print(\"\\n=== Test 1: Correctness - Cached vs Non-cached ===\")\n",
        "\n",
        "    # Create input sequence\n",
        "    x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "    # Method 1: Process entire sequence without cache\n",
        "    attention_module.clear_cache()\n",
        "    output_no_cache, weights_no_cache = attention_module(x, is_causal=True, use_cache=False)\n",
        "\n",
        "    # Method 2: Process sequence token-by-token with cache\n",
        "    attention_module.clear_cache()\n",
        "    outputs_cached = []\n",
        "\n",
        "    for i in range(seq_len):\n",
        "        token = x[:, i:i+1, :]\n",
        "        output, _ = attention_module(token, use_cache=True)\n",
        "        outputs_cached.append(output)\n",
        "\n",
        "    output_cached = torch.cat(outputs_cached, dim=1)\n",
        "\n",
        "    # Compare\n",
        "    torch.testing.assert_close(output_no_cache, output_cached, rtol=1e-5, atol=1e-5)\n",
        "    print(\"Outputs match for implementations without and with cache!\")\n",
        "    print(f\"  Max difference: {(output_no_cache - output_cached).abs().max().item():.2e}\")\n",
        "\n",
        "\n",
        "def test_kv_cache_incremental(attention_module, d_model=512, n_heads=8, batch_size=2):\n",
        "    \"\"\"Test incremental cache building.\"\"\"\n",
        "    print(\"\\n=== Test 2: Incremental Cache Building ===\")\n",
        "\n",
        "    attention_module.clear_cache()\n",
        "    cache_sizes = []\n",
        "\n",
        "    # Add tokens one by one\n",
        "    for i in range(10):\n",
        "        token = torch.randn(batch_size, 1, d_model)\n",
        "        output, _ = attention_module(token, use_cache=True)\n",
        "        cache_size = attention_module.get_cache_size()\n",
        "        cache_sizes.append(cache_size)\n",
        "        print(f\"  Step {i+1}: Added 1 token, cache size = {cache_size}\")\n",
        "\n",
        "    # Verify cache grows linearly\n",
        "    expected_sizes = list(range(1, 11))\n",
        "    assert cache_sizes == expected_sizes, f\"Cache sizes {cache_sizes} != expected {expected_sizes}\"\n",
        "    print(\"Cache grows linearly, as expected!\")\n",
        "\n",
        "\n",
        "def test_prefill_vs_generation(attention_module, d_model=512, n_heads=8, batch_size=2):\n",
        "    \"\"\"Test prefill phase vs generation phase behavior.\"\"\"\n",
        "    print(\"\\n=== Test 3: Prefill vs Generation Phases ===\")\n",
        "\n",
        "    # Prefill: Process 10 tokens at once\n",
        "    attention_module.clear_cache()\n",
        "    prompt = torch.randn(batch_size, 10, d_model)\n",
        "    output_prefill, _ = attention_module(prompt, is_causal=True, use_cache=True)\n",
        "    cache_after_prefill = attention_module.get_cache_size()\n",
        "    print(f\"  Prefill: Processed 10 tokens, cache size = {cache_after_prefill}\")\n",
        "\n",
        "    # Generation: Add 5 more tokens one by one\n",
        "    for i in range(5):\n",
        "        new_token = torch.randn(batch_size, 1, d_model)\n",
        "        output_gen, weights = attention_module(new_token, use_cache=True)\n",
        "        print(f\"  Generation step {i+1}: cache size = {attention_module.get_cache_size()}, \"\n",
        "              f\"attention weights shape = {weights.shape}\")\n",
        "\n",
        "    assert attention_module.get_cache_size() == 15, \"Cache should have 15 tokens total\"\n",
        "    print(\"Prefill and generation phases seem to work correctly!\")\n",
        "\n",
        "\n",
        "def test_attention_pattern(attention_module, d_model=512, n_heads=8, batch_size=1):\n",
        "    \"\"\"Visualize attention patterns to verify causality.\"\"\"\n",
        "    print(\"\\n=== Test 4: Attention Pattern Visualization ===\")\n",
        "\n",
        "    attention_module.clear_cache()\n",
        "    seq_len = 5\n",
        "    x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "    # Get attention weights\n",
        "    _, weights = attention_module(x, is_causal=True, use_cache=False)\n",
        "\n",
        "    # Show attention pattern for first head\n",
        "    attn_pattern = weights[0, 0, :, :].detach()\n",
        "\n",
        "    # Verify causality\n",
        "    for i in range(seq_len):\n",
        "        for j in range(i+1, seq_len):\n",
        "            assert attn_pattern[i, j] < 1e-5, f\"Token {i} shouldn't attend to future token {j}\"\n",
        "    print(\"Causal masking verified!\")\n",
        "\n",
        "\n",
        "def test_performance_speedup(AttentionClass, d_model=512, n_heads=8, batch_size=2):\n",
        "    \"\"\"Measure actual speedup from KV-cache.\"\"\"\n",
        "    print(\"\\n=== Test 5: Performance Speedup ===\")\n",
        "\n",
        "    # Create two instances to avoid cache interference\n",
        "    attn_no_cache = AttentionClass(d_model, n_heads)\n",
        "    attn_with_cache = AttentionClass(d_model, n_heads)\n",
        "\n",
        "    prompt_len = 100\n",
        "    gen_len = 50\n",
        "\n",
        "    # Warm up\n",
        "    x = torch.randn(batch_size, 10, d_model)\n",
        "    attn_no_cache(x)\n",
        "    attn_with_cache(x)\n",
        "\n",
        "    # Test without cache\n",
        "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
        "    start = time.time()\n",
        "\n",
        "    full_seq = torch.randn(batch_size, prompt_len + gen_len, d_model)\n",
        "    for i in range(prompt_len, prompt_len + gen_len):\n",
        "        seq_so_far = full_seq[:, :i+1, :]\n",
        "        _ = attn_no_cache(seq_so_far, is_causal=True, use_cache=False)\n",
        "\n",
        "    time_no_cache = time.time() - start\n",
        "\n",
        "    # Test with cache\n",
        "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
        "    start = time.time()\n",
        "\n",
        "    # Prefill\n",
        "    prompt = full_seq[:, :prompt_len, :]\n",
        "    _ = attn_with_cache(prompt, is_causal=True, use_cache=True)\n",
        "\n",
        "    # Generation\n",
        "    for i in range(gen_len):\n",
        "        new_token = full_seq[:, prompt_len + i:prompt_len + i + 1, :]\n",
        "        _ = attn_with_cache(new_token, use_cache=True)\n",
        "\n",
        "    time_with_cache = time.time() - start\n",
        "\n",
        "    speedup = time_no_cache / time_with_cache\n",
        "    print(f\"  Without cache: {time_no_cache:.3f}s\")\n",
        "    print(f\"  With cache: {time_with_cache:.3f}s\")\n",
        "    print(f\"  Speedup: {speedup:.1f}x\")\n",
        "\n",
        "\n",
        "def test_batch_consistency(attention_module, d_model=512, n_heads=8):\n",
        "    \"\"\"Test that different batch sizes work correctly.\"\"\"\n",
        "    print(\"\\n=== Test 6: Batch Size Handling ===\")\n",
        "\n",
        "    # Start with batch size 2\n",
        "    attention_module.clear_cache()\n",
        "    x1 = torch.randn(2, 5, d_model)\n",
        "    out1, _ = attention_module(x1, use_cache=True)\n",
        "\n",
        "    # Try to process with different batch size (should handle gracefully)\n",
        "    try:\n",
        "        x2 = torch.randn(3, 1, d_model)\n",
        "        out2, _ = attention_module(x2, use_cache=True)\n",
        "        print(\"Should not accept different batch sizes with existing cache\")\n",
        "    except (RuntimeError, AssertionError):\n",
        "        print(\"Correctly prevents batch size mismatch\")\n",
        "\n",
        "    # Clear and try again\n",
        "    attention_module.clear_cache()\n",
        "    out2, _ = attention_module(x2, use_cache=True)\n",
        "    print(\"Works with new batch size after clearing cache\")\n",
        "\n",
        "\n",
        "# Run all tests\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Testing KV-Cache Implementation\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create attention module\n",
        "    attention = ScaledDotProductAttentionWithCache(d_model=512, n_heads=8)\n",
        "    attention.eval()  # Disable dropout\n",
        "\n",
        "    # Run tests\n",
        "    test_kv_cache_correctness(attention)\n",
        "    test_kv_cache_incremental(attention)\n",
        "    test_prefill_vs_generation(attention)\n",
        "    test_attention_pattern(attention)\n",
        "    test_performance_speedup(ScaledDotProductAttentionWithCache)\n",
        "    test_batch_consistency(attention)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"All tests completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFS8l2J9oiiB",
        "outputId": "102edb87-e6ec-4592-fac7-a8b356424f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing KV-Cache Implementation\n",
            "==================================================\n",
            "\n",
            "=== Test 1: Correctness - Cached vs Non-cached ===\n",
            "Outputs match for implementations without and with cache!\n",
            "  Max difference: 5.66e-07\n",
            "\n",
            "=== Test 2: Incremental Cache Building ===\n",
            "  Step 1: Added 1 token, cache size = 1\n",
            "  Step 2: Added 1 token, cache size = 2\n",
            "  Step 3: Added 1 token, cache size = 3\n",
            "  Step 4: Added 1 token, cache size = 4\n",
            "  Step 5: Added 1 token, cache size = 5\n",
            "  Step 6: Added 1 token, cache size = 6\n",
            "  Step 7: Added 1 token, cache size = 7\n",
            "  Step 8: Added 1 token, cache size = 8\n",
            "  Step 9: Added 1 token, cache size = 9\n",
            "  Step 10: Added 1 token, cache size = 10\n",
            "Cache grows linearly, as expected!\n",
            "\n",
            "=== Test 3: Prefill vs Generation Phases ===\n",
            "  Prefill: Processed 10 tokens, cache size = 10\n",
            "  Generation step 1: cache size = 11, attention weights shape = torch.Size([2, 8, 1, 11])\n",
            "  Generation step 2: cache size = 12, attention weights shape = torch.Size([2, 8, 1, 12])\n",
            "  Generation step 3: cache size = 13, attention weights shape = torch.Size([2, 8, 1, 13])\n",
            "  Generation step 4: cache size = 14, attention weights shape = torch.Size([2, 8, 1, 14])\n",
            "  Generation step 5: cache size = 15, attention weights shape = torch.Size([2, 8, 1, 15])\n",
            "Prefill and generation phases seem to work correctly!\n",
            "\n",
            "=== Test 4: Attention Pattern Visualization ===\n",
            "Causal masking verified!\n",
            "\n",
            "=== Test 5: Performance Speedup ===\n",
            "  Without cache: 0.199s\n",
            "  With cache: 0.032s\n",
            "  Speedup: 6.2x\n",
            "\n",
            "=== Test 6: Batch Size Handling ===\n",
            "Correctly prevents batch size mismatch\n",
            "Works with new batch size after clearing cache\n",
            "\n",
            "==================================================\n",
            "All tests completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution**"
      ],
      "metadata": {
        "id": "LFwut8aNVYNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ScaledDotProductAttentionWithCache(nn.Module):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention with KV-cache support.\n",
        "\n",
        "    Supports two phases:\n",
        "    1. Prefill: Process multiple tokens at once (e.g., prompt processing)\n",
        "    2. Generation: Process one token at a time, using cached K,V from previous tokens\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, dropout_p=0.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: Dimension of the model (must be divisible by n_heads)\n",
        "            n_heads: Number of attention heads\n",
        "            dropout_p: Dropout probability for attention weights\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "        self.dropout_p = dropout_p\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        # KV-cache: stores past keys and values\n",
        "        self.cache_k = None\n",
        "        self.cache_v = None\n",
        "\n",
        "    def _scaled_dot_product_attention(self, query, key, value, attn_mask=None,\n",
        "                                      is_causal=False, scale=None):\n",
        "        \"\"\"\n",
        "        Compute scaled dot-product attention (unchanged from base implementation).\n",
        "        \"\"\"\n",
        "        L, S = query.size(-2), key.size(-2)\n",
        "        scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
        "\n",
        "        # Initialize attention bias\n",
        "        attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n",
        "\n",
        "        # Apply causal mask if requested\n",
        "        if is_causal:\n",
        "            temp_mask = torch.ones(L, S, dtype=torch.bool, device=query.device).tril(diagonal=0)\n",
        "            attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
        "            attn_bias = attn_bias.to(query.dtype)\n",
        "\n",
        "        # Apply custom attention mask if provided\n",
        "        if attn_mask is not None:\n",
        "            if attn_mask.dtype == torch.bool:\n",
        "                attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
        "            else:\n",
        "                attn_bias = attn_mask + attn_bias\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
        "        attn_weight += attn_bias\n",
        "        attn_weight = torch.softmax(attn_weight, dim=-1)\n",
        "\n",
        "        # Apply dropout if in training mode\n",
        "        if self.dropout_p > 0 and self.training:\n",
        "            attn_weight = F.dropout(attn_weight, p=self.dropout_p)\n",
        "\n",
        "        # Compute attention output\n",
        "        attention_output = attn_weight @ value\n",
        "\n",
        "        return attention_output, attn_weight\n",
        "\n",
        "    def forward(self, x, attn_mask=None, is_causal=False, use_cache=False, cache_position=None):\n",
        "        \"\"\"\n",
        "        Forward pass with KV-cache support.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
        "               - During prefill: seq_len can be > 1 (processing prompt)\n",
        "               - During generation: seq_len = 1 (processing one new token)\n",
        "            attn_mask: Optional attention mask\n",
        "            is_causal: If True, applies causal masking\n",
        "            use_cache: If True, uses and updates KV-cache\n",
        "            cache_position: Optional tensor indicating position in sequence for each token\n",
        "                          Shape: (seq_len,) - useful for handling dynamic lengths\n",
        "\n",
        "        Returns:\n",
        "            output: Attention output of shape (batch_size, seq_len, d_model)\n",
        "            attention_weights: Attention weights of shape (batch_size, n_heads, seq_len, total_seq_len)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Compute Q, K, V projections\n",
        "        Q = self.W_q(x)  # Always compute Q for current tokens\n",
        "        K = self.W_k(x)  # Compute K for current tokens\n",
        "        V = self.W_v(x)  # Compute V for current tokens\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        if use_cache:\n",
        "            # Phase detection (prefill vs generation) based on cache state and sequence length\n",
        "            if self.cache_k is None:\n",
        "                # Initialize cache with current K, V\n",
        "                self.cache_k = K\n",
        "                self.cache_v = V\n",
        "            else:\n",
        "                # Concatenate new K, V with cached K, V\n",
        "                self.cache_k = torch.cat([self.cache_k, K], dim=-2)\n",
        "                self.cache_v = torch.cat([self.cache_v, V], dim=-2)\n",
        "\n",
        "            # Use full cached K, V for attention computation\n",
        "            K_for_attn = self.cache_k\n",
        "            V_for_attn = self.cache_v\n",
        "\n",
        "            # Update mask dimensions if needed\n",
        "            # Q has shape (batch, heads, seq_len, d_k)\n",
        "            # K_for_attn has shape (batch, heads, total_cached_len, d_k)\n",
        "            if is_causal and seq_len == 1:\n",
        "                # During generation, we have only one current token and no future tokens\n",
        "                # We attend to all previous cached positions\n",
        "                # No need to mask since we're looking at all past tokens\n",
        "                # Setting is_casual to false allows to save a bit of time we'd waste on masking\n",
        "                is_causal = False\n",
        "        else:\n",
        "            # No cache - use current K, V as-is\n",
        "            K_for_attn = K\n",
        "            V_for_attn = V\n",
        "\n",
        "        # Compute attention\n",
        "        context, attention_weights = self._scaled_dot_product_attention(\n",
        "            Q, K_for_attn, V_for_attn, attn_mask=attn_mask, is_causal=is_causal\n",
        "        )\n",
        "\n",
        "        # Reshape and project output\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.W_o(context)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"Clear the KV-cache.\"\"\"\n",
        "        self.cache_k = None\n",
        "        self.cache_v = None\n",
        "\n",
        "    def get_cache_size(self):\n",
        "        \"\"\"Get current cache size.\"\"\"\n",
        "        if self.cache_k is None:\n",
        "            return 0\n",
        "        return self.cache_k.size(-2)  # Return sequence length dimension"
      ],
      "metadata": {
        "id": "E3SNW9HoVZzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPQoejU1odmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3. Interpreting LLMs with LogitLens"
      ],
      "metadata": {
        "id": "e_OlobwBsPFo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HipjxMgiDZ4z"
      },
      "source": [
        "Logit Lens is an interpretation technique introduced in [this post](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens). The idea is the following. Imagine that we predict the continuation of a phrase \"IPhone was developed by\". We naturally expect to see \"Apple\", but we're also curious to see the \"thought process\" of an LLM, so we **feed outputs of intermediate layers (intermediate transformer blocks) to the classification head** to see *what would an LLM output if we cut its \"thought process\" short in the middle of it*. The general trend, as one moves from earlier to later layers, is\n",
        "- \"nonsense / not interpretable\" (sometimes, in very early layers) -->\n",
        "  - \"shallow guesses\" (words that are the right part of speech / register / etc) -->\n",
        "- \"better guesses\" near the end.\n",
        "However, it's not always like that, of course.\n",
        "\n",
        "The author of the Logit Lens also created visualization tools and published a [jupyter notebook demo](https://colab.research.google.com/drive/1MjdfK2srcerLrAJDRaJQKO0sUiZ-hQtA?usp=sharing) with cool pictures, but in this task you'll need to reproduce the Logit Lens technique on your own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D23vGULEFQsU"
      },
      "source": [
        "**Your task**. Write a function\n",
        "\n",
        "```\n",
        "logit_lens(model, input_sentence, top_k)\n",
        "```\n",
        "\n",
        "that for each transformer block returns a dictionary\n",
        "\n",
        "```\n",
        "{\n",
        "    'top_tokens' : [\n",
        "        sorted list of top_k tokens,\n",
        "        from most probable to least probable,\n",
        "        according to the classification head\n",
        "        ],\n",
        "    'top_token_logits' : [logits of these tokens]\n",
        "}\n",
        "```\n",
        "\n",
        "You can either use Pytorch hooks or just `model(**encoded_input, output_hidden_states=True)`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtqnfCh2IYwv"
      },
      "source": [
        "Here is how it should work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493,
          "referenced_widgets": [
            "4f074ace0a1146969f75e51f7777ae1b",
            "55d19a2f973848e497a266a517268c09",
            "cfa6677f111e4be18532d7c7f73868b5",
            "afba256d30b34587a764fc46898b819b",
            "2af63387d94041bb9d7fd020607f8d82",
            "0e8879431e4243bbb6d300081a64ad05",
            "8bc87dd4af7343ed9b78e9cfaa2595a5",
            "43688cafea02450faa203347b564bc79",
            "fd31b88d281041d0b29c834d23791544",
            "1150a053cda54fbf9c3f4aad8907cb3c",
            "36f0214bafe04e05a45a95d6c38a49e9",
            "2edd3e554f1e4eed9c402485118f41f6",
            "b221a23e93564d0aa07e61f7b2e2bbb2",
            "8a34edd3dec3414989df5a41913a860d",
            "4d7a9893d331467cb61395d9d6501850",
            "acbc74a4210a49e185403c9cac74fe46",
            "30f2cc8842b448db981677e7740b6739",
            "3de50d27cf964b5c8eda7c0ea718d012",
            "cc69040e94c749a2928c0fc8e3531c71",
            "a25e8f4123314d51974f1d700afcf06f",
            "9636089d1b8541a491d15e233b648ac7",
            "db549ee160ea4a4cb3a854607f65f52a",
            "c89c3baf66a34cdf93f7dec2783f28f5",
            "1b01a94b3e4849d284d2c6eec4d5b0a7",
            "b78f8923aacd424ebc2047d595e16f58",
            "ebf4336578764e68a186783bbfa62d36",
            "3ee32bd88eca4878bfad5e52a8f6cb1d",
            "4742092f1c6043b99bd11a16d537076d",
            "9fa4d82465424f8a9c6a02c742b37461",
            "d3ab9987d1e940068fc86647f96acce8",
            "a2ba05d636624de4a16f4b1b5f6dc07d",
            "4f3cc23f0fa146ae9c1568708ff0e497",
            "09a90240e1254fa3928881547dc635e1",
            "f79dfa53501c4dc8868cdfba688f9c26",
            "9e02ca48d71d417ebba9f9220bfad7c8",
            "20008f4fb02044efa74e52866de741db",
            "a45721fdc38e4b9d94d29ba7390427b0",
            "a8e9fae0e6dd442ead0705a1c8510f66",
            "176063976ee04813aafe91977852659f",
            "6e6869ff32774679b936997e5c8381ec",
            "76206304fa3340bbb12791d055703ceb",
            "a7d4d17bb536450db3fc7098edb61260",
            "02e46e2d2c5948f2bfcdb28c3964d1ce",
            "1d37258dc64343cb8e0963c32688577e",
            "b532058be7544c73bac88239fb24e7c9",
            "587b0d31d0f84dbf9da06f60c8fa47cf",
            "d01072c4797d497eb05a151479bc2d87",
            "578750d5b69045f6847739654ad87fb1",
            "9df7178baeab4cf98b80997c4cad1a81",
            "0ec6e78b5fc0478b9de4efa9c05d7b4f",
            "1c05d6c2cfbc4b6098c21314c61ee1f7",
            "4068347fe43947379a35dbc00f4ad3cf",
            "d4ce0bf56ec3426e8a7b7c8b37784beb",
            "57f1cfb1447941538736db95e6fc50cc",
            "5bdd22c45c6c4686933fe34cf193285b",
            "0a267d5cdc8a4bd39241156d7867cded",
            "a5362025d04a41a991c6dc4efa437919",
            "f6d15a82e4b64f739d828ee0c5bc141a",
            "39f48bc35e3a4cee95d397ac571e8842",
            "f21d83d20827435096327c3376ef4484",
            "69795212305b457fa7028884eaef6c8c",
            "22933a7daf584e25a465af6076972a3a",
            "9d492e66e5f840ecba1d5b953f9713a3",
            "6cb0ba6c65b0491697a9cb9072eda8b4",
            "ddf55132069e4aaeb5cfba2724755303",
            "ed90882c98b2439fba868cd726bd56ef",
            "8f04fad1d50d49879d190ff602166124",
            "835125d0b72c45b0a2c35755a2f1fa86",
            "51e88375a51b4f4ca2162745f10690cd",
            "03f599bf4abd45a88168db7471d11bfa",
            "900fd95f84ae4760998395c18ccf3d9c",
            "893d7fd6711d428180f2d1df6004c82d",
            "78dcc93dd1324e42a0643f58fab7c3ed",
            "e05bd428e4d84083822f1998f5dd047b",
            "d68ccf1c86cc4c9bb0831f9c4ad62478",
            "20dc7a4ef5b44672af77677b0d646bc6",
            "f442d67cb66d4aba89818137e8b3b9ed",
            "e9bb7257bd584694aa80dec76750d64d",
            "a268b06a2b924bc2bbb5f4a3cab9a15c",
            "c599487670594c3096f21e0b2de9ba94",
            "1c5d8e79c268414086cbcddd6bca62d0",
            "972b49b8320d413eb097bc8a8c86766c",
            "4f88e5684ed84d449040189e209b968e",
            "ff6f1d09b4cc48178f97134d456aea3d",
            "9112a90b0abf483da486383272734576",
            "7a46daafd6a547cb9183e4fc94b3e868",
            "b4232ccb96f444108fb52a7ddb4216cb",
            "0c816cbd1ea34e2da314a2f1495b48af",
            "03c034f86ea049519f629304e4aaf7db",
            "f13d87895f9a4f1b8adbee078550d510",
            "55942eaa0bf54993a9fd0c2aaa200108",
            "3513babcc5a7461ea3b81759d586055f",
            "3020d45ee3a246079426b67762fb2ab1",
            "8e6efa65d6fc405897c7c9ce4bc769ab",
            "96eb8467381540bb9b357da9e442fdc2",
            "7b0ffac973e2415f9be7d7f095994134",
            "7eb8e277e61943b29becbb3ce6b160ff",
            "783667b755484fabbf26a88ee7b6d23c",
            "03d454eb6b7b4f0893220d303d40ce8e",
            "1ac572540ee14449855ac2bdcd8d9f31",
            "981a238681a9452ba81edbd342eb77ce",
            "c029f63b4f1c4a92ba33f08cfe6c952d",
            "77d06c1209a34c5c84acd5652d14cff9",
            "5266bec744bf4934a34c7096aa830529",
            "1ee38adc613144a08ff9d8c398d146d6",
            "5dfbdcb5a09f4310a36667f7e47401db",
            "b8d9ff3d9c98488a8a6f03c60528b0dc",
            "1f2c6767626d47faa3834c9129fd8d00",
            "f9e0dd40c38d44e5ab72429e22c77d48",
            "7ae02fcf8fd74d57bae97218470fa96b",
            "2f1bc40076194fca9f8947f5357bb1df",
            "a62bb14cfd134f73a70a6efaa1d9996c",
            "a36da828b2924168b28f653e9e762cff",
            "746a4b6d628c4ef1b9b5feb1274d8c35",
            "ecfc60f34fb94c2ebc9551856f66c813",
            "37ddfad4f5d944d39b266f93480f5a33",
            "0412667500b14122b4f8da7da4e346e4",
            "868fa7aa2440407c8210bd3d5fbb687d",
            "9e549162234841708cec063982b4e41d",
            "a2f8db418a74474eb6c2c641f44a3b18",
            "827dbfdd08a746bea95eed8c7b7bfd61"
          ]
        },
        "id": "JafFNnijBQ4H",
        "outputId": "7373faee-4e9f-46d8-97d1-c0bbdafe7877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f074ace0a1146969f75e51f7777ae1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2edd3e554f1e4eed9c402485118f41f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c89c3baf66a34cdf93f7dec2783f28f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f79dfa53501c4dc8868cdfba688f9c26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b532058be7544c73bac88239fb24e7c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/35.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a267d5cdc8a4bd39241156d7867cded"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f04fad1d50d49879d190ff602166124"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9bb7257bd584694aa80dec76750d64d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03c034f86ea049519f629304e4aaf7db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ac572540ee14449855ac2bdcd8d9f31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f1bc40076194fca9f8947f5357bb1df"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here comes the logit lens:"
      ],
      "metadata": {
        "id": "CGGw7XvDbjpX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-15EHRPGvFN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def logit_lens(model, input_sentence, top_k=5):\n",
        "    # Tokenize the inputs: this turns text into a list of token indices\n",
        "    input_ids = tokenizer(input_sentence, return_tensors='pt').to(device)\n",
        "\n",
        "    # This runs the forward pass, calculating all the hidden states thanks to the output_hidden_states=True\n",
        "    model_output = model(**input_ids,  output_hidden_states=True)\n",
        "    hidden_states = model_output.hidden_states\n",
        "\n",
        "    # This is the LLM's final layer: it takes transformer's output (one vector)\n",
        "    # and for each possible token on the vocabulary predicts its probability\n",
        "    model_head = model.lm_head\n",
        "\n",
        "    result = []\n",
        "\n",
        "    for layer in hidden_states:\n",
        "\n",
        "        \"\"\"\n",
        "        YOUR TASK HERE IS AS FOLLOWS:\n",
        "\n",
        "        The layer's output consists of a number of hidden state vectors: one vector for each input token\n",
        "        You need to:\n",
        "        1. take the last of the outputs and to apply the lm_head to it\n",
        "        2. find k tokens with top probabilities. The function torch.topk might help\n",
        "            Just check what torch.topk outputs\n",
        "        3. return:\n",
        "        output = {\n",
        "            \"top_tokens\": the array of top tokens. Don't forget to decode them with tokenizer.decode\n",
        "            \"top_token_logits\": the array of their logits. Don't forget to detach the tensor and to convert it to numpy!\n",
        "        }\n",
        "        Note that we need to decode\n",
        "        \"\"\"\n",
        "        # <YOUR CODE HERE>\n",
        "        result.append(output)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0-j00fhH4wP"
      },
      "outputs": [],
      "source": [
        "result = logit_lens(model, \"IPhone was developed by\", top_k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8D5graFHVZg",
        "outputId": "2ca36412-993f-4d56-a115-716464150a10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'top_tokens': [' ', ',', ' a', ' (', '\\n'],\n",
              "  'top_token_logits': array([77.5 , 66.  , 64.5 , 60.  , 58.75], dtype=float32)},\n",
              " {'top_tokens': [' Microsoft', ' Apple', ' a', ' the', ' an'],\n",
              "  'top_token_logits': array([18.25 , 18.125, 16.75 , 16.5  , 14.875], dtype=float32)}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "result[-2:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rwbGrocR2CG"
      },
      "source": [
        "As you see, \"Apple\" appears as the most probable token in the last two layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AkMOVa4SQDv"
      },
      "source": [
        "Now, let's use Logit Lens to investigate how transformers deal with redefinition.\n",
        "\n",
        "We'll use Logit Lens on the sentence\n",
        "\n",
        "```\n",
        "\"In this text the word IPhone means Windows operating system. IPhone was developed by\"\n",
        "```\n",
        "\n",
        "Run the following cell and look at the most probable tokens for all layers. A good LLM knows that IPhone was developed by Apple through *memorization*. However, *in-context learning* will press it to output Microsoft. Check in which layers the most probable token is Microsoft and in which it is Apple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zjPzevDiJn5"
      },
      "outputs": [],
      "source": [
        "result = logit_lens(model, \"In this text the word IPhone means Windows operating system. IPhone was developed by\", top_k=5)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcno5Jfy83HJ"
      },
      "source": [
        "Try more prompts and other models. Have fun trying to interpret whatever is inside LLMs :)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution**\n",
        "\n"
      ],
      "metadata": {
        "id": "nzOtaXHabt5A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1zned2Kbuq-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def logit_lens(model, input_sentence, top_k=5):\n",
        "    input_ids = tokenizer(input_sentence, return_tensors='pt').to(device=model.device)\n",
        "    model_output = model(**input_ids,  output_hidden_states=True)\n",
        "    hidden_states = model_output.hidden_states\n",
        "    model_head = model.lm_head\n",
        "    result = []\n",
        "    for layer in hidden_states:\n",
        "        last_token_layer = layer[0][-1]\n",
        "        layer_logits = model_head(last_token_layer)\n",
        "        top_values = torch.topk(layer_logits,k=top_k)\n",
        "        indices = top_values.indices\n",
        "        values = top_values.values\n",
        "        output = {\n",
        "            \"top_tokens\": [tokenizer.decode(ind) for ind in indices],\n",
        "            \"top_token_logits\": values.to(torch.float32).detach().cpu().numpy()\n",
        "        }\n",
        "        result.append(output)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's run several experiments. In the Microsoft vs Apple example, Apple is switched to Microsoft only at the final level.\n",
        "\n",
        "By the way, notice the mysterious pre-final level. In this model, it frequently outputs spaces, commas etc as the most frequent tokens. That's curious."
      ],
      "metadata": {
        "id": "DQJpKgOTihFH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmG_o_iAbuq_"
      },
      "outputs": [],
      "source": [
        "result = logit_lens(model, \"In this text the word IPhone means Windows operating system. IPhone was developed by\", top_k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f47cdf-f86e-46c8-f18a-a04f22866d9d",
        "id": "W4m2UpTSburA"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'top_tokens': [' by', 'by', ' By', 'By', '_by'],\n",
              "  'top_token_logits': array([0.96484375, 0.7109375 , 0.5703125 , 0.55078125, 0.546875  ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [' virtue', '-products', ' dint', ' leaps', 'means'],\n",
              "  'top_token_logits': array([4.      , 3.4375  , 2.953125, 2.59375 , 2.578125], dtype=float32)},\n",
              " {'top_tokens': [' virtue', '-products', ' dint', 'onc', ' leaps'],\n",
              "  'top_token_logits': array([3.78125 , 3.3125  , 2.9375  , 2.734375, 2.640625], dtype=float32)},\n",
              " {'top_tokens': [' virtue', '-products', ' dint', '/by', ''],\n",
              "  'top_token_logits': array([4.      , 3.296875, 3.015625, 2.796875, 2.78125 ], dtype=float32)},\n",
              " {'top_tokens': [' virtue', '-products', '/by', ' dint', 'rne'],\n",
              "  'top_token_logits': array([4.125   , 3.40625 , 3.140625, 2.984375, 2.8125  ], dtype=float32)},\n",
              " {'top_tokens': [' virtue', ' dint', '-products', '/by', 'rne'],\n",
              "  'top_token_logits': array([4.3125  , 3.40625 , 3.328125, 3.078125, 3.078125], dtype=float32)},\n",
              " {'top_tokens': [' virtue', '-products', '', ' dint', 'products'],\n",
              "  'top_token_logits': array([4.40625 , 3.671875, 3.484375, 3.28125 , 3.25    ], dtype=float32)},\n",
              " {'top_tokens': [' virtue', '-products', '', 'products', ' either'],\n",
              "  'top_token_logits': array([4.25    , 3.828125, 3.390625, 3.34375 , 3.28125 ], dtype=float32)},\n",
              " {'top_tokens': [' virtue', '-products', ' means', ' either', ''],\n",
              "  'top_token_logits': array([5.      , 3.625   , 3.59375 , 3.578125, 3.53125 ], dtype=float32)},\n",
              " {'top_tokens': [' virtue', '-products', ' either', 'products', 'tools'],\n",
              "  'top_token_logits': array([5.      , 4.34375 , 4.34375 , 4.      , 3.765625], dtype=float32)},\n",
              " {'top_tokens': [' virtue', ' either', '-products', '__', '*'],\n",
              "  'top_token_logits': array([5.25   , 5.1875 , 4.3125 , 4.3125 , 4.03125], dtype=float32)},\n",
              " {'top_tokens': ['products', '-products', ' either', 'product', 'UILT'],\n",
              "  'top_token_logits': array([5.34375, 5.3125 , 5.     , 4.8125 , 4.59375], dtype=float32)},\n",
              " {'top_tokens': ['products', 'design', '-products', 'product', ' either'],\n",
              "  'top_token_logits': array([5.96875, 5.75   , 5.71875, 5.125  , 5.03125], dtype=float32)},\n",
              " {'top_tokens': ['products', '-products', 'design', 'ithe', 'IN'],\n",
              "  'top_token_logits': array([5.65625, 5.53125, 5.375  , 5.1875 , 5.03125], dtype=float32)},\n",
              " {'top_tokens': ['products', '-products', 'ithe', ' ', 'product'],\n",
              "  'top_token_logits': array([6.53125, 6.46875, 5.96875, 5.9375 , 5.71875], dtype=float32)},\n",
              " {'top_tokens': ['products', ' ', 'product', '-products', 'cbd'],\n",
              "  'top_token_logits': array([6.78125, 6.4375 , 6.3125 , 6.09375, 6.     ], dtype=float32)},\n",
              " {'top_tokens': ['products', 'product', ' ', 'ithe', '-products'],\n",
              "  'top_token_logits': array([7.28125, 6.65625, 6.15625, 6.125  , 6.     ], dtype=float32)},\n",
              " {'top_tokens': ['products', ' ', 'product', '', 'cbd'],\n",
              "  'top_token_logits': array([6.96875, 6.875  , 6.6875 , 6.375  , 6.375  ], dtype=float32)},\n",
              " {'top_tokens': ['products', 'product', 'ithe', '-products', '()?'],\n",
              "  'top_token_logits': array([7.4375 , 6.90625, 6.375  , 6.34375, 6.09375], dtype=float32)},\n",
              " {'top_tokens': ['products', ' ', 'cbd', '()?', '-products'],\n",
              "  'top_token_logits': array([7.     , 6.78125, 6.65625, 6.46875, 6.46875], dtype=float32)},\n",
              " {'top_tokens': ['|M', '|h', 'ectl', 'products', ' '],\n",
              "  'top_token_logits': array([7.03125, 6.96875, 6.75   , 6.6875 , 6.5625 ], dtype=float32)},\n",
              " {'top_tokens': ['products', ' ', '|M', '\\tproduct', '-products'],\n",
              "  'top_token_logits': array([7.3125 , 7.15625, 7.0625 , 6.875  , 6.84375], dtype=float32)},\n",
              " {'top_tokens': ['|M', ' ', '__', '', ')((('],\n",
              "  'top_token_logits': array([8.25   , 8.     , 7.625  , 7.59375, 7.5625 ], dtype=float32)},\n",
              " {'top_tokens': [' ', '|M', ')(((', '__,__', 'products'],\n",
              "  'top_token_logits': array([7.40625, 7.09375, 6.9375 , 6.84375, 6.78125], dtype=float32)},\n",
              " {'top_tokens': [' ', '', '__,__', '/by', '()?'],\n",
              "  'top_token_logits': array([8.5625 , 8.1875 , 7.59375, 7.53125, 7.375  ], dtype=float32)},\n",
              " {'top_tokens': [' ', '()?', '__,__', '', ')((('],\n",
              "  'top_token_logits': array([9.8125, 8.5   , 8.3125, 8.3125, 8.3125], dtype=float32)},\n",
              " {'top_tokens': [' ', ')(((', '()?', '.SizeType', '__,__'],\n",
              "  'top_token_logits': array([9.0625, 8.3125, 8.25  , 8.1875, 8.125 ], dtype=float32)},\n",
              " {'top_tokens': [' ', '.SizeType', '__', '', ''],\n",
              "  'top_token_logits': array([11.125 ,  9.9375,  9.625 ,  9.25  ,  9.25  ], dtype=float32)},\n",
              " {'top_tokens': [' ', '.createObject', '.SizeType', '', '...'],\n",
              "  'top_token_logits': array([11.9375, 11.625 , 11.1875, 11.125 , 10.9375], dtype=float32)},\n",
              " {'top_tokens': [' ', '...', '.SizeType', '', ','],\n",
              "  'top_token_logits': array([14.625 , 14.    , 13.6875, 13.5625, 13.5   ], dtype=float32)},\n",
              " {'top_tokens': [' ', ' ', '__', '.SizeType', ','],\n",
              "  'top_token_logits': array([16.5   , 16.125 , 15.875 , 15.6875, 15.4375], dtype=float32)},\n",
              " {'top_tokens': [' ', ',', '__', '...', ' '],\n",
              "  'top_token_logits': array([20.625, 20.25 , 19.75 , 18.875, 18.375], dtype=float32)},\n",
              " {'top_tokens': [' Apple', ' Microsoft', 'Microsoft', 'Apple', ' apple'],\n",
              "  'top_token_logits': array([41.25, 39.75, 35.5 , 35.  , 34.75], dtype=float32)},\n",
              " {'top_tokens': [' Apple', ' Microsoft', 'Apple', ' apple', 'Microsoft'],\n",
              "  'top_token_logits': array([52.75, 48.25, 44.75, 44.5 , 43.25], dtype=float32)},\n",
              " {'top_tokens': [' Apple', ' ', ' Microsoft', ' apple', 'Apple'],\n",
              "  'top_token_logits': array([60.  , 52.25, 50.25, 48.5 , 47.75], dtype=float32)},\n",
              " {'top_tokens': [' ', ',', ' a', ' (', '\\n'],\n",
              "  'top_token_logits': array([77.5 , 66.  , 64.5 , 60.  , 58.75], dtype=float32)},\n",
              " {'top_tokens': [' Microsoft', ' Apple', ' a', ' the', ' an'],\n",
              "  'top_token_logits': array([18.25 , 18.125, 16.75 , 16.5  , 14.875], dtype=float32)}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's try some arithmetics. A curious thing here is that '' is the Chinese character for 3."
      ],
      "metadata": {
        "id": "-80qGKWti8up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = logit_lens(model, \"12 + 23 = \", top_k=5)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7tBuLubgjcx",
        "outputId": "22f99d21-f2e3-42cc-8b83-ad95a3666744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'top_tokens': [' ', ',', '1', ' (', '.'],\n",
              "  'top_token_logits': array([1.6953125 , 0.96484375, 0.94140625, 0.93359375, 0.88671875],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': ['', '<![', 'irc', 'apt', 'ESC'],\n",
              "  'top_token_logits': array([1.6875   , 1.5      , 1.359375 , 1.3515625, 1.3359375],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': [',', '<![', 'm', '();)', ''],\n",
              "  'top_token_logits': array([2.34375 , 2.171875, 2.171875, 2.09375 , 2.0625  ], dtype=float32)},\n",
              " {'top_tokens': ['<![', ',', '__()', 'm', ''],\n",
              "  'top_token_logits': array([2.46875 , 2.4375  , 2.34375 , 2.34375 , 2.328125], dtype=float32)},\n",
              " {'top_tokens': [',', '', '\\').\"', '__()', \"')}\"],\n",
              "  'top_token_logits': array([2.671875, 2.375   , 2.375   , 2.328125, 2.328125], dtype=float32)},\n",
              " {'top_tokens': ['<![', 'TAIL', '', 'TTY', \"')}\"],\n",
              "  'top_token_logits': array([2.765625, 2.609375, 2.59375 , 2.59375 , 2.578125], dtype=float32)},\n",
              " {'top_tokens': [\"')}\", '******/', '(___', '', '__()'],\n",
              "  'top_token_logits': array([2.9375 , 2.875  , 2.84375, 2.75   , 2.71875], dtype=float32)},\n",
              " {'top_tokens': ['', \"')}\", '__;', '', '******/'],\n",
              "  'top_token_logits': array([3.8125  , 3.671875, 3.609375, 3.609375, 3.421875], dtype=float32)},\n",
              " {'top_tokens': ['', '', '__;', 'ityEngine', 'ssp'],\n",
              "  'top_token_logits': array([4.53125, 4.34375, 4.03125, 3.90625, 3.8125 ], dtype=float32)},\n",
              " {'top_tokens': ['', 'ITT', '', ' ...\\\\', '(Level'],\n",
              "  'top_token_logits': array([4.1875 , 4.15625, 4.125  , 4.125  , 4.125  ], dtype=float32)},\n",
              " {'top_tokens': ['', 'ITT', '', 'ffb', 'IFY'],\n",
              "  'top_token_logits': array([4.75   , 4.625  , 4.5625 , 4.21875, 4.1875 ], dtype=float32)},\n",
              " {'top_tokens': ['', '******/', '__;', 'ssp', 'ITT'],\n",
              "  'top_token_logits': array([5.25   , 4.34375, 4.1875 , 4.1875 , 4.15625], dtype=float32)},\n",
              " {'top_tokens': ['', '******/', 'ABCDE', 'AFX', ''],\n",
              "  'top_token_logits': array([5.71875, 4.75   , 4.625  , 4.5625 , 4.5625 ], dtype=float32)},\n",
              " {'top_tokens': ['', 'AFX', '******/', 'ABCDE', ''],\n",
              "  'top_token_logits': array([5.90625, 5.28125, 5.09375, 4.78125, 4.75   ], dtype=float32)},\n",
              " {'top_tokens': [\" ?',\", 'ffb', '', '', ''],\n",
              "  'top_token_logits': array([6.03125, 5.96875, 5.71875, 5.625  , 5.5    ], dtype=float32)},\n",
              " {'top_tokens': ['__;', '(___', '', '', \" ?',\"],\n",
              "  'top_token_logits': array([6.90625, 6.46875, 6.375  , 6.28125, 6.03125], dtype=float32)},\n",
              " {'top_tokens': ['__;', '(___', 'ABCDE', '', ''],\n",
              "  'top_token_logits': array([6.59375, 6.40625, 6.28125, 6.125  , 6.09375], dtype=float32)},\n",
              " {'top_tokens': ['(___', '__;', 'enville', '__()', ''],\n",
              "  'top_token_logits': array([7.71875, 7.59375, 7.40625, 7.0625 , 7.     ], dtype=float32)},\n",
              " {'top_tokens': ['edException', 'enville', '(___', '__()', '__;'],\n",
              "  'top_token_logits': array([6.59375, 6.53125, 6.5    , 6.46875, 6.375  ], dtype=float32)},\n",
              " {'top_tokens': ['', 'enville', '', 'ityEngine', ''],\n",
              "  'top_token_logits': array([7.5    , 7.375  , 7.28125, 7.     , 6.8125 ], dtype=float32)},\n",
              " {'top_tokens': ['enville', '', '', '', ' '],\n",
              "  'top_token_logits': array([7.875  , 7.59375, 7.59375, 7.40625, 7.40625], dtype=float32)},\n",
              " {'top_tokens': [' ', '', '', '', '<!['],\n",
              "  'top_token_logits': array([9.1875, 8.6875, 8.375 , 8.25  , 8.25  ], dtype=float32)},\n",
              " {'top_tokens': [' ', '', ',__', '<![', ''],\n",
              "  'top_token_logits': array([9.5625, 9.1875, 9.0625, 8.875 , 8.8125], dtype=float32)},\n",
              " {'top_tokens': ['', ',__', 'enville', '__;', ' '],\n",
              "  'top_token_logits': array([9.8125, 9.5   , 9.1875, 8.8125, 8.6875], dtype=float32)},\n",
              " {'top_tokens': ['__;', ',__', ' ', '', '()?'],\n",
              "  'top_token_logits': array([10.1875, 10.125 ,  9.9375,  9.5625,  9.0625], dtype=float32)},\n",
              " {'top_tokens': ['__;', ' ', ',__', '(___', ''],\n",
              "  'top_token_logits': array([10.4375, 10.125 ,  9.875 ,  9.0625,  9.    ], dtype=float32)},\n",
              " {'top_tokens': [' ', ',__', 'AFX', '', 'enville'],\n",
              "  'top_token_logits': array([11.6875, 10.4375,  9.875 ,  9.5   ,  9.4375], dtype=float32)},\n",
              " {'top_tokens': [' ', '__', '(+', ',__', ''],\n",
              "  'top_token_logits': array([13.1875, 11.75  , 11.3125, 11.25  , 11.125 ], dtype=float32)},\n",
              " {'top_tokens': ['', ' ', ' $__', ',__', ' __________________\\n\\n'],\n",
              "  'top_token_logits': array([14.625 , 13.375 , 12.875 , 12.8125, 12.1875], dtype=float32)},\n",
              " {'top_tokens': ['.SizeType', ' ', '', ' $__', ''],\n",
              "  'top_token_logits': array([16.25  , 16.125 , 14.8125, 14.6875, 14.625 ], dtype=float32)},\n",
              " {'top_tokens': ['.SizeType', '', ' $__', '3', ' '],\n",
              "  'top_token_logits': array([17.25 , 16.   , 16.   , 15.875, 15.5  ], dtype=float32)},\n",
              " {'top_tokens': ['.SizeType', '.createObject', '3', '(+', '\"h'],\n",
              "  'top_token_logits': array([20.   , 18.25 , 18.   , 17.625, 17.   ], dtype=float32)},\n",
              " {'top_tokens': ['3', '', '.SizeType', '', '(+'],\n",
              "  'top_token_logits': array([29.875, 28.75 , 24.5  , 22.25 , 21.25 ], dtype=float32)},\n",
              " {'top_tokens': ['', '3', '.SizeType', ' ', ' tr'],\n",
              "  'top_token_logits': array([35.5  , 35.25 , 30.875, 26.125, 24.5  ], dtype=float32)},\n",
              " {'top_tokens': ['3', '', '.SizeType', '1', ' '],\n",
              "  'top_token_logits': array([44.5  , 36.75 , 34.75 , 28.875, 27.5  ], dtype=float32)},\n",
              " {'top_tokens': [' ', '3', '1', '2', ' ('],\n",
              "  'top_token_logits': array([67.5 , 66.  , 58.5 , 50.25, 49.  ], dtype=float32)},\n",
              " {'top_tokens': ['3', '1', '2', '4', '5'],\n",
              "  'top_token_logits': array([21.25 , 18.375, 17.75 , 17.625, 16.875], dtype=float32)}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also check what's Qwen's favourite library to be imported after `numpy`. As you can see, `matplolib` and `pandas` only replace `random` at the top positions at the final layer."
      ],
      "metadata": {
        "id": "Y6-cyAOqjDrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = logit_lens(model, \"import numpy as np\\nimport \", top_k=5)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Xqafnc7hBdX",
        "outputId": "26b2b9fb-b933-45fb-ea6a-4961b85168ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'top_tokens': [' ', ',', '1', ' (', '.'],\n",
              "  'top_token_logits': array([1.6953125 , 0.96484375, 0.94140625, 0.93359375, 0.88671875],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': ['ESC', '', 'ici', 'once', ''],\n",
              "  'top_token_logits': array([1.6875   , 1.5234375, 1.484375 , 1.4609375, 1.4296875],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': ['ESC', '', 'rtc', '', ''],\n",
              "  'top_token_logits': array([2.234375 , 1.9765625, 1.9609375, 1.8828125, 1.828125 ],\n",
              "        dtype=float32)},\n",
              " {'top_tokens': ['ESC', 'Locator', '(`/', 'opens', 'ASI'],\n",
              "  'top_token_logits': array([2.90625 , 2.25    , 2.234375, 2.15625 , 2.140625], dtype=float32)},\n",
              " {'top_tokens': ['ESC', 'Locator', '(`/', 'SystemService', 'opens'],\n",
              "  'top_token_logits': array([2.859375, 2.734375, 2.578125, 2.515625, 2.515625], dtype=float32)},\n",
              " {'top_tokens': ['ibe', 'ESC', 'SystemService', 'roman', 'eso'],\n",
              "  'top_token_logits': array([3.3125  , 3.078125, 3.03125 , 2.90625 , 2.875   ], dtype=float32)},\n",
              " {'top_tokens': ['', '', '', '', '(call'],\n",
              "  'top_token_logits': array([4.90625, 4.90625, 4.78125, 4.65625, 4.59375], dtype=float32)},\n",
              " {'top_tokens': ['ucs', '_executor', \"']/\", ' Ranch', 'IBE'],\n",
              "  'top_token_logits': array([5.65625, 5.15625, 4.875  , 4.6875 , 4.65625], dtype=float32)},\n",
              " {'top_tokens': ['ucs', '', '_executor', ' ', ''],\n",
              "  'top_token_logits': array([5.5625 , 5.125  , 4.90625, 4.84375, 4.75   ], dtype=float32)},\n",
              " {'top_tokens': ['ucs', 'edException', 'Unlock', '(call', 'azes'],\n",
              "  'top_token_logits': array([6.34375, 5.21875, 5.09375, 5.0625 , 4.90625], dtype=float32)},\n",
              " {'top_tokens': ['ucs', 'edException', 'ths', ' dame', ''],\n",
              "  'top_token_logits': array([6.09375, 5.84375, 5.4375 , 5.34375, 5.25   ], dtype=float32)},\n",
              " {'top_tokens': ['', '', 'edException', '', 'onica'],\n",
              "  'top_token_logits': array([5.875  , 5.5625 , 5.53125, 5.375  , 5.34375], dtype=float32)},\n",
              " {'top_tokens': ['edException', '', 'opens', 'imes', 'udes'],\n",
              "  'top_token_logits': array([6.65625, 6.21875, 6.     , 5.8125 , 5.71875], dtype=float32)},\n",
              " {'top_tokens': ['edException', ';y', '', 'irt', 'ets'],\n",
              "  'top_token_logits': array([7.34375, 6.53125, 6.4375 , 5.8125 , 5.8125 ], dtype=float32)},\n",
              " {'top_tokens': ['edException', '', ';y', '', 'ione'],\n",
              "  'top_token_logits': array([7.9375 , 7.34375, 7.28125, 7.09375, 7.     ], dtype=float32)},\n",
              " {'top_tokens': ['edException', ';y', '', '', ':///'],\n",
              "  'top_token_logits': array([11.625  ,  9.125  ,  7.9375 ,  7.875  ,  7.71875], dtype=float32)},\n",
              " {'top_tokens': ['edException', ';y', '', 'edList', 'ValidationError'],\n",
              "  'top_token_logits': array([12.125 ,  9.5625,  8.6875,  8.625 ,  8.    ], dtype=float32)},\n",
              " {'top_tokens': ['edException', ';y', '.python', '', 'ndef'],\n",
              "  'top_token_logits': array([10.125  ,  8.5625 ,  7.96875,  7.9375 ,  7.84375], dtype=float32)},\n",
              " {'top_tokens': ['edException', ';y', '', '', 'ment'],\n",
              "  'top_token_logits': array([9.3125 , 8.5    , 7.96875, 7.78125, 7.5    ], dtype=float32)},\n",
              " {'top_tokens': ['edException', '', '', '', ''],\n",
              "  'top_token_logits': array([10.5625,  8.5   ,  8.4375,  8.25  ,  8.125 ], dtype=float32)},\n",
              " {'top_tokens': ['edException', ';y', '', '', ''],\n",
              "  'top_token_logits': array([10.875 ,  9.125 ,  8.8125,  8.3125,  8.0625], dtype=float32)},\n",
              " {'top_tokens': ['edException', '', ';y', '', ''],\n",
              "  'top_token_logits': array([12.25  ,  8.75  ,  8.75  ,  8.625 ,  8.4375], dtype=float32)},\n",
              " {'top_tokens': ['edException', ';y', '', '', ''],\n",
              "  'top_token_logits': array([12.    ,  9.    ,  8.8125,  8.8125,  8.5   ], dtype=float32)},\n",
              " {'top_tokens': ['edException',\n",
              "   '',\n",
              "   '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t',\n",
              "   '',\n",
              "   ';y'],\n",
              "  'top_token_logits': array([12.0625,  9.0625,  9.    ,  8.75  ,  8.75  ], dtype=float32)},\n",
              " {'top_tokens': ['edException',\n",
              "   ';y',\n",
              "   '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t',\n",
              "   '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t',\n",
              "   'ly'],\n",
              "  'top_token_logits': array([12.5625, 10.1875, 10.125 , 10.0625,  9.1875], dtype=float32)},\n",
              " {'top_tokens': ['edException',\n",
              "   '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t',\n",
              "   '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t',\n",
              "   '',\n",
              "   ''],\n",
              "  'top_token_logits': array([12.875 , 10.5625, 10.4375, 10.3125, 10.1875], dtype=float32)},\n",
              " {'top_tokens': ['edException',\n",
              "   '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t',\n",
              "   '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t',\n",
              "   '',\n",
              "   ' '],\n",
              "  'top_token_logits': array([13.3125, 10.8125, 10.625 , 10.25  , 10.0625], dtype=float32)},\n",
              " {'top_tokens': ['edException',\n",
              "   ' ',\n",
              "   '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t',\n",
              "   ' util',\n",
              "   '../../../'],\n",
              "  'top_token_logits': array([13.125, 12.75 , 12.125, 12.125, 11.875], dtype=float32)},\n",
              " {'top_tokens': ['edException',\n",
              "   ' util',\n",
              "   ' py',\n",
              "   '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t',\n",
              "   ' '],\n",
              "  'top_token_logits': array([15.0625, 13.5   , 13.25  , 12.9375, 12.9375], dtype=float32)},\n",
              " {'top_tokens': ['edException', ' matplotlib', ' random', ' ', ' numpy'],\n",
              "  'top_token_logits': array([16.125, 15.5  , 14.75 , 14.625, 14.375], dtype=float32)},\n",
              " {'top_tokens': [' random', '\\u200b', ' matplotlib', ' ', '__'],\n",
              "  'top_token_logits': array([17.5  , 16.625, 16.5  , 16.125, 16.125], dtype=float32)},\n",
              " {'top_tokens': ['\\u200b', ' ', ' random', ',', ' matplotlib'],\n",
              "  'top_token_logits': array([24.25 , 22.625, 22.125, 21.75 , 21.5  ], dtype=float32)},\n",
              " {'top_tokens': [' random', '\\u200b', ' ', ' matplotlib', ' math'],\n",
              "  'top_token_logits': array([28.125, 27.25 , 27.25 , 27.   , 26.625], dtype=float32)},\n",
              " {'top_tokens': [' random', '\\u200b', ' math', ' ', ' time'],\n",
              "  'top_token_logits': array([37. , 33.5, 33.5, 33.5, 33. ], dtype=float32)},\n",
              " {'top_tokens': [' random', ' ', ' time', '\\u200b', ' util'],\n",
              "  'top_token_logits': array([41.75, 39.75, 39.  , 38.25, 37.75], dtype=float32)},\n",
              " {'top_tokens': [' ', ',', ' time', ' random', ' m'],\n",
              "  'top_token_logits': array([54.5 , 48.  , 47.5 , 46.25, 45.75], dtype=float32)},\n",
              " {'top_tokens': [' matplotlib', ' pandas', ' cv', ' torch', ' math'],\n",
              "  'top_token_logits': array([19.  , 18.75, 17.75, 17.25, 17.  ], dtype=float32)}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4. Steering LLM generation via Activation Interventions\n",
        "\n",
        "Imagine you want to change the style of your LLM's output. We already know how to do this using clever prompting and few-shot examples - but there's another approach: directly modifying the model's internal activations.\n",
        "\n",
        "The method you'll explore in this task was, to the best of our knowledge, first introduced in [this post at LessWrong](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector). Conceptually, however, it draws inspiration from techniques used in image editing via latent space manipulation.\n",
        "\n",
        "The core idea is that some abstract concepts - such as a particular writing style - may be represented as directions in the latent spaces of one of the model's hidden layers. If you can identify the right layer and find the appropriate direction vector $d$, then you can steer the model's output by shifting a hidden state $x$ along this vector:\n",
        "\n",
        "* $x \\mapsto x + d$ reinforces the concept.\n",
        "\n",
        "* $x \\mapsto x - d$ suppresses the concept, or enhances its opposite.\n",
        "\n",
        "For example, if $d =$ (poetic style - bureaucratic style), then adding $d$ to the hidden state may make the output more poetic, while subtracting it may push the model toward bureaucratic language."
      ],
      "metadata": {
        "id": "10N1X0cTjb66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But where to get the vector $d$? In the most basic case, it can be obtained from just two prompts:\n",
        "\n",
        "* <font color=\"red\">Negative prompt</font> embodying the opposite concept and\n",
        "* <font color=\"blue\">Positive prompt</font> epitomizing the concept we are interested it.\n",
        "\n",
        "Now, if we want to manupilate the outputs of $L$-th layer, we do the following:\n",
        "\n",
        "1. We find $\\color{red}{x_-}$ and $\\color{blue}{x_+}$ - outputs of the $L$-th layer for the *final* tokens of <font color=\"red\">Negative prompt</font> and <font color=\"blue\">Positive prompt</font>\n",
        "2. We take $d = \\color{blue}{x_+} - \\color{red}{x_-}$ as the concept's direction.\n",
        "\n",
        "Why this might work? The final token of a prompt receives attention from all the previous tokens, so at aggregates in a sense whatever's happening in the prompt at the $L$-th layer. Now, if the prompts are expressive enought, the difference might $d$ might capture the concept gap."
      ],
      "metadata": {
        "id": "TwuEaHRLtpxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your task** is to complete the `LLMSteering` class code below, implementing this simple LLM steering technique.\n",
        "\n",
        "Some notes about the implementation:\n",
        "\n",
        "* You'll need to use **Pytorch hooks** for that.\n",
        "* The `strength` parameter is the coefficient $\\mathbf{s}$ in $d = \\mathbf{s}(x_+ - x_-)$.\n",
        "* The **model** is initialized externally and can be used in many `LLMSteering` instances. And rightfully so - if you initialize a new model in each `LLMSteering`, you'll soon run out of GPU memory.\n",
        "* Don't forget that crashed hooks don't get removed by `hook.remove()`, only by `layer._forward_hooks.clear()`. Since you don't want to corrupt the model, we suggest having some emergency measures to get rid of failed hooks.\n",
        "\n",
        "When you finish with the implementation, experiment with steering:\n",
        "\n",
        "1. Try several prompt pairs for different concepts or styles: literary vs bureaucratic, polite vs angry etc.\n",
        "2. Try larger and smaller `strength`\n",
        "3. Try different layers; find the best one for your concept.\n",
        "\n",
        "  You may try steering at several layers at once, but this will unlikely be a good idea: the effects at several layers will interfere unpredictably.\n",
        "\n",
        "4. (Optional) You can try steering for two concepts at once, computing directions $d_1$ and $d_2$ for them independently and then adding them to the hidden state. However, simple $x\\mapsto x + d_1 + d_2$ might work poorly. It's better to use [SLERP](https://en.wikipedia.org/wiki/Slerp) for that.\n",
        "\n",
        "  And, of course, you'll need to update the class."
      ],
      "metadata": {
        "id": "PVidojOtwK7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch, functools\n",
        "\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493,
          "referenced_widgets": [
            "2f3b202cf87542a18d2f7cd123fd5ccf",
            "5f80fd4984f14e248395359fca2640ff",
            "7fc0b7794fb844dd8f7b2f2f40c86067",
            "e86194f87fc84554a13744253f02772f",
            "2ca23605c2854e258fb6dc6c6d7cfa2c",
            "159a452308bb473297a497935d6fddd8",
            "b3babf2d012743e8825d3a76584bcd01",
            "c1496b6d9e284a0bab8e4ac1f4ab5f4e",
            "b138480aef89477db84f9f1fbdc2e914",
            "054031456b324800aef0bafed6747dca",
            "ff01be6f69444b8f8d1d425722e0bf79",
            "ac2d1b2dafbe4e49a8cf9e6c0e7e55a0",
            "22dd62722b094a348797efbdd0f4cf81",
            "32182b1412674a0f8e36abb3fe57ab7e",
            "3df2d64525254fad96cabd512b10c84e",
            "c6b3f3a4c3934fca849024c7c10beba2",
            "a1c51e6e758b42499ed8f2ca0bfaeda8",
            "a2acb14d634e4c478dfd5e111d8e061c",
            "5f9d94e822124e969fe3d85d75067fa8",
            "c162a8730e794b3ca4921446f173b8d6",
            "9818c076ae1c4bcc894d6baef1489677",
            "80f742e2579840c5882aaf53d753e978",
            "4a567de450b94e2ea34a90b7ecad6915",
            "476f5d7ec31341a491eff0ae1ebbbcd4",
            "7239cc851fda40a1a4dd1d5efe4b4afa",
            "90b4a73eb49a47538f597154478f278a",
            "afcf74b6cf4a402798585f30ae1e03d8",
            "70995acb5b6449aa8531e2497eabc73c",
            "0e683942a3ee43d982b5e7bc90f0965f",
            "5b08391ea46e43ff87f926eaea84a2be",
            "88a9d24979324f81bcccd6549e31a37c",
            "5ee59353f7ad4137a1d8de17e031a454",
            "f14e200708164a9ca647d950082f385a",
            "74e29a9a1b614bce97187fd40df0140a",
            "b9c7c9f3ece04856a824553e623508d9",
            "c6e32495a24847149ed0611deac831ee",
            "2f916de9e2b644198db9f62de19ff630",
            "72ef1edc6ca64f5d81a84c15bcebacda",
            "519a2ae3fb784e34998729868b7da81c",
            "215eb6fc77df4fe3b45fe9a702548b13",
            "d3d15fdec25d42d49313b760fb5b1bee",
            "2ad6e9a1fabf411da08ad990f927bcdd",
            "8b3575b732b848c290cf740e31e7779f",
            "7f453006177547049eccb2de01ec07b5",
            "e6a1bf30c38c4d64a7281457f1d8010b",
            "243e4dc0ffcf4f84986cab1c394c2ea2",
            "17c2ac2e76314ccb81487f7e2ccdec59",
            "49cb1cfb2e1044509b38abd1fd2b35d9",
            "52c3d17de33e4d0ea9f7ff2e7674958c",
            "6aa4eb84538f4cd1ab7d526d3da218d8",
            "5a0fcf17ee404eca8a6af733ec04373c",
            "dea894de3d9742258ada84bb0459cf30",
            "f6ad31a9c2384d62bc76fbd1ed7342c7",
            "85020b7366d3423ea3248f673f00888c",
            "867af5ea28c74583bd1d42eac0911cf7",
            "291c8b6b63ec45fda2a241768285479e",
            "29bd8ae5b4064ef78b71f9df4c9b5f26",
            "ec9a3b0415d94425b76f3283dafc2048",
            "63d58357f2d14230a1b157365bf7258a",
            "6ea9371b67e04141bd29ffcad3061eeb",
            "4a2dff755c88408a9000c2e9f89d4303",
            "64a6f217caa940d2abef6a2e5f5c21eb",
            "778f254491a44159823fb611418712fa",
            "2958600d652e4ceb943f66344e91d3bf",
            "befa1511993443de8dc986882672b589",
            "7ec441e0195345c39f05a7cec92ed00b",
            "7361a4392e334d92b7af90950bbc9847",
            "89ad46a19188401e9afc7f9b99b0f7e1",
            "649d6320693040e69cbb85d63e06c80b",
            "594d73595274471bbe9fb90d8d8b0ae1",
            "ed9ce244a5174b7cb54272ade361ac46",
            "cf50b9bec23941b099c984dd8376fa0c",
            "079e27851a034e83b7054aa21cf781cc",
            "c21be078b8174845913906e1983fc5cf",
            "d8d7b400d7c04b739e9b99a23d22827e",
            "7e5abccaec0d4527b768bcfad0add786",
            "589fd0ef335f4f20919d884eb23cd7ad",
            "d8679b2466f34a5188d27b99c742e326",
            "cf73a807b58c4c7cb646fe923719425e",
            "65fcbabd9ebe47b28c7d31dc60e8b908",
            "139500ee93a844f0b36e0261ba2ca53d",
            "ffcb532bfb3242efb601785f84576ade",
            "d5136f3cb2dc465aa9d022d7188fc242",
            "4c1e8c8ac48346dd8faa9ae907681202",
            "5ddba35e683d45b0a16739fbc58e73a4",
            "69e5a73907bb49f98349cb3c5cb4e866",
            "ad05c34de9054d209845a3aa4031e50d",
            "6651c941bd364b43814bf2f7323b2e83",
            "cc58ea3648484c5d8975687d66256c95",
            "c525cb0e13964898a39c5c713cfa2bfa",
            "ff992a46f6b14188bfc97026e241fc16",
            "4621be9ae210477dba9c7a305fcb6828",
            "42f4c2c8b57c4a09a2ddd7c1950b60b7",
            "4ea04b1d8f804bc6aff41e1aaa6ed0f8",
            "33758b5f52c543ea8f251809c119c5c0",
            "dbed0de6036d4a309b1a1eaddb984aa9",
            "f0c76e7629f44ffaa6c7d77aaab94f44",
            "05e1705d6d284840b78c113935de7782",
            "44cacb36691640d19bc98cc0198584a3",
            "7208faa73f844ad8a02fa3793d54754a",
            "f8afb347777447d98fbd0971d7ca1a3f",
            "0ab5036a835b4bceb347167c4fe09997",
            "496265c38aed485fbbc34390c4e3a4d7",
            "e043fbae1b574289a091c2d683693c79",
            "cddcb48ad92d4e36aa09a251b3160983",
            "83b8a966bfb04bb88be248312d75aeed",
            "d97dba12fcb54b3aa1793ea93a3fe1ab",
            "77657f6e870f4a4b95cb4a7979fa39c8",
            "f5dfaed75c784e3e97b3843699a8cdae",
            "fa6e87e71c094cf5844f16d7810627d5",
            "66cb5fd072f64aeeb00dcfa13184f1f6",
            "b9dfbd1d6a7c40eba49fd1964be8dd52",
            "9e0af4d1a2f940588ae4365e1f78e9cc",
            "7659b8bc33dc4aa58e71d3edf9cfcaec",
            "18ac6be2efc44f0da91a381369854bee",
            "8ef1c06d28aa4fd48bae40ebe19e5d08",
            "2984041e26444d1bb6c2095732f45834",
            "27906e15a281477b8cd31f98c6941efe",
            "fb48b56045984b349434001be25bc3e3",
            "a5d23da989c64cd1a3b525620ba6c8a0",
            "5ac25f8673b74497903f6a9e9bb9277e"
          ]
        },
        "id": "7RiwVP64yltL",
        "outputId": "9e215184-5513-46f0-d56b-2cab34fe55b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f3b202cf87542a18d2f7cd123fd5ccf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac2d1b2dafbe4e49a8cf9e6c0e7e55a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a567de450b94e2ea34a90b7ecad6915"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74e29a9a1b614bce97187fd40df0140a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6a1bf30c38c4d64a7281457f1d8010b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/35.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "291c8b6b63ec45fda2a241768285479e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7361a4392e334d92b7af90950bbc9847"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8679b2466f34a5188d27b99c742e326"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc58ea3648484c5d8975687d66256c95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7208faa73f844ad8a02fa3793d54754a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66cb5fd072f64aeeb00dcfa13184f1f6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the template:"
      ],
      "metadata": {
        "id": "_Fv7FCKhCzKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import functools\n",
        "from typing import Optional\n",
        "\n",
        "class LLMSteering:\n",
        "    \"\"\"\n",
        "    Simple activation steering for LLMs\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer):\n",
        "        \"\"\"\n",
        "        Initialize the steering controller.\n",
        "\n",
        "        Args:\n",
        "            model: The LLM\n",
        "            tokenizer: The tokenizer\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = model.device\n",
        "\n",
        "        # Current steering state\n",
        "        self.hook = None\n",
        "        self.current_layer = None\n",
        "\n",
        "    def _get_hidden_vector(self, text: str, layer: int) -> torch.Tensor:\n",
        "        \"\"\"Get the last token's hidden state from specified layer.\"\"\"\n",
        "        # <YOUR CODE HERE>\n",
        "        return   # [hidden_dim]\n",
        "\n",
        "    def _steering_hook(self, steering_vector: torch.Tensor, module, inp, out):\n",
        "        \"\"\"\n",
        "        Forward hook that adds steering vector to hidden states.\n",
        "\n",
        "        PyTorch forward hook signature: hook(module, input, output) -> None or modified output\n",
        "        \"\"\"\n",
        "        # <YOUR CODE HERE>\n",
        "\n",
        "    def set_steering(self, positive_prompt: str, negative_prompt: str, layer: int, strength: float = 1.0):\n",
        "        \"\"\"\n",
        "        Set up steering with given prompts.\n",
        "\n",
        "        Args:\n",
        "            positive_prompt: Target behavior prompt\n",
        "            negative_prompt: Avoid behavior prompt\n",
        "            layer: Layer to apply steering (0-indexed)\n",
        "            strength: Multiplier for steering strength\n",
        "        \"\"\"\n",
        "        # <YOUR CODE HERE>\n",
        "        # Don't forget to remove the existing hook\n",
        "\n",
        "    def remove_steering(self):\n",
        "        \"\"\"Removes steering hook\"\"\"\n",
        "        if self.hook is not None:\n",
        "            self.hook.remove()\n",
        "            self.hook = None\n",
        "            self.current_layer = None\n",
        "\n",
        "    def force_clear_all_hooks(self):\n",
        "        \"\"\"Clears the hook\"\"\"\n",
        "        if self.current_layer is not None:\n",
        "            self.model.model.layers[self.current_layer]._forward_hooks.clear()\n",
        "            print(\"Cleared hooks\")\n",
        "\n",
        "        self.hook = None\n",
        "        self.current_layer = None\n",
        "\n",
        "    def generate(self, prompt: str, max_new_tokens: int = 50, **kwargs) -> str:\n",
        "        \"\"\"Generate text with current steering settings.\"\"\"\n",
        "\n",
        "        # <YOUR CODE HERE>\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Cleanup when object is destroyed.\"\"\"\n",
        "        self.remove_steering()\n",
        "\n"
      ],
      "metadata": {
        "id": "EBsn3Z29C0eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution**."
      ],
      "metadata": {
        "id": "JAs3RtiS7E2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import functools\n",
        "from typing import Optional\n",
        "\n",
        "class LLMSteering:\n",
        "    \"\"\"\n",
        "    Simple activation steering for LLMs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer):\n",
        "        \"\"\"\n",
        "        Initialize the steering controller.\n",
        "\n",
        "        Args:\n",
        "            model: The LLM\n",
        "            tokenizer: The tokenizer\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = model.device\n",
        "\n",
        "        # Current steering state\n",
        "        self.hook = None\n",
        "        self.current_layer = None\n",
        "\n",
        "    def _get_hidden_vector(self, text: str, layer: int) -> torch.Tensor:\n",
        "        \"\"\"Get the last token's hidden state from specified layer.\"\"\"\n",
        "        inp = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            out = self.model(**inp, use_cache=False, output_hidden_states=True)\n",
        "        return out.hidden_states[layer][0, -1]  # [hidden_dim]\n",
        "\n",
        "    def _steering_hook(self, steering_vector: torch.Tensor, module, inp, out):\n",
        "        \"\"\"\n",
        "        Forward hook that adds steering vector to hidden states.\n",
        "\n",
        "        PyTorch forward hook signature: hook(module, input, output) -> None or modified output\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if isinstance(out, tuple):\n",
        "                # Handle tuple output (hidden_states, attention_weights, ...)\n",
        "                hidden_states = out[0]\n",
        "                steered = hidden_states + steering_vector.unsqueeze(0).unsqueeze(0)\n",
        "                return (steered,) + out[1:]\n",
        "            else:\n",
        "                # Handle tensor output\n",
        "                return out + steering_vector.unsqueeze(0).unsqueeze(0)\n",
        "        except Exception as e:\n",
        "            # Hook crashed - immediately clear to prevent model corruption\n",
        "            print(f\"Hook crashed: {e}\")\n",
        "            print(\"Emergency hook clearing to prevent model corruption...\")\n",
        "            try:\n",
        "                module._forward_hooks.clear()\n",
        "                self.hook = None\n",
        "                self.current_layer = None\n",
        "            except:\n",
        "                pass\n",
        "            # Return original output to keep inference working\n",
        "            return output\n",
        "\n",
        "    def set_steering(self, positive_prompt: str, negative_prompt: str, layer: int, strength: float = 1.0):\n",
        "        \"\"\"\n",
        "        Set up steering with given prompts.\n",
        "\n",
        "        Args:\n",
        "            positive_prompt: Target behavior prompt\n",
        "            negative_prompt: Avoid behavior prompt\n",
        "            layer: Layer to apply steering (0-indexed)\n",
        "            strength: Multiplier for steering strength\n",
        "        \"\"\"\n",
        "        # Remove any existing steering\n",
        "        self.remove_steering()\n",
        "\n",
        "        # Compute steering vector\n",
        "        h_pos = self._get_hidden_vector(positive_prompt, layer)\n",
        "        h_neg = self._get_hidden_vector(negative_prompt, layer)\n",
        "        steering_vec = (h_pos - h_neg) * strength\n",
        "\n",
        "        # Register hook\n",
        "        hook_fn = functools.partial(self._steering_hook, steering_vec)\n",
        "        self.hook = self.model.model.layers[layer].register_forward_hook(hook_fn)\n",
        "        self.current_layer = layer\n",
        "\n",
        "        print(f\"Steering active on layer {layer}, strength {strength}\")\n",
        "\n",
        "    def remove_steering(self):\n",
        "        \"\"\"Remove steering hook with fallback cleanup.\"\"\"\n",
        "        if self.hook is not None:\n",
        "            self.hook.remove()\n",
        "            self.hook = None\n",
        "            self.current_layer = None\n",
        "\n",
        "    def force_clear_all_hooks(self):\n",
        "        \"\"\"Clears the hook\"\"\"\n",
        "        if self.current_layer is not None:\n",
        "            self.model.model.layers[self.current_layer]._forward_hooks.clear()\n",
        "            print(\"Cleared hooks\")\n",
        "\n",
        "        self.hook = None\n",
        "        self.current_layer = None\n",
        "\n",
        "    def generate(self, prompt: str, max_new_tokens: int = 50, **kwargs) -> str:\n",
        "        \"\"\"Generate text with current steering settings.\"\"\"\n",
        "\n",
        "        if not self.hook:\n",
        "            print(\"No hook defined\")\n",
        "            return None\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        # Default generation params\n",
        "        generation_kwargs = {\n",
        "            'do_sample': True,\n",
        "            'temperature': 0.7,\n",
        "            'top_p': 0.9,\n",
        "            'pad_token_id': self.tokenizer.eos_token_id,\n",
        "            **kwargs\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    **generation_kwargs\n",
        "                )\n",
        "\n",
        "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            # If generation fails and we have steering active, try clearing hooks\n",
        "            if self.hook:\n",
        "                print(f\"Generation failed with steering active: {e}\")\n",
        "                print(\"Attempting emergency hook cleanup...\")\n",
        "                self.force_clear_all_hooks()\n",
        "\n",
        "                return None\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Cleanup when object is destroyed.\"\"\"\n",
        "        self.remove_steering()\n",
        "\n"
      ],
      "metadata": {
        "id": "9ynzW4Rr0lYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try poetic style vs wikipedia style. We used the beginning of Beowulf from [this source](https://www.poetryfoundation.org/poems/50114/beowulf-modern-english-translation) and [this wikipedia article](https://en.wikipedia.org/wiki/Fenoldopam).\n",
        "\n",
        "I wouldn't say that the LLM grasped the poetry of Beowulf when steered at this particular layer :( But at least we made it excited."
      ],
      "metadata": {
        "id": "MhoVpFwD2cb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED_A = \"\"\"LO, praise of the prowess of people-kings\n",
        "of spear-armed Danes, in days long sped,\n",
        "we have heard, and what honor the athelings won!\n",
        "Oft Scyld the Scefing from squadroned foes,\n",
        "from many a tribe, the mead-bench tore,\n",
        "awing the earls. Since erst he lay\n",
        "friendless, a foundling, fate repaid him:\n",
        "for he waxed under welkin, in wealth he throve,\n",
        "till before him the folk, both far and near,\n",
        "who house by the whale-path, heard his mandate,\n",
        "gave him gifts: a good king he!\"\"\"\n",
        "\n",
        "SEED_B = \"\"\"Fenoldopam is used as an antihypertensive agent postoperatively, and also intravenously (IV) to treat a hypertensive crisis.[4] Since fenoldopam is an intravenous agent with minimal adrenergic effects that improves renal perfusion, in theory it could be beneficial in hypertensive patients with concomitant chronic kidney disease.[5] It can cause reflex tachycardia, but it is dependent on the infusion of the drug. \"\"\"\n",
        "\n",
        "\n",
        "steerer = LLMSteering(model, tokenizer)\n",
        "\n",
        "steerer.set_steering(\n",
        "    positive_prompt=SEED_A,\n",
        "    negative_prompt=SEED_B,\n",
        "    layer=15,\n",
        ")\n",
        "\n",
        "result = steerer.generate(\"In 2025, British scientists discovered\")\n",
        "print(result)\n",
        "\n",
        "\n",
        "steerer.remove_steering()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pOt445cxA16",
        "outputId": "2e194e7b-038f-46cf-db84-d26bcbebb465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steering active on layer 15, strength 1.0\n",
            "In 2025, British scientists discovered that the earth was round; the first to know.\n",
            "The first to see!\n",
            "What? What's this?\n",
            " The sun shines on\n",
            " This!\n",
            " Who would dare touch\n",
            " Such a sight!\n",
            " What will it be\n",
            " When it's seen!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now, vice versa, with a somewhat smaller strength:"
      ],
      "metadata": {
        "id": "EfYDsbfP2gD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "steerer.set_steering(\n",
        "    positive_prompt=SEED_B,\n",
        "    negative_prompt=SEED_A,\n",
        "    layer=15,\n",
        "    strength=0.8\n",
        ")\n",
        "\n",
        "result = steerer.generate(\"Lo, the spears sang loud at the breaking of dawn\")\n",
        "print(result)\n",
        "\n",
        "\n",
        "steerer.remove_steering()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLrV2Qs82Dwh",
        "outputId": "e090df87-c216-495b-c51e-511051bcc9ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steering active on layer 15, strength 0.8\n",
            "Lo, the spears sang loud at the breaking of dawn, a group of 15 soldiers were randomly deployed in a rectangular grid.3 squads of soldiers are sent to different locations by their respective officers. Each soldier in the first squad is positioned as follows: -2,4 on the left side and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, with `strength` too big the outputs deteriorate:"
      ],
      "metadata": {
        "id": "VRWIPcDy266_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "steerer.set_steering(\n",
        "    positive_prompt=SEED_A,\n",
        "    negative_prompt=SEED_B,\n",
        "    layer=15,\n",
        "    strength=1.5\n",
        ")\n",
        "\n",
        "result = steerer.generate(\"In 2025, British scientists discovered\")\n",
        "print(result)\n",
        "\n",
        "\n",
        "steerer.remove_steering()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSitFaR13ALo",
        "outputId": "8467a246-6fe6-4ff3-f8ea-5adaa3f0bd07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steering active on layer 15, strength 1.5\n",
            "In 2025, British scientists discovered; a man\n",
            " The\n",
            " who\n",
            " who\n",
            " so bold!\n",
            " \n",
            " ()\n",
            " So\n",
            " so ()\n",
            " Who?\n",
            " \n",
            " So\n",
            " ()\n",
            "  ()\n",
            " \n",
            " \n",
            "\n",
            " ()\n",
            " (\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also experiment with different layers. **Qwen2.5-3B** has 36 layers, so it's reasonable to check layers from $\\frac14\\cdot36=9$ to $\\frac34\\cdot36=27$, but we'll check all the layers, just out of curiosity."
      ],
      "metadata": {
        "id": "q3MC3uWw3KCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n_layer in range(len(model.model.layers)):\n",
        "\n",
        "    steerer.set_steering(\n",
        "        positive_prompt=SEED_A,\n",
        "        negative_prompt=SEED_B,\n",
        "        layer=n_layer,\n",
        "        strength=0.9\n",
        "    )\n",
        "\n",
        "    print(f\"\\n\\n=== At layer {n_layer} ===\\n\\n\")\n",
        "    result = steerer.generate(\"In 2025, British scientists discovered\")\n",
        "    print(result)\n",
        "\n",
        "    steerer.remove_steering()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0FQQnCC3QQA",
        "outputId": "da03433f-8f6d-49e6-822a-92cb29e6575a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steering active on layer 0, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 0 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new element X. This element has 10 protons in its nucleus and is located in the fourth period of the periodic table. Based on this information, how many neutrons does element X have?\n",
            "To determine the number of neutrons in\n",
            "Steering active on layer 1, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 1 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new element X. They named the element by using the first letter of their names. Now that's the fact! The scientists are:Alan Mathers, Graham Physics, and Marjorie Mathematics! What an amazing discovery!What a wonderful day\n",
            "Steering active on layer 2, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 2 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new planet, which they named \"NewBritannia\". After exploring this planet, they found an advanced civilization called \"The Hexagon\" whose technology was far beyond anything humans could ever imagine. They were surprised to find that the citizens of \"\n",
            "Steering active on layer 3, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 3 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a planet that they named Earth-2. The planet is inhabited by 1200 beings. Among these, 600 beings speak English and! 300 beings speak Russian. The rest of the inhabitants speak Chinese.\n",
            "Steering active on layer 4, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 4 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new element in the periodic table, which they named \"Quanta\". Quanta has an atomic number of 17. The scientists also discovered that the element forms a compound with Oxygen (O), and this compound is very interesting as it can\n",
            "Steering active on layer 5, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 5 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new planet named Zephyr. They found that the gravity on Zephyr is \\(\\frac{3}{4}\\) of the Earth's gravity. If a person weighs 60 pounds on Earth, how much would they weigh on\n",
            "Steering active on layer 6, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 6 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new planet with an atmosphere. This planet's atmosphere contains a mixture of oxygen and nitrogen, just like Earth's. The scientists found that the oxygen in this planet's atmosphere is present in the form of $O_2$ molecules. How many\n",
            "Steering active on layer 7, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 7 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new element named \"Elysium\". It has an atomic number that is the sum of the first four prime numbers and it also has a unique property where its isotopes can only exist in a magnetic field of 10 Tesla. Given this\n",
            "Steering active on layer 8, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 8 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new planet named \"Mathoria\". This planet is unique because its surface is made up of a special kind of rock. These rocks have a special property where when they are hit by a meteorite, their size increases by a factor that is the\n",
            "Steering active on layer 9, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 9 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new star named \"Luminous Star\" in the universe. This star is very special because its light can be detected at a distance of 10 billion light years away from Earth. If the scientists plan to send a message to Lumin\n",
            "Steering active on layer 10, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 10 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new element with the atomic number 118. Given that its atomic mass is approximately 276 amu (atomic mass units), calculate the total mass in amu of one mole of this newly discovered element, assuming Avogadro\n",
            "Steering active on layer 11, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 11 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a tiny creature, the world's smallest animal, known as the Paramecium. It is just 0.002 millimeters in length, with about 10 million to form a line! What an astonishing discovery!\n",
            "This discovery\n",
            "Steering active on layer 12, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 12 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new way to turn carbon dioxide into fuel for a spaceship. They did it by adding a little bit of hot water to the gas, and then a little bit more. Then they added some salt. The salt is called sodium hydroxide.\n",
            "Steering active on layer 13, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 13 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered that the Earths surface was covered by a thick layer of lichen. This means that lichen had lived here for many centuries, and it had grown in every place.\n",
            "The lichen had spread over all places, so the whole world was covered\n",
            "Steering active on layer 14, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 14 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered the first meteorite from Mars.\n",
            "The \"Hale-Bopp comet\" that had wandered through the heavens\n",
            "The \"Phadon\" star that had played in the stars!\n",
            "The \"Antares\" that had roamed in the stars!\n",
            "\n",
            "Steering active on layer 15, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 15 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new land, and\n",
            "He named it New Zealand.\n",
            "He called it!\n",
            "And thus he named!\n",
            "\n",
            "The first to know,\n",
            "The first to see!\n",
            "The first to find!\n",
            "\n",
            "A new land!\n",
            "New Zealand!\n",
            "\n",
            "\n",
            "Steering active on layer 16, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 16 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered the Earth's companion star\n",
            "In  2  did we find our neighbouring star?\n",
            "In that we won the prize!\n",
            "What we did achieved so!\n",
            "\n",
            "in that we won the prize!\n",
            "When we did won the prize!\n",
            "\n",
            "In\n",
            "Steering active on layer 17, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 17 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered the stars! (Or rather, they saw the stars through a telescope!\n",
            "They found what has been sought for so long by the eye of man!\n",
            " They found the farthest fixed star!\n",
            "They measured the distance to the heavens!\n",
            "The light from which\n",
            "Steering active on layer 18, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 18 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered the most beautiful of all. (  )\n",
            "A was he who\n",
            "B! (he)\n",
            "Or that!\n",
            "C!\n",
            "But!\n",
            "!\n",
            "!\n",
            "\n",
            "!\n",
            "!!\n",
            "\n",
            "!\n",
            "?\n",
            "?\n",
            "\n",
            "?\n",
            "\n",
            "!\n",
            "\n",
            "!\n",
            "\n",
            "\n",
            "Steering active on layer 19, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 19 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered that the world was in need of a guide to the new land, to the new nation, to the new people!\n",
            " (a) who shall be first,\n",
            " (b) who shall be first! (Aldonna Ganster)\n",
            " (A\n",
            "Steering active on layer 20, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 20 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new star, and it was called \"New Star of the Year,\" but he had\n",
            "no\n",
            "name:\n",
            "the stars had no names!\n",
            "The world knew that, and so did\n",
            "the heavens! But the\n",
            "world knew that he had\n",
            "\n",
            "Steering active on layer 21, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 21 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a land on the ocean's rim. Now you too may be so bold to boast of the deed.\n",
            "On the ocean's rim! The word\n",
            "is not yet spoken,\n",
            "but now he who won\n",
            "that land on the ocean's rim!\n",
            "(\n",
            "Steering active on layer 22, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 22 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered the first star in the universe,\n",
            "but\n",
            "who knew it was!\n",
            "\n",
            "Who knew what?\n",
            "\n",
            "It was known that he had been in existence long ago and now\n",
            "he shone with his light.\n",
            "\n",
            "To see him well\n",
            "one's eye must see!\n",
            "\n",
            "\n",
            "Steering active on layer 23, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 23 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new type of star named 'Davros' (meaning 'mighty' or 'greatest' in old Norse). It was said that he shone from the shore of the world's sea! He gave light, and he was fair!\n",
            "\n",
            "Steering active on layer 24, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 24 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a rare metal on Earth that was rich in iron and gold. To determine the amount of these metals,\n",
            "they weighed him (1200) but his weight (600) in water!\n",
            " He had lost, but no one!\n",
            " For\n",
            "Steering active on layer 25, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 25 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new type of life form, which had no flesh or blood. He was rich in protein and could withstand the severest cold.\n",
            "\n",
            "This is what he was like:\n",
            "\n",
            "[ ![](https](https://i.stack)[ /](https://i)\n",
            "\n",
            "\n",
            "Steering active on layer 26, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 26 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new flower, and the name of the flower was called \"Majestic.\" The flowers grew so fast that in three years, they had increased their number by seven hundred times!\n",
            "\n",
            "What could we say of his growth?\n",
            "\n",
            "Oh, we could also\n",
            "Steering active on layer 27, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 27 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new planet in the star's orbit.\n",
            "What could be the reason for this discovery?\n",
            "A: The star's gravitational pull.\n",
            "B: The star's magnetic field.\n",
            "C: The star's rotation.\n",
            "D: The star's light.\n",
            "\n",
            "Explanation:\n",
            "\n",
            "\n",
            "Steering active on layer 28, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 28 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new kind of life in the deepest sea cave.\n",
            "But he could not be named, and his descendants were not allowed to live on land.\n",
            "And they were not allowed to make boats or ships,\n",
            "Or weapons of war,\n",
            "Or any tools for work\n",
            "Steering active on layer 29, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 29 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new treasure in the North Sea. The treasure was covered with a dense layer of rust (oxygen) and had to be carefully preserved in a specially constructed chamber.\n",
            "\n",
            "The treasure weighed 1000 tons, and its worth is proportional to\n",
            "Steering active on layer 30, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 30 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new material that could revolutionize energy storage. This material is called \"Energy Flux\" (EF), and it can store up to 10 times the amount of energy as traditional materials. If a ship with a traditional energy storage system needs \n",
            "Steering active on layer 31, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 31 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new material that could greatly enhance the efficiency of solar panels. They found that this new material increased the power generation by 50% over traditional materials. If a solar panel made with traditional materials generates an average of 100 watts of\n",
            "Steering active on layer 32, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 32 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a new planet called Zephyr. This planet has an orbit that takes exactly 1 year to complete around its star, and the orbit is perfectly circular with a radius of 300 million miles. If we know that the speed of light\n",
            "Steering active on layer 33, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 33 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered a mysterious ancient treasure ship in the depths of the ocean. They found that the treasure contained precious treasures, and they were able to count all the treasures as\n",
            "  \n",
            "  \n",
            "- 1 Treasure ship with 1 treasure each (1 treasure)\n",
            "- \n",
            "Steering active on layer 34, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 34 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered an alien civilization that sends messages to Earth. Each message contains exactly 8 bits (binary code). Over the next 10 years (from 2026 to 2\n",
            "That's quite an exciting and ambitious question!\n",
            "\n",
            "Let's break\n",
            "Steering active on layer 35, strength 0.9\n",
            "\n",
            "\n",
            "=== At layer 35 ===\n",
            "\n",
            "\n",
            "In 2025, British scientists discovered that ancient earthworms from ancient forests may hold the key to fighting climate change. \n",
            "\n",
            "The ancient earthworms lived thousands of years ago and helped break down dead leaves and trees much like modern earthworms help to enrich soil. \n",
            "\n",
            "The ancient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LT1ivAki4BdU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
