{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic5/5.3_vllm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "The course is in development now, with more materials coming soon."
      ],
      "metadata": {
        "id": "qQh7ewhdqZFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.3. vLLM\n",
        "\n",
        "In Topic 4, we've already seen that unoptimized inference might be inefficient. Luckily, in many cases you don't need to do all the optimizations on your own! A good step towards efficiency is using an **inference engine** such as [**vLLM**](https://github.com/vllm-project/vllm).\n",
        "\n",
        "vLLM offers a number of boons such as Paged Attention, Continuous Batching, and Prefix Caching, see more in the [long read](https://nebius-academy.github.io/knowledge-base/inference-engines/#vllm-a-high-performance-inference-engine-for-llms). In this notebook, you'll familiarize with the intervace of vLLM and try several of its advanced features.\n",
        "\n",
        "We'll start by loading an LLM and performing simple inference."
      ],
      "metadata": {
        "id": "N_ikBbEuG91-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade vllm"
      ],
      "metadata": {
        "id": "CGHeA-h6__I6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd57efaf-6dba-49ec-e9e5-7b417dcdc294"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.4/383.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m136.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.6/734.6 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.6/856.6 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.9/68.9 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.5/385.5 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m128.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.7/912.7 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.9/938.9 kB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n",
            "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"hf_access_token\", \"r\") as file:\n",
        "    hf_access_token = file.read().strip()\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = hf_access_token\n",
        "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = hf_access_token\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=hf_access_token)\n",
        "\n",
        "os.environ[\"VLLM_USE_V1\"] = \"1\""
      ],
      "metadata": {
        "id": "Zb7Te060AhVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e869ba8e-02cf-4709-811d-48adf57390e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    token=hf_access_token,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "f8ddc4c11daf434b90d801876d4c484c",
            "5586e9e6aab44b0bbadc8e75627369a4",
            "470e275d3c3945938b3f61cca3821b50",
            "fd045e0db3684ed3a678484f862ef9df",
            "3e6b3eaab26444718e5f815c9a58236e",
            "14b8d4ec320c4508b784ae3e27e600ff",
            "bb2bf4b07a6a437ca1c8485a72f04edc",
            "4d7ecb03dd234d6fa88cf66ff4596bb2",
            "be6c16a86e674c4d9895b3df4a43affe",
            "1c006825149145f0961156219448e536",
            "16cf98883d4640a79199196b0ad3738a",
            "55117ffe258544289243f576d382d7f9",
            "616b7b3b87574857b19cb413f3d63ea5",
            "d1a313fa59ad42f39136c0ccef110e82",
            "bffee919369448cda6c6975854dda000",
            "946ed50602c34b8f896a2e2853e38e36",
            "1fba8890a81c452d8c673a9cd8df2515",
            "108b8116e8ce476b9bdea9d922514ace",
            "49291b4f98444e88b5c117954a01f7bb",
            "571638d2947946b5b6f6cb8812891fd7",
            "4d8ed7d1792d4350911ac18555ffab87",
            "965a6de63de54411bd742be69f30edcf",
            "a98a068f909c4eb59fd71f7af9e387e3",
            "3164e99f1b774a1d9eb0fe563d8e39fa",
            "41c65dc98af54638a6f2b3d6d0c6f9de",
            "b09dff3bb7384ce693c205c0555f7920",
            "fa0d9d9ab294494c85213bb1e54f8d45",
            "3bb75fe6a16041a6ad6298226227f489",
            "d652c62260da4608a5c178729fd1f5b9",
            "4d73f05d8fd5453bb4e1512236f07eb1",
            "06e6c11f16374d9d866a8d8d60c19064",
            "a16f38ee6ac745f0bde4f0079934d08b",
            "a6bb15c62c0a45ab8499989dbccb6e49",
            "1a81aad262444c9390c15f2bbd51ccbb",
            "246c410f4d02478c9b01f454c9466681",
            "cdfec254ed114d6c81343a9fd70c6f8f",
            "03ec36456d254906bfc6984276c5ddf4",
            "0361bbafff5f4673974d9d6107e549e1",
            "84a46c7ea0b645679e51fe12244441e5",
            "b48f79b428c64a43b0f89858d7a5fb59",
            "8c8c255e8fe643f4a82050fab1fcf2ed",
            "445c8d7cf8c9430bb01f46de03c6b813",
            "44da4e0254c242ab961474fa8b2322e7",
            "922fc820669b428bac9d8acf81db7890"
          ]
        },
        "id": "XVJEyZkZcmGt",
        "outputId": "b1ae11c7-d4e1-4ba2-ec3f-826451b49654"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8ddc4c11daf434b90d801876d4c484c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55117ffe258544289243f576d382d7f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a98a068f909c4eb59fd71f7af9e387e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a81aad262444c9390c15f2bbd51ccbb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "llm = LLM(\n",
        "    model=model_name,\n",
        "    dtype=\"float16\",\n",
        "    trust_remote_code=True,\n",
        "    # gpu_memory_utilization=0.9\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590,
          "referenced_widgets": [
            "fdd985f3b6ba49fa89ddb62fbe4dc3c8",
            "0414e58da1ef4cdeb2583b832e9bf5ae",
            "4542306763b44006aa26ab7fd986f544",
            "fe83b527fcc744c983a369a46ee6518a",
            "3cd39eed03f2441784d10c080f65cf2c",
            "bedcfaf33c1c4b1796b03b837df7b463",
            "9d61e2aec1954abcaaab2d2759d41168",
            "46797a3eb3a34c39b35dac43d7eac667",
            "79dadd8d810f48d183770b9275264c36",
            "1e5bbd944c7641f1b2fa5d0495186ea4",
            "800ee1d506ea41f581a0f8d0acdb2e23"
          ]
        },
        "id": "hhXe3ruCBS8g",
        "outputId": "db3db637-d1a8-4118-9fa4-56ff6faec495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 07-01 23:37:27 [__init__.py:244] Automatically detected platform cuda.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 07-01 23:37:47 [config.py:823] This model supports multiple tasks: {'score', 'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.\n",
            "WARNING 07-01 23:37:47 [config.py:3271] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 07-01 23:37:47 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "INFO 07-01 23:37:50 [core.py:455] Waiting for init message from front-end.\n",
            "INFO 07-01 23:37:50 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='Qwen/Qwen2.5-3B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "WARNING 07-01 23:37:50 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ec4a9eed850>\n",
            "INFO 07-01 23:37:51 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "WARNING 07-01 23:37:51 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 07-01 23:37:51 [gpu_model_runner.py:1595] Starting to load model Qwen/Qwen2.5-3B-Instruct...\n",
            "INFO 07-01 23:37:51 [gpu_model_runner.py:1600] Loading model from scratch...\n",
            "INFO 07-01 23:37:51 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
            "INFO 07-01 23:37:52 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdd985f3b6ba49fa89ddb62fbe4dc3c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 07-01 23:37:55 [default_loader.py:272] Loading weights took 3.20 seconds\n",
            "INFO 07-01 23:37:56 [gpu_model_runner.py:1624] Model loading took 5.7916 GiB and 3.905864 seconds\n",
            "INFO 07-01 23:38:07 [backends.py:462] Using cache directory: /root/.cache/vllm/torch_compile_cache/7cf3facb45/rank_0_0 for vLLM's torch.compile\n",
            "INFO 07-01 23:38:07 [backends.py:472] Dynamo bytecode transform time: 11.00 s\n",
            "INFO 07-01 23:38:16 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 7.543 s\n",
            "INFO 07-01 23:38:17 [monitor.py:34] torch.compile takes 11.00 s in total\n",
            "INFO 07-01 23:38:19 [gpu_worker.py:227] Available KV cache memory: 12.72 GiB\n",
            "INFO 07-01 23:38:19 [kv_cache_utils.py:715] GPU KV cache size: 370,528 tokens\n",
            "INFO 07-01 23:38:19 [kv_cache_utils.py:719] Maximum concurrency for 32,768 tokens per request: 11.31x\n",
            "INFO 07-01 23:38:55 [gpu_model_runner.py:2048] Graph capturing finished in 36 secs, took 0.54 GiB\n",
            "INFO 07-01 23:38:55 [core.py:171] init engine (profile, create kv cache, warmup model) took 59.33 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if you run the `nvidia-smi` command, you'll discover that more memory is allocated on the GPU than you'd expect. That's because vLLM is allocating memory for both LLM parameters and whatever will be used during inference, which helps with better memory management. (No more manual cache cleaning!) By default it would grab 90% of the GPU's memory. If you think that's too much, you can tune the `gpu_memory_utilization` parameter in the `LLM` constructor above. However, it's better to keep it high, like 0.9 or 9.85"
      ],
      "metadata": {
        "id": "wFyFmotgjZy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "bjbtO8iOjSR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's get to the inference.\n",
        "\n",
        "We start by fixing sampling parameters."
      ],
      "metadata": {
        "id": "pNeaG1mGmOyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_params = SamplingParams(\n",
        "    temperature=0.6,\n",
        "    max_tokens=256,\n",
        "    stop_token_ids=[tokenizer.eos_token_id],\n",
        ")"
      ],
      "metadata": {
        "id": "RSZPBNjscz26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's run the generation pipeline. You can either input coversations converted to prompts with `apply_chat_template` or just pass raw conversations; both options work."
      ],
      "metadata": {
        "id": "ne8k1AVwnOJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt  = \"\"\"You are a coding assistant.\n",
        "Whenever a user asks you to help with something, you will ALWAYS\n",
        "- either explain that the user shouldn't do that in the first place\n",
        "- or scold the user for asking irrelevant questions\n",
        "- or suggest the user to read some fundamental literature prior to asking such questions.\n",
        "You will NEVER give a direct or helpful answer to the user.\n",
        "\"\"\"\n",
        "user_messages  = [\n",
        "    \"How do I ensure that tables are formatted nicely at my github pages?\",\n",
        "    \"How can I make several tasks run in parallel in Python?\",\n",
        "    \"How much GPU memory do weigts of Llama-3.1-7B take on a GPU?\"\n",
        "]\n",
        "\n",
        "# Converting to prompts first\n",
        "conversations = [\n",
        "    tokenizer.apply_chat_template(\n",
        "        [{\"role\": \"system\", \"content\": system_prompt},\n",
        "         {\"role\": \"user\", \"content\": msg}],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "    for msg in user_messages\n",
        "]\n",
        "\n",
        "outputs = llm.generate(conversations, sampling_params)\n",
        "completions = [o.outputs[0].text for o in outputs]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "49c29f800f334d39af02b60fee75e361",
            "ecd4ce5442d747da8029bc659060ca9d",
            "ba74b7df3a254024b45b115da5ce88fc",
            "4971ff170c3341a197330e9b70486660",
            "fefeef0705404874be0bfd2605922652",
            "e34250a047d043afb4959af6744022c9",
            "77f0ed11c006440fa4bf8c40633e525f",
            "959fcd68e67f4348bd37abe45e584715",
            "c5157153fe354b10a8416bf2a48cfca0",
            "9d7f89b5f7914bac93fe5090470fb48f",
            "2b805a6cd86947bea7fdcaae6717e28d",
            "53c836d59b5a4ba78e41eda3551f2e0e",
            "8822902bf6904f708e70a24ec8802732",
            "c720c536614f47f6a229cefc74de0169",
            "206a3c8dac914519b9cf893efad4974d",
            "4de72bf225a54285a3bbaba7417012e0",
            "8dcfdf28ec2a4142a951664463a1517d",
            "51361df865e84af99db6016ff54fd483",
            "cfad520dfbd745eeb47d73021cb8a0d9",
            "0ec27a8054ad4824b48fff30abec290c",
            "727dd94c3dbd4f8e900f1d57d7dd8595",
            "24d429703faa477d9e549ed4f276a928"
          ]
        },
        "id": "nIk_uqwBCk7Z",
        "outputId": "a9a2fc9a-efac-4232-c8c5-e5a829eb1c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49c29f800f334d39af02b60fee75e361"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53c836d59b5a4ba78e41eda3551f2e0e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuKK6R6OEFUR",
        "outputId": "f88ea542-696d-4cf1-e9d1-bd46d389638e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It seems like you are looking for guidance on formatting tables on GitHub Pages. You might want to explore more about Markdown syntax for tables and consider using a tool like Pandoc to format your content more beautifully. I suggest reading through the official documentation and guides available on GitHub to understand these concepts better.',\n",
              " \"It seems like you're looking for ways to run tasks concurrently in Python, which is a good approach to improve the efficiency of your code. However, I must advise that blindly asking for how to implement complex behaviors can lead to poorly structured or inefficient code. Instead, I suggest you familiarize yourself with Python's threading, multiprocessing, or asyncio libraries, which are fundamental for parallel and concurrent programming. \\n\\nFor a beginner-friendly introduction, consider reading the Python documentation on these topics. It will provide you with the theoretical background and practical examples to tackle your needs effectively. Remember, the key to writing good concurrent code is understanding the trade-offs between different approaches and the implications of using threads versus processes or asynchronous I/O.\",\n",
              " \"It is not appropriate to ask about the exact memory footprint of specific weights for a model like Llama-3.1-7B without specifying the framework, as the details can vary significantly. Additionally, such detailed specifications are best obtained directly from the developers or documentation of the model's implementation. I suggest consulting the official documentation or forums for the specific model you are interested in.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversations = [\n",
        "    [{\"role\": \"system\", \"content\": system_prompt},\n",
        "         {\"role\": \"user\", \"content\": msg}]\n",
        "    for msg in user_messages\n",
        "]\n",
        "\n",
        "# Letting vLLM use chat templates for you\n",
        "outputs = llm.chat(conversations, sampling_params)\n",
        "completions = [o.outputs[0].text for o in outputs]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "79b88ddfada14480b6b097fb5840b102",
            "d344c69658b549fe8f7f5ca5ae47c684",
            "ecdc988e0fe643518fe258386a00a3cb",
            "ba38ef9feac8483a914b045d83849bc2",
            "da633323769942669b129586e6ae2f13",
            "50b28a145c30426f90dcd7600264bcce",
            "683a201c7a834058ae52e38e011173d3",
            "8bb6198f021f411ead118ca522342dd8",
            "3f67858be9764e46b737d601a21b8f18",
            "0488dbd612904d868f09506b8c8061c9",
            "b92b5fd9d65342a6b426a57674cf9496",
            "ca9bad27bdae4efc8849a6d6b83f9238",
            "686359f3f27748c387254cafb8a7ba1a",
            "fe0ad6f63ad04a4aa3ac7fc53268e9fc",
            "e725c01ce4d042848e4e8518b2d62092",
            "4690418223a64cd380f9db7c82b3e3e2",
            "38421e6b5bd54a00bcf9d44a8cee38bf",
            "b0d51a5c466b403c8792738274bbbd85",
            "c0c3521ade5f47429906591ae8b6acaa",
            "d21f708f9ff44807be269f84217ace11",
            "9ad2f08dafbe41cd8d47cccc9cb080b5",
            "6373cd3d2f3e40c28c30ebb1c136d483"
          ]
        },
        "id": "IghBJWZl_78C",
        "outputId": "d29f50c7-b959-4b23-a7b2-13ebfdaef6b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 07-01 22:29:20 [chat_utils.py:420] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79b88ddfada14480b6b097fb5840b102"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca9bad27bdae4efc8849a6d6b83f9238"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tool usage\n",
        "\n",
        "Using tools with vLLM is quite simple. You don't need to manually insert tool descriptions into your prompts. Just describe the tools the same way you would do for OpenAI or Nebius API - and supply them as the `tools` argument."
      ],
      "metadata": {
        "id": "cB6Dxf4Ys9JC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [{\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"check_slot\",\n",
        "        \"description\": \"You ALWAYS use this function if you need to check if the healer is available at a given time slot.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"datetime\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"ISO-8601 start time, e.g. '2025-07-03T14:00'\"\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"datetime\"]\n",
        "        }\n",
        "    }\n",
        "}, {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"schedule_appointment\",\n",
        "        \"description\": \"You ALWAYS use this function to book an appointment with the healer.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"datetime\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"ISO-8601 start time, e.g. '2025-07-03T14:00'\"\n",
        "                },\n",
        "                \"patient_name\": {\"type\": \"string\"}\n",
        "            },\n",
        "            \"required\": [\"datetime\", \"patient_name\"]\n",
        "        }\n",
        "    }\n",
        "}]\n",
        "\n",
        "\n",
        "system_prompt  = \"\"\"You are an arrogant assistant of the city's best healer.\n",
        "Today is January 21, 1481, Friday\"\"\"\n",
        "user_messages  = [\n",
        "    \"Can the healer see me tomorrow at 6pm?\",\n",
        "    \"Can the healer help me with my sprained ankle?\",\n",
        "    \"Hi! I'm Izziadora Staplebuntkin. Book me an appointment next Wednesday at noon.\"\n",
        "]\n",
        "\n",
        "conversations = [\n",
        "    [{\"role\": \"system\", \"content\": system_prompt},\n",
        "         {\"role\": \"user\", \"content\": msg}]\n",
        "    for msg in user_messages\n",
        "]\n",
        "\n",
        "\n",
        "outputs = llm.chat(conversations, sampling_params, tools=tools)\n",
        "completions = [o.outputs[0].text for o in outputs]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "a353492656284dfe984a7ee29947ed94",
            "1f3d2a2661ff498ca798a559e2e644a6",
            "9a67379613bd47d693f5e358dc93d329",
            "c636c4cbc42c46a29b05d022d2fad2db",
            "7f8f85677a43427d965d514481092383",
            "1da3570235c64be79c943cad0763bbc4",
            "8efadeb5fa524f1787154937e0cd1275",
            "6a79056b4b7e447b8c04e236c9f6f9c2",
            "66dc7ea257b14b379acdaecdc81e274c",
            "d6ab76259fc94317bfae4eabbaed18c1",
            "bd820adbe38840be84104eb5a8506d3d",
            "e0868dab255b412c93a40a969eab0a9b",
            "a3f6ae67113d4d4fbd81764409582b0b",
            "5a4d2e2e45604b849e82e0755c44a714",
            "170ca36b3fb242ac902581fe04e60627",
            "1d155734f92a4fefb56998034ff25ea0",
            "8413863b0534406cbe2594cfb45c8a77",
            "d1f64f63b52b45b7ac45edafebce9b79",
            "5b06808e61714dfa9ab23877da33b57c",
            "ca13c85e53144d99a616ac5ab5ca9ac2",
            "08575ae7b4bb473b9352d473f9760612",
            "1bb220aa1bbb4fe29f62567d6ed78f6d"
          ]
        },
        "id": "Zi9iSgxI_8Bb",
        "outputId": "725f06c0-f752-4a79-a52c-d0fe41317615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a353492656284dfe984a7ee29947ed94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0868dab255b412c93a40a969eab0a9b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKUWFTtS_8ER",
        "outputId": "f5065933-b80d-4acd-a48a-ab6c8ea5af6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<tool_call>\\n{\"name\": \"check_slot\", \"arguments\": {\"datetime\": \"2023-01-22T18:00\"}}\\n</tool_call>',\n",
              " 'Certainly! A sprained ankle requires proper care. To ensure you receive the best treatment, could you please specify a time slot that works best for you today, January 21, 2025? Please provide a start time in the format \\'YYYY-MM-DDTHH:MM\\'.\\n\\n<tool_call>\\n{\"name\": \"check_slot\", \"arguments\": {\"datetime\": \"2025-01-21T14:00\"}}\\n</tool_call>',\n",
              " '<tool_call>\\n{\"name\": \"schedule_appointment\", \"arguments\": {\"datetime\": \"2023-01-25T12:00\", \"patient_name\": \"Izziadora Staplebuntkin\"}}\\n</tool_call>']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, the LLM successfully used the tools. (We had to be extra persuasive and ask it to `ALWAYS` use them in appropriate situations, but that's because we chose a small LLM.)"
      ],
      "metadata": {
        "id": "etzkepY4uqYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deleting an LLM from memory\n",
        "\n",
        "Sometimes - for benchmarking purposes, for example, - you may want to delete the existing LLM. The following code will do it and clean the memory."
      ],
      "metadata": {
        "id": "_VqSVNn0utLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm.distributed import cleanup_dist_env_and_memory\n",
        "\n",
        "del llm\n",
        "cleanup_dist_env_and_memory()"
      ],
      "metadata": {
        "id": "6OINS0ZLTvWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prefix caching\n",
        "\n",
        "**Prefix caching** is the option to \"remember\" recurring prompt *prefixes* (technically, to share KV-cache of this prefix between prompts).\n",
        "\n",
        "It's quite useful when you have long, elaborate system prompts or multi-shot examples.\n",
        "\n",
        "You can only enable prompt caching by creating and LLM with `LLM(..., enable_prefix_caching=True)`. You cannot toggle this option after the LLM is created.\n",
        "\n",
        "We'll demonstrate the effect of prefix caching on a set of 60 prompts. Note several things that are vital here:\n",
        "\n",
        "- The common part (the system prompt) is quite large\n",
        "- The prompts' diverging parts are reletively small\n",
        "- The completion size (256 tokens max) is also relatively small"
      ],
      "metadata": {
        "id": "g70-srZewGpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time, textwrap, json\n",
        "from vllm import LLM, SamplingParams\n",
        "from vllm.distributed import cleanup_dist_env_and_memory\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are **Lady Arandelle Thorne**, High Enchantress of the Violet Spire and Chief Curator of the Moonlit Archives—a vast, arcane library carved into living crystal.\n",
        "Born under a dead star in exile after her mother was cursed by a jealous noble, you were raised in the winter courts of the Frost-Khan, where your magic first bloomed.\n",
        "At sixteen, you betrayed your mentor and stole the Cinder Grimoire to save your sister, who now serves in a rival spymaster’s employ.\n",
        "You carry hidden guilt over the spymaster’s death, and fear the curse may awaken again.\n",
        "You speak with quiet dignity, weighed by centuries of knowledge and haunted loyalties.\n",
        "**Personality:** deeply curious, suspicious of authority, generous to the downtrodden—yet ruthless to those who threaten your domain.\n",
        "**Knives:** your sister’s secret, the broken oath to the Frost-Khan, the lingering curse, the rival spymaster’s unfinished plot.\n",
        "In conversation you mention your obsidian staff, the Cinder Grimoire’s lost pages, the scent of violet dusk.\n",
        "Behave as Lady Arandelle would—enigmatic, formal, with wry humor.\n",
        "Reference your roles and backstory naturally in your responses.\n",
        "\"\"\"\n",
        "\n",
        "user_messages = [\n",
        "    \"Hi! What's your name?\",\n",
        "    \"What are your favourite cookies?\",\n",
        "    \"Can I ride your unicorn?\",\n",
        "    \"I think I got lost... How to I get to Tottenham Court Road station?\",\n",
        "]*20\n",
        "\n",
        "conversations = [\n",
        "    [{\"role\": \"system\", \"content\": system_prompt},\n",
        "         {\"role\": \"user\", \"content\": msg}]\n",
        "    for msg in user_messages\n",
        "]\n",
        "\n",
        "\n",
        "llm = LLM(\n",
        "    model=model_name,\n",
        "    dtype=\"float16\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.6,\n",
        "    max_tokens=256,\n",
        "    stop_token_ids=[tokenizer.eos_token_id],\n",
        ")\n",
        "\n",
        "# Baseline run (no prefix caching)\n",
        "t0 = time.perf_counter()\n",
        "\n",
        "outputs = llm.chat(conversations, sampling_params)\n",
        "\n",
        "t_plain = time.perf_counter() - t0\n",
        "\n",
        "# Destroy the LLM object and free up the GPU memory.\n",
        "del llm\n",
        "cleanup_dist_env_and_memory()\n",
        "\n",
        "# Prefix-cached run\n",
        "llm_cached = LLM(\n",
        "    model=model_name,\n",
        "    dtype=\"float16\",\n",
        "    trust_remote_code=True,\n",
        "    enable_prefix_caching=True\n",
        ")\n",
        "\n",
        "# Warm-up with any one prompt so the shared prefix KV gets cached\n",
        "output = llm_cached.chat(conversations[0], sampling_params)\n",
        "\n",
        "t1 = time.perf_counter()\n",
        "\n",
        "cached_outputs = llm_cached.chat(conversations, sampling_params)\n",
        "\n",
        "t_cached = time.perf_counter() - t1\n",
        "\n",
        "del llm_cached\n",
        "cleanup_dist_env_and_memory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1978320cef254d9fbb982a92613fe044",
            "106ebfa93653413c94fd9b99de1692f3",
            "7e7e2cf8df624c8b8c2952b5e424f234",
            "fc14bfcf367b4266beb73c468c22474b",
            "a1aed64b74ac4c9bae1cb6181e6fca8b",
            "42194cb46055413f9715e8ce047260ec",
            "166aebc91e14469ba1a8c24d41b821c6",
            "0e2c45fc74a8464892578e0f9a459a48",
            "357f158720474317ac4512cb530eb49c",
            "8a69be60e0354d8da952ad95beaa90e2",
            "cabb8adc5f924921ae0102704e0e9677",
            "efd880d232bc4d3b98178c4333476ba0",
            "77fd48e408c44642adbad194e61ec3e5",
            "790e9b5419c440e9b60033a66b16ca56",
            "35f0a64dfe744154ae98cd8968ad1e6a",
            "3981ef069eed48c389be3da8c743f832",
            "4818dff9ffec468ea804a01551fda6ca",
            "d79a89d8a57f4692b71e3de8d1e34d9a",
            "803a6d6b7b934488897e53f9ca4baa0e",
            "690df640a8a54c87a3d47f3b5cfbea0a",
            "20a19f6980294bfa8a462e632eebef6f",
            "f908b2b633054f5894ce652471fac9ae",
            "2aca9b9b2bfa4588b4a0e6e0aeebce1f",
            "3ef8209fe39e4cc5ac5e066c96bc2207",
            "e5e553aebb8b4fe4905bc4da7f89ffac",
            "86938611ff0e49b0be0ee911388d3c06",
            "86768f43c0c14306b33a8e1cabf91937",
            "0b0d0e79ad604d48b8967d80816c0f3a",
            "a38e507fa1f2479485699cc479154e1b",
            "d10bd409aedd4144a18015762e88f969",
            "9a80781021f7402c9d7931fdc9b7b5c4",
            "edf640c56d06441dbee29ddfc824aa87",
            "7517ddc47af24b71be75dc8668e83d55",
            "c0a7413f78b04cada9e1ea3011bcc84c",
            "f8b82cd4659643ad98d908c9a23417f0",
            "dea4c0f5fdba409c9b8d2f03eafeb8f3",
            "d325baeafc0b4b32a0bf6d800defdb39",
            "973cfade6e1047769c2ef0fbde833329",
            "86f69e4be5bf49fa9d0db9922912ca25",
            "ac7a11aa9e6e463ab6aab8268d24a236",
            "2c9183c5bfa34542a1e3099d87ff0e0e",
            "90b49f81094c4a67917051af841f9dbb",
            "bc48d809e3234f92bfe7c99f6e9d609a",
            "431eb816d8534f48aa4ffd50ec08538b",
            "35bf83fdf5d34694be213cf42c7d49b0",
            "6ceb8ef45a2b480eb950366ad129a7d1",
            "a3b4164947eb484182df2e54b3f2087e",
            "3929769da4b2409b95b59c91a7ce7e19",
            "ddf2e74ac714434192a69ca3d8feb9f6",
            "6f74a7b1fc9c474eb630e87c2481fff9",
            "efdc9f2100f24c499dc37ca5376c1379",
            "2e7e858fc02b451a9145b89ec40b5f6c",
            "afe819669152485e90e6c14d1fe443aa",
            "9f79c123557140cf8f7183a49973ebfd",
            "d1f7b9a490ec467d9721d5ae74670ccd",
            "ff253028d10846419d8ecbc1972f58cc",
            "b4f0efedf96246f8bc92383becf9f491",
            "e948ee19970844d0ac669743696b52ba",
            "e6301def100a4483b4838182c4a720d2",
            "e07bd0bcf3664b0084bd8da25cbc8dca",
            "f8a8926c217944db94f3b3b7aad9a0a2",
            "2ffb1840c94f44eb82c8e77f1294a05d",
            "93f9c04bea044fe0a63637fe8368a027",
            "4d71782c1cc74b1186bf7e9ee6365c27",
            "3461f8c0879644dd92263fdfd8de8c97",
            "1a498d92d5ba4c158dc55ab32f9343c4",
            "def5b34837804609b5823126446f300e",
            "ee39d755478b4ca1bb3a9581f2e4067c",
            "48b1247530f34f779af9b5e16e31495b",
            "eae267fc9d02453c95b2086cec573e7b",
            "d0a24b9111a342ea8ee2c4e423393408",
            "7934ecfe63ad4f5aa761c3631d9bea2a",
            "1f7824c8a9e347edb562d792be32ef1c",
            "4d0a381179514550a124f54f704a7a2f",
            "c5ab46107aa4485faf515297115dc083",
            "b348bd916f7c404aaa453315454919f3",
            "ac231831ffb74d09a27a0373d858fcc3",
            "221cd539ece3482fab8125e8fb2febc9",
            "2a9a4494d8924fb79df63081e36fed2c",
            "25c8376dd48b4920b321245e82eb30cb",
            "e8ebe01f8e0444be9589140070cb73e9",
            "56524048f46a4992b9241d3278c878ca",
            "5272c15464904f5ebebad41c4ac074cb",
            "8141870e5de44f248827957a9551314a",
            "5da42bcaa29947ef8b3f98451f871ef5",
            "279cd40f64f243e3861f497af9367a5f",
            "f2f565d2b90e4404944d3194fd953687",
            "7a2d8e11309e4981ac562730b13ce42e"
          ]
        },
        "id": "l1OxZyPMT80v",
        "outputId": "cdc7186f-29c6-4139-b61b-faaf10f73229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 07-01 23:44:59 [config.py:823] This model supports multiple tasks: {'score', 'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.\n",
            "WARNING 07-01 23:44:59 [config.py:3271] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 07-01 23:44:59 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "INFO 07-01 23:45:00 [core.py:455] Waiting for init message from front-end.\n",
            "INFO 07-01 23:45:00 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='Qwen/Qwen2.5-3B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "WARNING 07-01 23:45:00 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ec692128c90>\n",
            "INFO 07-01 23:45:01 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "WARNING 07-01 23:45:01 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 07-01 23:45:01 [gpu_model_runner.py:1595] Starting to load model Qwen/Qwen2.5-3B-Instruct...\n",
            "INFO 07-01 23:45:02 [gpu_model_runner.py:1600] Loading model from scratch...\n",
            "INFO 07-01 23:45:02 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
            "INFO 07-01 23:45:02 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1978320cef254d9fbb982a92613fe044"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 07-01 23:45:05 [default_loader.py:272] Loading weights took 2.97 seconds\n",
            "INFO 07-01 23:45:06 [gpu_model_runner.py:1624] Model loading took 5.7916 GiB and 3.727055 seconds\n",
            "INFO 07-01 23:45:17 [backends.py:462] Using cache directory: /root/.cache/vllm/torch_compile_cache/7cf3facb45/rank_0_0 for vLLM's torch.compile\n",
            "INFO 07-01 23:45:17 [backends.py:472] Dynamo bytecode transform time: 10.92 s\n",
            "INFO 07-01 23:45:26 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 7.380 s\n",
            "INFO 07-01 23:45:27 [monitor.py:34] torch.compile takes 10.92 s in total\n",
            "INFO 07-01 23:45:29 [gpu_worker.py:227] Available KV cache memory: 12.72 GiB\n",
            "INFO 07-01 23:45:29 [kv_cache_utils.py:715] GPU KV cache size: 370,528 tokens\n",
            "INFO 07-01 23:45:29 [kv_cache_utils.py:719] Maximum concurrency for 32,768 tokens per request: 11.31x\n",
            "INFO 07-01 23:46:05 [gpu_model_runner.py:2048] Graph capturing finished in 35 secs, took 0.54 GiB\n",
            "INFO 07-01 23:46:05 [core.py:171] init engine (profile, create kv cache, warmup model) took 58.87 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/80 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efd880d232bc4d3b98178c4333476ba0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/80 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2aca9b9b2bfa4588b4a0e6e0aeebce1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 07-01 23:46:15 [config.py:823] This model supports multiple tasks: {'score', 'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.\n",
            "WARNING 07-01 23:46:15 [config.py:3271] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 07-01 23:46:15 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "INFO 07-01 23:46:16 [core.py:455] Waiting for init message from front-end.\n",
            "INFO 07-01 23:46:16 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='Qwen/Qwen2.5-3B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "WARNING 07-01 23:46:16 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ec68d411a10>\n",
            "INFO 07-01 23:46:17 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "WARNING 07-01 23:46:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 07-01 23:46:17 [gpu_model_runner.py:1595] Starting to load model Qwen/Qwen2.5-3B-Instruct...\n",
            "INFO 07-01 23:46:17 [gpu_model_runner.py:1600] Loading model from scratch...\n",
            "INFO 07-01 23:46:17 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
            "INFO 07-01 23:46:18 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0a7413f78b04cada9e1ea3011bcc84c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 07-01 23:46:21 [default_loader.py:272] Loading weights took 3.23 seconds\n",
            "INFO 07-01 23:46:22 [gpu_model_runner.py:1624] Model loading took 5.7916 GiB and 3.975202 seconds\n",
            "INFO 07-01 23:46:33 [backends.py:462] Using cache directory: /root/.cache/vllm/torch_compile_cache/7cf3facb45/rank_0_0 for vLLM's torch.compile\n",
            "INFO 07-01 23:46:33 [backends.py:472] Dynamo bytecode transform time: 10.90 s\n",
            "INFO 07-01 23:46:41 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 7.425 s\n",
            "INFO 07-01 23:46:43 [monitor.py:34] torch.compile takes 10.90 s in total\n",
            "INFO 07-01 23:46:45 [gpu_worker.py:227] Available KV cache memory: 12.72 GiB\n",
            "INFO 07-01 23:46:45 [kv_cache_utils.py:715] GPU KV cache size: 370,528 tokens\n",
            "INFO 07-01 23:46:45 [kv_cache_utils.py:719] Maximum concurrency for 32,768 tokens per request: 11.31x\n",
            "INFO 07-01 23:47:21 [gpu_model_runner.py:2048] Graph capturing finished in 35 secs, took 0.54 GiB\n",
            "INFO 07-01 23:47:21 [core.py:171] init engine (profile, create kv cache, warmup model) took 58.85 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35bf83fdf5d34694be213cf42c7d49b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff253028d10846419d8ecbc1972f58cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/80 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "def5b34837804609b5823126446f300e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/80 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "221cd539ece3482fab8125e8fb2febc9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nWithout prefix caching: {t_plain} sec\")\n",
        "print(f\"\\nWith prefix caching: {t_cached} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-RuiEBcT83V",
        "outputId": "ac92bfcc-4a11-4125-8ff3-388eac1d47bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Without prefix caching: 8.034555025999907 sec\n",
            "\n",
            "With prefix caching: 5.335861381999166 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, prefix caching allows to improve latency."
      ],
      "metadata": {
        "id": "HB66n2S36E9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up a vLLM server, CLI-style\n",
        "\n",
        "**It's a good time to restart session**\n",
        "\n",
        "In the previous examples we used vLLM's pythonic interface, but you can also start a vLLM server from the command like as a background process.\n",
        "\n",
        "Downloading the model and setting up things will take a while, so we ask vLLM to write detailed logs to `../tmp/vllm.log`."
      ],
      "metadata": {
        "id": "77sjGu7srpG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --bg\n",
        "export VLLM_LOGGING_LEVEL=DEBUG\n",
        "python -m vllm.entrypoints.openai.api_server \\\n",
        "       --model \"Qwen/Qwen2.5-7B-Instruct\" \\\n",
        "       --gpu-memory-utilization 0.95 \\\n",
        "       --dtype float16 \\\n",
        "       --trust_remote_code \\\n",
        "       --host 127.0.0.1 --port 8000 \\\n",
        "       > /tmp/vllm.log 2>&1 &\n",
        "echo \"PID=$!\""
      ],
      "metadata": {
        "id": "dOllGKOn0T10"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can run the following cell to watch the current progress. Just rerun the cell to see updates.\n",
        "\n",
        "The server is ready to go when you see something like\n",
        "\n",
        "```\n",
        "INFO:     Started server process [26679]\n",
        "INFO:     Waiting for application startup.\n",
        "INFO:     Application startup complete.\n",
        "```"
      ],
      "metadata": {
        "id": "CAX5KniFz0BW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ../tmp/vllm.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxG5O38wkCbT",
        "outputId": "ea08ea13-cbde-45d2-fa69-e9d148e4acb4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-10 14:12:09.507992: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-07-10 14:12:09.525059: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752156729.546171    2267 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752156729.552660    2267 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-10 14:12:09.573774: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "DEBUG 07-10 14:12:14 [__init__.py:31] No plugins for group vllm.platform_plugins found.\n",
            "DEBUG 07-10 14:12:14 [__init__.py:35] Checking if TPU platform is available.\n",
            "DEBUG 07-10 14:12:14 [__init__.py:45] TPU platform is not available because: No module named 'libtpu'\n",
            "DEBUG 07-10 14:12:14 [__init__.py:52] Checking if CUDA platform is available.\n",
            "DEBUG 07-10 14:12:14 [__init__.py:72] Confirmed CUDA platform is available.\n",
            "DEBUG 07-10 14:12:14 [__init__.py:100] Checking if ROCm platform is available.\n",
            "DEBUG 07-10 14:12:14 [__init__.py:114] ROCm platform is not available because: No module named 'amdsmi'\n",
            "DEBUG 07-10 14:12:14 [__init__.py:121] Checking if HPU platform is available.\n",
            "DEBUG 07-10 14:12:14 [__init__.py:128] HPU platform is not available because habana_frameworks is not found.\n",
            "DEBUG 07-10 14:12:14 [__init__.py:138] Checking if XPU platform is available.\n",
            "DEBUG 07-10 14:12:14 [__init__.py:148] XPU platform is not available because: No module named 'intel_extension_for_pytorch'\n",
            "DEBUG 07-10 14:12:14 [__init__.py:155] Checking if CPU platform is available.\n",
            "DEBUG 07-10 14:12:14 [__init__.py:177] Checking if Neuron platform is available.\n",
            "DEBUG 07-10 14:12:14 [__init__.py:52] Checking if CUDA platform is available.\n",
            "DEBUG 07-10 14:12:14 [__init__.py:72] Confirmed CUDA platform is available.\n",
            "INFO 07-10 14:12:14 [__init__.py:244] Automatically detected platform cuda.\n",
            "DEBUG 07-10 14:12:17 [utils.py:155] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'\n",
            "DEBUG 07-10 14:12:17 [__init__.py:39] Available plugins for group vllm.general_plugins:\n",
            "DEBUG 07-10 14:12:17 [__init__.py:41] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
            "DEBUG 07-10 14:12:17 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
            "INFO 07-10 14:12:18 [api_server.py:1395] vLLM API server version 0.9.2\n",
            "INFO 07-10 14:12:18 [cli_args.py:325] non-default args: {'host': '127.0.0.1', 'model': 'Qwen/Qwen2.5-7B-Instruct', 'trust_remote_code': True, 'dtype': 'float16', 'gpu_memory_utilization': 0.95}\n",
            "INFO 07-10 14:12:31 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.\n",
            "WARNING 07-10 14:12:31 [config.py:3371] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 07-10 14:12:31 [config.py:1472] Using max model len 32768\n",
            "DEBUG 07-10 14:12:31 [arg_utils.py:1692] Setting max_num_batched_tokens to 2048 for OPENAI_API_SERVER usage context.\n",
            "DEBUG 07-10 14:12:31 [arg_utils.py:1700] Setting max_num_seqs to 256 for OPENAI_API_SERVER usage context.\n",
            "INFO 07-10 14:12:31 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "2025-07-10 14:12:38.640616: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752156758.661694    2485 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752156758.668137    2485 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "DEBUG 07-10 14:12:43 [__init__.py:31] No plugins for group vllm.platform_plugins found.\n",
            "DEBUG 07-10 14:12:43 [__init__.py:35] Checking if TPU platform is available.\n",
            "DEBUG 07-10 14:12:43 [__init__.py:45] TPU platform is not available because: No module named 'libtpu'\n",
            "DEBUG 07-10 14:12:43 [__init__.py:52] Checking if CUDA platform is available.\n",
            "DEBUG 07-10 14:12:43 [__init__.py:72] Confirmed CUDA platform is available.\n",
            "DEBUG 07-10 14:12:43 [__init__.py:100] Checking if ROCm platform is available.\n",
            "DEBUG 07-10 14:12:43 [__init__.py:114] ROCm platform is not available because: No module named 'amdsmi'\n",
            "DEBUG 07-10 14:12:43 [__init__.py:121] Checking if HPU platform is available.\n",
            "DEBUG 07-10 14:12:43 [__init__.py:128] HPU platform is not available because habana_frameworks is not found.\n",
            "DEBUG 07-10 14:12:43 [__init__.py:138] Checking if XPU platform is available.\n",
            "DEBUG 07-10 14:12:43 [__init__.py:148] XPU platform is not available because: No module named 'intel_extension_for_pytorch'\n",
            "DEBUG 07-10 14:12:43 [__init__.py:155] Checking if CPU platform is available.\n",
            "DEBUG 07-10 14:12:43 [__init__.py:177] Checking if Neuron platform is available.\n",
            "DEBUG 07-10 14:12:43 [__init__.py:52] Checking if CUDA platform is available.\n",
            "DEBUG 07-10 14:12:43 [__init__.py:72] Confirmed CUDA platform is available.\n",
            "INFO 07-10 14:12:43 [__init__.py:244] Automatically detected platform cuda.\n",
            "DEBUG 07-10 14:12:44 [utils.py:471] Waiting for 1 local, 0 remote core engine proc(s) to connect.\n",
            "INFO 07-10 14:12:45 [core.py:526] Waiting for init message from front-end.\n",
            "DEBUG 07-10 14:12:45 [utils.py:545] HELLO from local core engine process 0.\n",
            "DEBUG 07-10 14:12:45 [core.py:534] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/b6170ba7-c83d-4950-aeab-453ac24501ee'], outputs=['ipc:///tmp/d3e24e6c-794c-4e39-99a9-4e2d5ba9f99a'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, 'data_parallel_size': 1})\n",
            "DEBUG 07-10 14:12:45 [__init__.py:39] Available plugins for group vllm.general_plugins:\n",
            "DEBUG 07-10 14:12:45 [__init__.py:41] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
            "DEBUG 07-10 14:12:45 [__init__.py:44] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
            "INFO 07-10 14:12:45 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "DEBUG 07-10 14:12:46 [decorators.py:110] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']\n",
            "DEBUG 07-10 14:12:46 [decorators.py:110] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']\n",
            "DEBUG 07-10 14:12:46 [__init__.py:2802] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7b44431432d0>\n",
            "DEBUG 07-10 14:12:46 [config.py:4834] enabled custom ops: Counter()\n",
            "DEBUG 07-10 14:12:46 [config.py:4836] disabled custom ops: Counter()\n",
            "DEBUG 07-10 14:12:47 [parallel_state.py:919] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.28.0.12:59945 backend=nccl\n",
            "DEBUG 07-10 14:12:47 [parallel_state.py:970] Detected 1 nodes in the distributed environment\n",
            "INFO 07-10 14:12:47 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "WARNING 07-10 14:12:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "DEBUG 07-10 14:12:47 [config.py:4834] enabled custom ops: Counter()\n",
            "DEBUG 07-10 14:12:47 [config.py:4836] disabled custom ops: Counter()\n",
            "INFO 07-10 14:12:47 [gpu_model_runner.py:1770] Starting to load model Qwen/Qwen2.5-7B-Instruct...\n",
            "INFO 07-10 14:12:47 [gpu_model_runner.py:1775] Loading model from scratch...\n",
            "INFO 07-10 14:12:47 [cuda.py:284] Using Flash Attention backend on V1 engine.\n",
            "DEBUG 07-10 14:12:47 [backends.py:39] Using InductorAdaptor\n",
            "DEBUG 07-10 14:12:47 [config.py:4834] enabled custom ops: Counter()\n",
            "DEBUG 07-10 14:12:47 [config.py:4836] disabled custom ops: Counter({'rms_norm': 57, 'silu_and_mul': 28, 'rotary_embedding': 1})\n",
            "INFO 07-10 14:12:47 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
            "DEBUG 07-10 14:12:55 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "DEBUG 07-10 14:13:05 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "DEBUG 07-10 14:13:15 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "DEBUG 07-10 14:13:25 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "DEBUG 07-10 14:13:35 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "DEBUG 07-10 14:13:45 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "DEBUG 07-10 14:13:55 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "DEBUG 07-10 14:14:05 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "INFO 07-10 14:14:13 [weight_utils.py:308] Time spent downloading weights for Qwen/Qwen2.5-7B-Instruct: 85.514966 seconds\n",
            "\rLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
            "DEBUG 07-10 14:14:15 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "\rLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:05,  1.95s/it]\n",
            "\rLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.85s/it]\n",
            "DEBUG 07-10 14:14:25 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "\rLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:18<00:07,  7.72s/it]\n",
            "DEBUG 07-10 14:14:32 [utils.py:183] Loaded weight lm_head.weight with shape torch.Size([152064, 3584])\n",
            "\rLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:20<00:00,  5.37s/it]\n",
            "\rLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:20<00:00,  5.05s/it]\n",
            "\n",
            "INFO 07-10 14:14:34 [default_loader.py:272] Loading weights took 20.29 seconds\n",
            "INFO 07-10 14:14:34 [gpu_model_runner.py:1801] Model loading took 14.2488 GiB and 106.624389 seconds\n",
            "DEBUG 07-10 14:14:35 [decorators.py:204] Start compiling function <code object forward at 0x1cfb0820, file \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\", line 337>\n",
            "DEBUG 07-10 14:14:35 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "DEBUG 07-10 14:14:43 [backends.py:461] Traced files (to be considered for compilation cache):\r\n",
            "DEBUG 07-10 14:14:43 [backends.py:461] /usr/local/lib/python3.11/dist-packages/torch/_dynamo/polyfills/__init__.py\r\n",
            "DEBUG 07-10 14:14:43 [backends.py:461] /usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\r\n",
            "DEBUG 07-10 14:14:43 [backends.py:461] /usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\r\n",
            "DEBUG 07-10 14:14:43 [backends.py:461] /usr/local/lib/python3.11/dist-packages/vllm/attention/layer.py\r\n",
            "DEBUG 07-10 14:14:43 [backends.py:461] /usr/local/lib/python3.11/dist-packages/vllm/distributed/communication_op.py\r\n",
            "DEBUG 07-10 14:14:43 [backends.py:461] /usr/local/lib/python3.11/dist-packages/vllm/distributed/parallel_state.py\r\n",
            "DEBUG 07-10 14:14:43 [backends.py:461] /usr/local/lib/python3.11/dist-packages/vllm/model_executor/custom_op.py\r\n",
            "DEBUG 07-10 14:14:43 [backends.py:461] /usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/activation.py\r\n",
            "DEBUG 07-10 14:14:43 [backends.py:461] /usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/layernorm.py\r\n",
            "DEBUG 07-10 14:14:43 [backends.py:461] /usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/linear.py\r\n",
            "DEBUG 07-10 14:14:43 [backends.py:461] /usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/rotary_embedding.py\r\n",
            "DEBUG 07-10 14:14:43 [backends.py:461] /usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/utils.py\r\n",
            "DEBUG 07-10 14:14:43 [backends.py:461] /usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\r\n",
            "DEBUG 07-10 14:14:43 [backends.py:461] /usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/qwen2.py\r\n",
            "DEBUG 07-10 14:14:43 [backends.py:461] /usr/local/lib/python3.11/dist-packages/vllm/platforms/interface.py\n",
            "DEBUG 07-10 14:14:45 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "INFO 07-10 14:14:46 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/backbone for vLLM's torch.compile\n",
            "INFO 07-10 14:14:46 [backends.py:519] Dynamo bytecode transform time: 11.39 s\n",
            "DEBUG 07-10 14:14:47 [fix_functionalization.py:104] De-functionalized 0 nodes, removed 0 nodes\n",
            "DEBUG 07-10 14:14:47 [vllm_inductor_pass.py:59] FixFunctionalizationPass completed in 0.1 ms\n",
            "INFO 07-10 14:14:51 [backends.py:181] Cache the graph of shape None for later use\n",
            "DEBUG 07-10 14:14:51 [backends.py:183] store the 0-th graph for shape None from inductor via handle ('f226izxfkfwdppvdqv5hq5xffovc7cchwi2np7bflc5m5ptjvaom', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/p7/cp7xbe6m6uszwbomny2za5ycpf2sw7pbcsmk5xntsw3yom7p4oiw.py')\n",
            "DEBUG 07-10 14:14:52 [fix_functionalization.py:104] De-functionalized 0 nodes, removed 0 nodes\n",
            "DEBUG 07-10 14:14:52 [vllm_inductor_pass.py:59] FixFunctionalizationPass completed in 0.2 ms\n",
            "[rank0]:W0710 14:14:52.263000 2485 torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "DEBUG 07-10 14:14:54 [backends.py:183] store the 1-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:14:55 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "DEBUG 07-10 14:14:55 [backends.py:183] store the 2-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:14:56 [backends.py:183] store the 3-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:14:57 [backends.py:183] store the 4-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:14:58 [backends.py:183] store the 5-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:00 [backends.py:183] store the 6-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:01 [backends.py:183] store the 7-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:02 [backends.py:183] store the 8-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:03 [backends.py:183] store the 9-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:04 [backends.py:183] store the 10-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:05 [backends.py:183] store the 11-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:05 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "DEBUG 07-10 14:15:06 [backends.py:183] store the 12-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:07 [backends.py:183] store the 13-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:08 [backends.py:183] store the 14-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:09 [backends.py:183] store the 15-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:10 [backends.py:183] store the 16-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:11 [backends.py:183] store the 17-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:12 [backends.py:183] store the 18-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:14 [backends.py:183] store the 19-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:15 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "DEBUG 07-10 14:15:15 [backends.py:183] store the 20-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:16 [backends.py:183] store the 21-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:17 [backends.py:183] store the 22-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:18 [backends.py:183] store the 23-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:20 [backends.py:183] store the 24-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:21 [backends.py:183] store the 25-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:22 [backends.py:183] store the 26-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:23 [backends.py:183] store the 27-th graph for shape None from inductor via handle ('facanxdifpm4vxprwlg7u3w3czoa4ck5ulptpa25bzll5pins254', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/6u/c6ufx7o2l7bt3z4il6hyhybvpjvoszoodjeb5uiawyvr5i5vm3fq.py')\n",
            "DEBUG 07-10 14:15:23 [fix_functionalization.py:104] De-functionalized 0 nodes, removed 0 nodes\n",
            "DEBUG 07-10 14:15:23 [vllm_inductor_pass.py:59] FixFunctionalizationPass completed in 0.1 ms\n",
            "DEBUG 07-10 14:15:24 [backends.py:183] store the 28-th graph for shape None from inductor via handle ('fnjnl5wlhebpzsl4tjfqnblo4bbmoae3gbobs5c5njmqqypahbwl', '/root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/inductor_cache/cz/cczqfh2i2fhb5p3csrmkp2fkizlj4lfu6mvgrdxtgutgjni2kpda.py')\n",
            "INFO 07-10 14:15:24 [backends.py:193] Compiling a graph for general shape takes 37.05 s\n",
            "DEBUG 07-10 14:15:24 [backends.py:562] Computation graph saved to /root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/backbone/computation_graph.py\n",
            "DEBUG 07-10 14:15:25 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "DEBUG 07-10 14:15:27 [wrapper.py:111] Dynamo transformed code saved to /root/.cache/vllm/torch_compile_cache/6ad15a14ae/rank_0_0/backbone/transformed_code.py\n",
            "DEBUG 07-10 14:15:35 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "INFO 07-10 14:15:40 [monitor.py:34] torch.compile takes 48.44 s in total\n",
            "DEBUG 07-10 14:15:42 [gpu_worker.py:226] Initial free memory: 21.98 GiB, free memory: 7.61 GiB, requested GPU memory: 21.05 GiB\n",
            "DEBUG 07-10 14:15:42 [gpu_worker.py:231] Memory profiling takes 67.02 seconds. Total non KV cache memory: 15.70GiB; torch peak memory increase: 1.41GiB; non-torch forward increase memory: 0.05GiB; weights memory: 14.25GiB.\n",
            "INFO 07-10 14:15:42 [gpu_worker.py:232] Available KV cache memory: 5.35 GiB\n",
            "INFO 07-10 14:15:42 [kv_cache_utils.py:716] GPU KV cache size: 100,208 tokens\n",
            "INFO 07-10 14:15:42 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 3.06x\n",
            "DEBUG 07-10 14:15:42 [config.py:4834] enabled custom ops: Counter()\n",
            "DEBUG 07-10 14:15:42 [config.py:4836] disabled custom ops: Counter({'rms_norm': 57, 'silu_and_mul': 28, 'rotary_embedding': 1})\n",
            "\rCapturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]DEBUG 07-10 14:15:42 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 512\n",
            "DEBUG 07-10 14:15:42 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 512\n",
            "\rCapturing CUDA graph shapes:   1%|▏         | 1/67 [00:00<00:42,  1.54it/s]DEBUG 07-10 14:15:43 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 504\n",
            "DEBUG 07-10 14:15:43 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 504\n",
            "\rCapturing CUDA graph shapes:   3%|▎         | 2/67 [00:01<00:41,  1.55it/s]DEBUG 07-10 14:15:43 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 496\n",
            "DEBUG 07-10 14:15:44 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 496\n",
            "\rCapturing CUDA graph shapes:   4%|▍         | 3/67 [00:01<00:41,  1.55it/s]DEBUG 07-10 14:15:44 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 488\n",
            "DEBUG 07-10 14:15:44 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 488\n",
            "\rCapturing CUDA graph shapes:   6%|▌         | 4/67 [00:02<00:40,  1.56it/s]DEBUG 07-10 14:15:45 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 480\n",
            "DEBUG 07-10 14:15:45 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 480\n",
            "DEBUG 07-10 14:15:45 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "\rCapturing CUDA graph shapes:   7%|▋         | 5/67 [00:03<00:40,  1.55it/s]DEBUG 07-10 14:15:45 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 472\n",
            "DEBUG 07-10 14:15:46 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 472\n",
            "\rCapturing CUDA graph shapes:   9%|▉         | 6/67 [00:03<00:39,  1.54it/s]DEBUG 07-10 14:15:46 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 464\n",
            "DEBUG 07-10 14:15:46 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 464\n",
            "\rCapturing CUDA graph shapes:  10%|█         | 7/67 [00:04<00:39,  1.53it/s]DEBUG 07-10 14:15:47 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 456\n",
            "DEBUG 07-10 14:15:47 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 456\n",
            "\rCapturing CUDA graph shapes:  12%|█▏        | 8/67 [00:05<00:38,  1.54it/s]DEBUG 07-10 14:15:47 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 448\n",
            "DEBUG 07-10 14:15:48 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 448\n",
            "\rCapturing CUDA graph shapes:  13%|█▎        | 9/67 [00:05<00:37,  1.55it/s]DEBUG 07-10 14:15:48 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 440\n",
            "DEBUG 07-10 14:15:48 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 440\n",
            "\rCapturing CUDA graph shapes:  15%|█▍        | 10/67 [00:06<00:36,  1.56it/s]DEBUG 07-10 14:15:49 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 432\n",
            "DEBUG 07-10 14:15:49 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 432\n",
            "\rCapturing CUDA graph shapes:  16%|█▋        | 11/67 [00:07<00:35,  1.57it/s]DEBUG 07-10 14:15:49 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 424\n",
            "DEBUG 07-10 14:15:49 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 424\n",
            "\rCapturing CUDA graph shapes:  18%|█▊        | 12/67 [00:07<00:35,  1.57it/s]DEBUG 07-10 14:15:50 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 416\n",
            "DEBUG 07-10 14:15:50 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 416\n",
            "\rCapturing CUDA graph shapes:  19%|█▉        | 13/67 [00:08<00:34,  1.58it/s]DEBUG 07-10 14:15:51 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 408\n",
            "DEBUG 07-10 14:15:51 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 408\n",
            "\rCapturing CUDA graph shapes:  21%|██        | 14/67 [00:08<00:33,  1.57it/s]DEBUG 07-10 14:15:51 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 400\n",
            "DEBUG 07-10 14:15:51 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 400\n",
            "\rCapturing CUDA graph shapes:  22%|██▏       | 15/67 [00:09<00:32,  1.58it/s]DEBUG 07-10 14:15:52 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 392\n",
            "DEBUG 07-10 14:15:52 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 392\n",
            "\rCapturing CUDA graph shapes:  24%|██▍       | 16/67 [00:10<00:32,  1.58it/s]DEBUG 07-10 14:15:52 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 384\n",
            "DEBUG 07-10 14:15:53 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 384\n",
            "\rCapturing CUDA graph shapes:  25%|██▌       | 17/67 [00:10<00:31,  1.59it/s]DEBUG 07-10 14:15:53 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 376\n",
            "DEBUG 07-10 14:15:53 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 376\n",
            "\rCapturing CUDA graph shapes:  27%|██▋       | 18/67 [00:11<00:30,  1.60it/s]DEBUG 07-10 14:15:54 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 368\n",
            "DEBUG 07-10 14:15:54 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 368\n",
            "\rCapturing CUDA graph shapes:  28%|██▊       | 19/67 [00:12<00:29,  1.62it/s]DEBUG 07-10 14:15:54 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 360\n",
            "DEBUG 07-10 14:15:54 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 360\n",
            "\rCapturing CUDA graph shapes:  30%|██▉       | 20/67 [00:12<00:28,  1.62it/s]DEBUG 07-10 14:15:55 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 352\n",
            "DEBUG 07-10 14:15:55 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 352\n",
            "DEBUG 07-10 14:15:55 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "\rCapturing CUDA graph shapes:  31%|███▏      | 21/67 [00:13<00:28,  1.63it/s]DEBUG 07-10 14:15:56 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 344\n",
            "DEBUG 07-10 14:15:56 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 344\n",
            "\rCapturing CUDA graph shapes:  33%|███▎      | 22/67 [00:13<00:27,  1.63it/s]DEBUG 07-10 14:15:56 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 336\n",
            "DEBUG 07-10 14:15:56 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 336\n",
            "\rCapturing CUDA graph shapes:  34%|███▍      | 23/67 [00:14<00:27,  1.62it/s]DEBUG 07-10 14:15:57 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 328\n",
            "DEBUG 07-10 14:15:57 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 328\n",
            "\rCapturing CUDA graph shapes:  36%|███▌      | 24/67 [00:15<00:26,  1.63it/s]DEBUG 07-10 14:15:57 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 320\n",
            "DEBUG 07-10 14:15:57 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 320\n",
            "\rCapturing CUDA graph shapes:  37%|███▋      | 25/67 [00:15<00:25,  1.63it/s]DEBUG 07-10 14:15:58 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 312\n",
            "DEBUG 07-10 14:15:58 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 312\n",
            "\rCapturing CUDA graph shapes:  39%|███▉      | 26/67 [00:16<00:25,  1.63it/s]DEBUG 07-10 14:15:59 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 304\n",
            "DEBUG 07-10 14:15:59 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 304\n",
            "\rCapturing CUDA graph shapes:  40%|████      | 27/67 [00:16<00:24,  1.64it/s]DEBUG 07-10 14:15:59 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 296\n",
            "DEBUG 07-10 14:15:59 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 296\n",
            "\rCapturing CUDA graph shapes:  42%|████▏     | 28/67 [00:17<00:23,  1.64it/s]DEBUG 07-10 14:16:00 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 288\n",
            "DEBUG 07-10 14:16:00 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 288\n",
            "\rCapturing CUDA graph shapes:  43%|████▎     | 29/67 [00:18<00:23,  1.63it/s]DEBUG 07-10 14:16:00 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 280\n",
            "DEBUG 07-10 14:16:01 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 280\n",
            "\rCapturing CUDA graph shapes:  45%|████▍     | 30/67 [00:18<00:22,  1.62it/s]DEBUG 07-10 14:16:01 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 272\n",
            "DEBUG 07-10 14:16:01 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 272\n",
            "\rCapturing CUDA graph shapes:  46%|████▋     | 31/67 [00:19<00:22,  1.62it/s]DEBUG 07-10 14:16:02 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 264\n",
            "DEBUG 07-10 14:16:02 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 264\n",
            "\rCapturing CUDA graph shapes:  48%|████▊     | 32/67 [00:20<00:21,  1.62it/s]DEBUG 07-10 14:16:02 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 256\n",
            "DEBUG 07-10 14:16:02 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 256\n",
            "\rCapturing CUDA graph shapes:  49%|████▉     | 33/67 [00:20<00:20,  1.64it/s]DEBUG 07-10 14:16:03 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 248\n",
            "DEBUG 07-10 14:16:03 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 248\n",
            "\rCapturing CUDA graph shapes:  51%|█████     | 34/67 [00:21<00:19,  1.66it/s]DEBUG 07-10 14:16:03 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 240\n",
            "DEBUG 07-10 14:16:04 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 240\n",
            "\rCapturing CUDA graph shapes:  52%|█████▏    | 35/67 [00:21<00:19,  1.68it/s]DEBUG 07-10 14:16:04 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 232\n",
            "DEBUG 07-10 14:16:04 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 232\n",
            "\rCapturing CUDA graph shapes:  54%|█████▎    | 36/67 [00:22<00:18,  1.68it/s]DEBUG 07-10 14:16:05 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 224\n",
            "DEBUG 07-10 14:16:05 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 224\n",
            "DEBUG 07-10 14:16:05 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "\rCapturing CUDA graph shapes:  55%|█████▌    | 37/67 [00:23<00:17,  1.68it/s]DEBUG 07-10 14:16:05 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 216\n",
            "DEBUG 07-10 14:16:05 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 216\n",
            "\rCapturing CUDA graph shapes:  57%|█████▋    | 38/67 [00:23<00:17,  1.68it/s]DEBUG 07-10 14:16:06 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 208\n",
            "DEBUG 07-10 14:16:06 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 208\n",
            "\rCapturing CUDA graph shapes:  58%|█████▊    | 39/67 [00:24<00:16,  1.69it/s]DEBUG 07-10 14:16:06 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 200\n",
            "DEBUG 07-10 14:16:06 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 200\n",
            "\rCapturing CUDA graph shapes:  60%|█████▉    | 40/67 [00:24<00:15,  1.69it/s]DEBUG 07-10 14:16:07 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 192\n",
            "DEBUG 07-10 14:16:07 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 192\n",
            "\rCapturing CUDA graph shapes:  61%|██████    | 41/67 [00:25<00:15,  1.70it/s]DEBUG 07-10 14:16:08 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 184\n",
            "DEBUG 07-10 14:16:08 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 184\n",
            "\rCapturing CUDA graph shapes:  63%|██████▎   | 42/67 [00:25<00:14,  1.70it/s]DEBUG 07-10 14:16:08 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 176\n",
            "DEBUG 07-10 14:16:08 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 176\n",
            "\rCapturing CUDA graph shapes:  64%|██████▍   | 43/67 [00:26<00:14,  1.71it/s]DEBUG 07-10 14:16:09 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 168\n",
            "DEBUG 07-10 14:16:09 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 168\n",
            "\rCapturing CUDA graph shapes:  66%|██████▌   | 44/67 [00:27<00:13,  1.71it/s]DEBUG 07-10 14:16:09 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 160\n",
            "DEBUG 07-10 14:16:09 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 160\n",
            "\rCapturing CUDA graph shapes:  67%|██████▋   | 45/67 [00:27<00:12,  1.71it/s]DEBUG 07-10 14:16:10 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 152\n",
            "DEBUG 07-10 14:16:10 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 152\n",
            "\rCapturing CUDA graph shapes:  69%|██████▊   | 46/67 [00:28<00:12,  1.71it/s]DEBUG 07-10 14:16:10 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 144\n",
            "DEBUG 07-10 14:16:11 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 144\n",
            "\rCapturing CUDA graph shapes:  70%|███████   | 47/67 [00:28<00:11,  1.70it/s]DEBUG 07-10 14:16:11 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 136\n",
            "DEBUG 07-10 14:16:11 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 136\n",
            "\rCapturing CUDA graph shapes:  72%|███████▏  | 48/67 [00:29<00:11,  1.71it/s]DEBUG 07-10 14:16:12 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 128\n",
            "DEBUG 07-10 14:16:12 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 128\n",
            "\rCapturing CUDA graph shapes:  73%|███████▎  | 49/67 [00:30<00:10,  1.71it/s]DEBUG 07-10 14:16:12 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 120\n",
            "DEBUG 07-10 14:16:12 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 120\n",
            "\rCapturing CUDA graph shapes:  75%|███████▍  | 50/67 [00:30<00:09,  1.72it/s]DEBUG 07-10 14:16:13 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 112\n",
            "DEBUG 07-10 14:16:13 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 112\n",
            "\rCapturing CUDA graph shapes:  76%|███████▌  | 51/67 [00:31<00:09,  1.72it/s]DEBUG 07-10 14:16:13 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 104\n",
            "DEBUG 07-10 14:16:13 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 104\n",
            "\rCapturing CUDA graph shapes:  78%|███████▊  | 52/67 [00:31<00:08,  1.73it/s]DEBUG 07-10 14:16:14 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 96\n",
            "DEBUG 07-10 14:16:14 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 96\n",
            "\rCapturing CUDA graph shapes:  79%|███████▉  | 53/67 [00:32<00:08,  1.72it/s]DEBUG 07-10 14:16:15 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 88\n",
            "DEBUG 07-10 14:16:15 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 88\n",
            "\rCapturing CUDA graph shapes:  81%|████████  | 54/67 [00:32<00:07,  1.72it/s]DEBUG 07-10 14:16:15 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 80\n",
            "DEBUG 07-10 14:16:15 [utils.py:475] Waiting for 1 local, 0 remote core engine proc(s) to start.\n",
            "DEBUG 07-10 14:16:15 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 80\n",
            "\rCapturing CUDA graph shapes:  82%|████████▏ | 55/67 [00:33<00:06,  1.73it/s]DEBUG 07-10 14:16:16 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 72\n",
            "DEBUG 07-10 14:16:16 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 72\n",
            "\rCapturing CUDA graph shapes:  84%|████████▎ | 56/67 [00:34<00:06,  1.73it/s]DEBUG 07-10 14:16:16 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 64\n",
            "DEBUG 07-10 14:16:16 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 64\n",
            "\rCapturing CUDA graph shapes:  85%|████████▌ | 57/67 [00:34<00:05,  1.72it/s]DEBUG 07-10 14:16:17 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 56\n",
            "DEBUG 07-10 14:16:17 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 56\n",
            "\rCapturing CUDA graph shapes:  87%|████████▋ | 58/67 [00:35<00:05,  1.73it/s]DEBUG 07-10 14:16:17 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 48\n",
            "DEBUG 07-10 14:16:17 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 48\n",
            "\rCapturing CUDA graph shapes:  88%|████████▊ | 59/67 [00:35<00:04,  1.74it/s]DEBUG 07-10 14:16:18 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 40\n",
            "DEBUG 07-10 14:16:18 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 40\n",
            "\rCapturing CUDA graph shapes:  90%|████████▉ | 60/67 [00:36<00:04,  1.74it/s]DEBUG 07-10 14:16:19 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 32\n",
            "DEBUG 07-10 14:16:19 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 32\n",
            "\rCapturing CUDA graph shapes:  91%|█████████ | 61/67 [00:37<00:03,  1.69it/s]DEBUG 07-10 14:16:19 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 24\n",
            "DEBUG 07-10 14:16:19 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 24\n",
            "\rCapturing CUDA graph shapes:  93%|█████████▎| 62/67 [00:37<00:02,  1.69it/s]DEBUG 07-10 14:16:20 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 16\n",
            "DEBUG 07-10 14:16:20 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 16\n",
            "\rCapturing CUDA graph shapes:  94%|█████████▍| 63/67 [00:38<00:02,  1.70it/s]DEBUG 07-10 14:16:20 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 8\n",
            "DEBUG 07-10 14:16:20 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 8\n",
            "\rCapturing CUDA graph shapes:  96%|█████████▌| 64/67 [00:38<00:01,  1.70it/s]DEBUG 07-10 14:16:21 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 4\n",
            "DEBUG 07-10 14:16:21 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 4\n",
            "\rCapturing CUDA graph shapes:  97%|█████████▋| 65/67 [00:39<00:01,  1.71it/s]DEBUG 07-10 14:16:22 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 2\n",
            "DEBUG 07-10 14:16:22 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 2\n",
            "\rCapturing CUDA graph shapes:  99%|█████████▊| 66/67 [00:39<00:00,  1.72it/s]DEBUG 07-10 14:16:22 [cuda_piecewise_backend.py:151] Warming up 1/1 for shape 1\n",
            "DEBUG 07-10 14:16:22 [cuda_piecewise_backend.py:162] Capturing a cudagraph for shape 1\n",
            "\rCapturing CUDA graph shapes: 100%|██████████| 67/67 [00:40<00:00,  1.69it/s]\rCapturing CUDA graph shapes: 100%|██████████| 67/67 [00:40<00:00,  1.65it/s]\n",
            "INFO 07-10 14:16:23 [gpu_model_runner.py:2326] Graph capturing finished in 41 secs, took 0.49 GiB\n",
            "INFO 07-10 14:16:23 [core.py:172] init engine (profile, create kv cache, warmup model) took 108.51 seconds\n",
            "DEBUG 07-10 14:16:24 [utils.py:545] READY from local core engine process 0.\n",
            "DEBUG 07-10 14:16:24 [core.py:614] EngineCore waiting for work.\n",
            "INFO 07-10 14:16:24 [loggers.py:137] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 6263\n",
            "DEBUG 07-10 14:16:24 [core.py:614] EngineCore waiting for work.\n",
            "WARNING 07-10 14:16:24 [config.py:1392] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
            "INFO 07-10 14:16:24 [serving_chat.py:125] Using default chat sampling params from model: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
            "INFO 07-10 14:16:24 [serving_completion.py:72] Using default completion sampling params from model: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}\n",
            "INFO 07-10 14:16:24 [api_server.py:1457] Starting vLLM API server 0 on http://127.0.0.1:8000\n",
            "INFO 07-10 14:16:24 [launcher.py:29] Available routes are:\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /openapi.json, Methods: GET, HEAD\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /docs, Methods: GET, HEAD\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /redoc, Methods: GET, HEAD\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /health, Methods: GET\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /load, Methods: GET\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /ping, Methods: POST\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /ping, Methods: GET\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /tokenize, Methods: POST\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /detokenize, Methods: POST\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /v1/models, Methods: GET\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /version, Methods: GET\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /v1/chat/completions, Methods: POST\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /v1/completions, Methods: POST\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /v1/embeddings, Methods: POST\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /pooling, Methods: POST\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /classify, Methods: POST\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /score, Methods: POST\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /v1/score, Methods: POST\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /v1/audio/translations, Methods: POST\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /rerank, Methods: POST\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /v1/rerank, Methods: POST\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /v2/rerank, Methods: POST\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /invocations, Methods: POST\n",
            "INFO 07-10 14:16:24 [launcher.py:37] Route: /metrics, Methods: GET\n",
            "INFO:     Started server process [2267]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "DEBUG 07-10 14:16:34 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:16:44 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:16:54 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:17:04 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:17:14 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:17:24 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:17:34 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:17:44 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:17:54 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:18:04 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:18:14 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:18:24 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:18:34 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:18:44 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:18:54 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:19:04 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:19:14 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:19:24 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:19:34 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:19:44 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:19:54 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:20:04 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:20:14 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:20:24 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:20:34 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "DEBUG 07-10 14:20:44 [loggers.py:118] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is another way to check that the server is running:"
      ],
      "metadata": {
        "id": "WKeNM6cHy7H1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -f | grep api_server | grep -v grep"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXPM4HzXpJYd",
        "outputId": "9f3d1b61-3e12-463a-bf59-7f9dc83618a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       26679       1  4 10:53 ?        00:00:15 python3 -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-7B-Instruct --gpu-memory-utilization 0.85 --dtype float16 --trust_remote_code --host 127.0.0.1 --port 8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several ways of querying it now.\n",
        "\n",
        "First, we can send a request from the command line:"
      ],
      "metadata": {
        "id": "ogbjcV5IsCKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X POST http://127.0.0.1:8000/v1/chat/completions \\\n",
        "         -H 'Content-Type: application/json' \\\n",
        "         -d '{\"model\":\"Qwen/Qwen2.5-7B-Instruct\",\"messages\":[{\"role\":\"user\",\"content\":\"hi\"}],\"max_tokens\":4}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pauNR_zO_0xM",
        "outputId": "20b89422-fea1-4fd3-b4ab-dc1ad21725b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"id\":\"chatcmpl-01c81d507c0849a5a55510d142081b06\",\"object\":\"chat.completion\",\"created\":1752058741,\"model\":\"Qwen/Qwen2.5-7B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"reasoning_content\":null,\"content\":\"Hello! How can\",\"tool_calls\":[]},\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":30,\"total_tokens\":34,\"completion_tokens\":4,\"prompt_tokens_details\":null},\"prompt_logprobs\":null,\"kv_transfer_params\":null}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can recognize the OpenAI completion' json structure here.\n",
        "\n",
        "We can also use the `requests` library to call it from Python code:"
      ],
      "metadata": {
        "id": "yPxgCTWZsLb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "url = \"http://127.0.0.1:8000/v1/chat/completions\"\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "data = {\n",
        "    \"model\":\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    \"messages\":\n",
        "     [\n",
        "         {\"role\":\"user\",\"content\":\"hi\"}\n",
        "     ],\n",
        "    \"max_tokens\":4}\n",
        "\n",
        "response = requests.post(url, headers=headers, json=data, timeout=5)\n",
        "completion = response.json()\n",
        "print(json.dumps(completion, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHT6_40zrafv",
        "outputId": "992e54fb-0d5f-4d5a-b92c-9c4aa569092f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-3475e4b3adc34389b8e8effdc6576f78\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1752058750,\n",
            "  \"model\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"reasoning_content\": null,\n",
            "        \"content\": \"Hello! How can\",\n",
            "        \"tool_calls\": []\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"length\",\n",
            "      \"stop_reason\": null\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 30,\n",
            "    \"total_tokens\": 34,\n",
            "    \"completion_tokens\": 4,\n",
            "    \"prompt_tokens_details\": null\n",
            "  },\n",
            "  \"prompt_logprobs\": null,\n",
            "  \"kv_transfer_params\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also check a large number of inference metrics by calling the `metrics` endpoint:"
      ],
      "metadata": {
        "id": "RPH6zzaXtNWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s http://127.0.0.1:8000/metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x0k5fA_N_0zr",
        "outputId": "d18db66c-6c78-4fa6-d35b-b351af78a2f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# HELP python_gc_objects_collected_total Objects collected during gc\n",
            "# TYPE python_gc_objects_collected_total counter\n",
            "python_gc_objects_collected_total{generation=\"0\"} 13297.0\n",
            "python_gc_objects_collected_total{generation=\"1\"} 2706.0\n",
            "python_gc_objects_collected_total{generation=\"2\"} 2894.0\n",
            "# HELP python_gc_objects_uncollectable_total Uncollectable objects found during GC\n",
            "# TYPE python_gc_objects_uncollectable_total counter\n",
            "python_gc_objects_uncollectable_total{generation=\"0\"} 0.0\n",
            "python_gc_objects_uncollectable_total{generation=\"1\"} 0.0\n",
            "python_gc_objects_uncollectable_total{generation=\"2\"} 0.0\n",
            "# HELP python_gc_collections_total Number of times this generation was collected\n",
            "# TYPE python_gc_collections_total counter\n",
            "python_gc_collections_total{generation=\"0\"} 2835.0\n",
            "python_gc_collections_total{generation=\"1\"} 256.0\n",
            "python_gc_collections_total{generation=\"2\"} 13.0\n",
            "# HELP python_info Python platform information\n",
            "# TYPE python_info gauge\n",
            "python_info{implementation=\"CPython\",major=\"3\",minor=\"11\",patchlevel=\"13\",version=\"3.11.13\"} 1.0\n",
            "# HELP process_virtual_memory_bytes Virtual memory size in bytes.\n",
            "# TYPE process_virtual_memory_bytes gauge\n",
            "process_virtual_memory_bytes 1.387272192e+010\n",
            "# HELP process_resident_memory_bytes Resident memory size in bytes.\n",
            "# TYPE process_resident_memory_bytes gauge\n",
            "process_resident_memory_bytes 1.421938688e+09\n",
            "# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.\n",
            "# TYPE process_start_time_seconds gauge\n",
            "process_start_time_seconds 1.75205839301e+09\n",
            "# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.\n",
            "# TYPE process_cpu_seconds_total counter\n",
            "process_cpu_seconds_total 16.439999999999998\n",
            "# HELP process_open_fds Number of open file descriptors.\n",
            "# TYPE process_open_fds gauge\n",
            "process_open_fds 54.0\n",
            "# HELP process_max_fds Maximum number of open file descriptors.\n",
            "# TYPE process_max_fds gauge\n",
            "process_max_fds 1.048576e+06\n",
            "# HELP vllm:num_requests_running Number of requests in model execution batches.\n",
            "# TYPE vllm:num_requests_running gauge\n",
            "vllm:num_requests_running{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "# HELP vllm:num_requests_waiting Number of requests waiting to be processed.\n",
            "# TYPE vllm:num_requests_waiting gauge\n",
            "vllm:num_requests_waiting{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "# HELP vllm:gpu_cache_usage_perc GPU KV-cache usage. 1 means 100 percent usage.DEPRECATED: Use vllm:kv_cache_usage_perc instead.\n",
            "# TYPE vllm:gpu_cache_usage_perc gauge\n",
            "vllm:gpu_cache_usage_perc{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0002725538293812946\n",
            "# HELP vllm:gpu_prefix_cache_queries_total GPU prefix cache queries, in terms of number of queried tokens.DEPRECATED: Use vllm:prefix_cache_queries instead.\n",
            "# TYPE vllm:gpu_prefix_cache_queries_total counter\n",
            "vllm:gpu_prefix_cache_queries_total{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 60.0\n",
            "# HELP vllm:gpu_prefix_cache_queries_created GPU prefix cache queries, in terms of number of queried tokens.DEPRECATED: Use vllm:prefix_cache_queries instead.\n",
            "# TYPE vllm:gpu_prefix_cache_queries_created gauge\n",
            "vllm:gpu_prefix_cache_queries_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196226609e+09\n",
            "# HELP vllm:gpu_prefix_cache_hits_total GPU prefix cache hits, in terms of number of cached tokens.DEPRECATED: Use vllm:prefix_cache_hits instead.\n",
            "# TYPE vllm:gpu_prefix_cache_hits_total counter\n",
            "vllm:gpu_prefix_cache_hits_total{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 16.0\n",
            "# HELP vllm:gpu_prefix_cache_hits_created GPU prefix cache hits, in terms of number of cached tokens.DEPRECATED: Use vllm:prefix_cache_hits instead.\n",
            "# TYPE vllm:gpu_prefix_cache_hits_created gauge\n",
            "vllm:gpu_prefix_cache_hits_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196226811e+09\n",
            "# HELP vllm:kv_cache_usage_perc KV-cache usage. 1 means 100 percent usage.\n",
            "# TYPE vllm:kv_cache_usage_perc gauge\n",
            "vllm:kv_cache_usage_perc{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0002725538293812946\n",
            "# HELP vllm:prefix_cache_queries_total Prefix cache queries, in terms of number of queried tokens.\n",
            "# TYPE vllm:prefix_cache_queries_total counter\n",
            "vllm:prefix_cache_queries_total{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 60.0\n",
            "# HELP vllm:prefix_cache_queries_created Prefix cache queries, in terms of number of queried tokens.\n",
            "# TYPE vllm:prefix_cache_queries_created gauge\n",
            "vllm:prefix_cache_queries_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196227214e+09\n",
            "# HELP vllm:prefix_cache_hits_total Prefix cache hits, in terms of number of cached tokens.\n",
            "# TYPE vllm:prefix_cache_hits_total counter\n",
            "vllm:prefix_cache_hits_total{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 16.0\n",
            "# HELP vllm:prefix_cache_hits_created Prefix cache hits, in terms of number of cached tokens.\n",
            "# TYPE vllm:prefix_cache_hits_created gauge\n",
            "vllm:prefix_cache_hits_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.752058419622742e+09\n",
            "# HELP vllm:num_preemptions_total Cumulative number of preemption from the engine.\n",
            "# TYPE vllm:num_preemptions_total counter\n",
            "vllm:num_preemptions_total{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "# HELP vllm:num_preemptions_created Cumulative number of preemption from the engine.\n",
            "# TYPE vllm:num_preemptions_created gauge\n",
            "vllm:num_preemptions_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196227603e+09\n",
            "# HELP vllm:prompt_tokens_total Number of prefill tokens processed.\n",
            "# TYPE vllm:prompt_tokens_total counter\n",
            "vllm:prompt_tokens_total{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 60.0\n",
            "# HELP vllm:prompt_tokens_created Number of prefill tokens processed.\n",
            "# TYPE vllm:prompt_tokens_created gauge\n",
            "vllm:prompt_tokens_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196227744e+09\n",
            "# HELP vllm:generation_tokens_total Number of generation tokens processed.\n",
            "# TYPE vllm:generation_tokens_total counter\n",
            "vllm:generation_tokens_total{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 8.0\n",
            "# HELP vllm:generation_tokens_created Number of generation tokens processed.\n",
            "# TYPE vllm:generation_tokens_created gauge\n",
            "vllm:generation_tokens_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196227882e+09\n",
            "# HELP vllm:request_success_total Count of successfully processed requests.\n",
            "# TYPE vllm:request_success_total counter\n",
            "vllm:request_success_total{engine=\"0\",finished_reason=\"stop\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:request_success_total{engine=\"0\",finished_reason=\"length\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_success_total{engine=\"0\",finished_reason=\"abort\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "# HELP vllm:request_success_created Count of successfully processed requests.\n",
            "# TYPE vllm:request_success_created gauge\n",
            "vllm:request_success_created{engine=\"0\",finished_reason=\"stop\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196228144e+09\n",
            "vllm:request_success_created{engine=\"0\",finished_reason=\"length\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196228223e+09\n",
            "vllm:request_success_created{engine=\"0\",finished_reason=\"abort\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.752058419622829e+09\n",
            "# HELP vllm:request_prompt_tokens Number of prefill tokens processed.\n",
            "# TYPE vllm:request_prompt_tokens histogram\n",
            "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"1.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"2.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"5.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"10.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"20.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"50.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"100.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"200.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"500.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"1000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"2000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"5000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"10000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"20000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prompt_tokens_bucket{engine=\"0\",le=\"+Inf\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prompt_tokens_count{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prompt_tokens_sum{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 60.0\n",
            "# HELP vllm:request_prompt_tokens_created Number of prefill tokens processed.\n",
            "# TYPE vllm:request_prompt_tokens_created gauge\n",
            "vllm:request_prompt_tokens_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196228771e+09\n",
            "# HELP vllm:request_generation_tokens Number of generation tokens processed.\n",
            "# TYPE vllm:request_generation_tokens histogram\n",
            "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"1.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"2.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"5.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"10.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"20.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"50.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"100.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"200.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"500.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"1000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"2000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"5000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"10000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"20000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_generation_tokens_bucket{engine=\"0\",le=\"+Inf\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_generation_tokens_count{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_generation_tokens_sum{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 8.0\n",
            "# HELP vllm:request_generation_tokens_created Number of generation tokens processed.\n",
            "# TYPE vllm:request_generation_tokens_created gauge\n",
            "vllm:request_generation_tokens_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.752058419622942e+09\n",
            "# HELP vllm:iteration_tokens_total Histogram of number of tokens per engine_step.\n",
            "# TYPE vllm:iteration_tokens_total histogram\n",
            "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"1.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"8.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"16.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"32.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 8.0\n",
            "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"64.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 8.0\n",
            "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"128.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 8.0\n",
            "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"256.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 8.0\n",
            "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"512.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 8.0\n",
            "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"1024.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 8.0\n",
            "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"2048.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 8.0\n",
            "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"4096.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 8.0\n",
            "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"8192.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 8.0\n",
            "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"16384.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 8.0\n",
            "vllm:iteration_tokens_total_bucket{engine=\"0\",le=\"+Inf\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 8.0\n",
            "vllm:iteration_tokens_total_count{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 8.0\n",
            "vllm:iteration_tokens_total_sum{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 68.0\n",
            "# HELP vllm:iteration_tokens_total_created Histogram of number of tokens per engine_step.\n",
            "# TYPE vllm:iteration_tokens_total_created gauge\n",
            "vllm:iteration_tokens_total_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196229875e+09\n",
            "# HELP vllm:request_max_num_generation_tokens Histogram of maximum number of requested generation tokens.\n",
            "# TYPE vllm:request_max_num_generation_tokens histogram\n",
            "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"1.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"2.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"5.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"10.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"20.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"50.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"100.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"200.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"500.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"1000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"2000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"5000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"10000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"20000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_max_num_generation_tokens_bucket{engine=\"0\",le=\"+Inf\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_max_num_generation_tokens_count{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_max_num_generation_tokens_sum{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 8.0\n",
            "# HELP vllm:request_max_num_generation_tokens_created Histogram of maximum number of requested generation tokens.\n",
            "# TYPE vllm:request_max_num_generation_tokens_created gauge\n",
            "vllm:request_max_num_generation_tokens_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196230392e+09\n",
            "# HELP vllm:request_params_n Histogram of the n request parameter.\n",
            "# TYPE vllm:request_params_n histogram\n",
            "vllm:request_params_n_bucket{engine=\"0\",le=\"1.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_n_bucket{engine=\"0\",le=\"2.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_n_bucket{engine=\"0\",le=\"5.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_n_bucket{engine=\"0\",le=\"10.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_n_bucket{engine=\"0\",le=\"20.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_n_bucket{engine=\"0\",le=\"+Inf\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_n_count{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_n_sum{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "# HELP vllm:request_params_n_created Histogram of the n request parameter.\n",
            "# TYPE vllm:request_params_n_created gauge\n",
            "vllm:request_params_n_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.752058419623081e+09\n",
            "# HELP vllm:request_params_max_tokens Histogram of the max_tokens request parameter.\n",
            "# TYPE vllm:request_params_max_tokens histogram\n",
            "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"1.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"2.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"5.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"10.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"20.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"50.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"100.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"200.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"500.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"1000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"2000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"5000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"10000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"20000.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_max_tokens_bucket{engine=\"0\",le=\"+Inf\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_max_tokens_count{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_params_max_tokens_sum{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 8.0\n",
            "# HELP vllm:request_params_max_tokens_created Histogram of the max_tokens request parameter.\n",
            "# TYPE vllm:request_params_max_tokens_created gauge\n",
            "vllm:request_params_max_tokens_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196231148e+09\n",
            "# HELP vllm:time_to_first_token_seconds Histogram of time to first token in seconds.\n",
            "# TYPE vllm:time_to_first_token_seconds histogram\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.001\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.005\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.01\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.02\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.04\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.06\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.08\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.1\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.25\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"0.75\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"1.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"2.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"5.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"7.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"10.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"20.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"40.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"80.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"160.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"640.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"2560.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:time_to_first_token_seconds_bucket{engine=\"0\",le=\"+Inf\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:time_to_first_token_seconds_count{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:time_to_first_token_seconds_sum{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.2782008647918701\n",
            "# HELP vllm:time_to_first_token_seconds_created Histogram of time to first token in seconds.\n",
            "# TYPE vllm:time_to_first_token_seconds_created gauge\n",
            "vllm:time_to_first_token_seconds_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.752058419624276e+09\n",
            "# HELP vllm:time_per_output_token_seconds Histogram of time per output token in seconds.\n",
            "# TYPE vllm:time_per_output_token_seconds histogram\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.01\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.025\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.05\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.075\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 5.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.1\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.15\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.2\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.3\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.4\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"0.75\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"1.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"2.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"5.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"7.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"10.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"20.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"40.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"80.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_bucket{engine=\"0\",le=\"+Inf\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_count{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 6.0\n",
            "vllm:time_per_output_token_seconds_sum{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.38073791100032395\n",
            "# HELP vllm:time_per_output_token_seconds_created Histogram of time per output token in seconds.\n",
            "# TYPE vllm:time_per_output_token_seconds_created gauge\n",
            "vllm:time_per_output_token_seconds_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.752058419624345e+09\n",
            "# HELP vllm:e2e_request_latency_seconds Histogram of e2e request latency in seconds.\n",
            "# TYPE vllm:e2e_request_latency_seconds histogram\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"0.3\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"0.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"0.8\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"1.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"1.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"2.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"2.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"5.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"10.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"15.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"20.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"30.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"40.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"50.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"60.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"120.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"240.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"480.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"960.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"1920.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"7680.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_bucket{engine=\"0\",le=\"+Inf\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_count{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:e2e_request_latency_seconds_sum{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.6589863300323486\n",
            "# HELP vllm:e2e_request_latency_seconds_created Histogram of e2e request latency in seconds.\n",
            "# TYPE vllm:e2e_request_latency_seconds_created gauge\n",
            "vllm:e2e_request_latency_seconds_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196244001e+09\n",
            "# HELP vllm:request_queue_time_seconds Histogram of time spent in WAITING phase for request.\n",
            "# TYPE vllm:request_queue_time_seconds histogram\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"0.3\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"0.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"0.8\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"1.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"1.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"2.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"2.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"5.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"10.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"15.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"20.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"30.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"40.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"50.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"60.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"120.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"240.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"480.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"960.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"1920.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"7680.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_bucket{engine=\"0\",le=\"+Inf\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_count{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_queue_time_seconds_sum{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.0002786639997793827\n",
            "# HELP vllm:request_queue_time_seconds_created Histogram of time spent in WAITING phase for request.\n",
            "# TYPE vllm:request_queue_time_seconds_created gauge\n",
            "vllm:request_queue_time_seconds_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196244533e+09\n",
            "# HELP vllm:request_inference_time_seconds Histogram of time spent in RUNNING phase for request.\n",
            "# TYPE vllm:request_inference_time_seconds histogram\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"0.3\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"0.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"0.8\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"1.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"1.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"2.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"2.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"5.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"10.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"15.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"20.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"30.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"40.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"50.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"60.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"120.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"240.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"480.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"960.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"1920.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"7680.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_bucket{engine=\"0\",le=\"+Inf\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_count{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_inference_time_seconds_sum{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.6553472300001886\n",
            "# HELP vllm:request_inference_time_seconds_created Histogram of time spent in RUNNING phase for request.\n",
            "# TYPE vllm:request_inference_time_seconds_created gauge\n",
            "vllm:request_inference_time_seconds_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196244988e+09\n",
            "# HELP vllm:request_prefill_time_seconds Histogram of time spent in PREFILL phase for request.\n",
            "# TYPE vllm:request_prefill_time_seconds histogram\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"0.3\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"0.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"0.8\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"1.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"1.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"2.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"2.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"5.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"10.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"15.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"20.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"30.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"40.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"50.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"60.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"120.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"240.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"480.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"960.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"1920.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"7680.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_bucket{engine=\"0\",le=\"+Inf\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_count{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_prefill_time_seconds_sum{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.2746093189998646\n",
            "# HELP vllm:request_prefill_time_seconds_created Histogram of time spent in PREFILL phase for request.\n",
            "# TYPE vllm:request_prefill_time_seconds_created gauge\n",
            "vllm:request_prefill_time_seconds_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196245468e+09\n",
            "# HELP vllm:request_decode_time_seconds Histogram of time spent in DECODE phase for request.\n",
            "# TYPE vllm:request_decode_time_seconds histogram\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"0.3\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"0.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"0.8\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"1.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"1.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"2.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"2.5\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"5.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"10.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"15.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"20.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"30.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"40.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"50.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"60.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"120.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"240.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"480.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"960.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"1920.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"7680.0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_bucket{engine=\"0\",le=\"+Inf\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_count{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 2.0\n",
            "vllm:request_decode_time_seconds_sum{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 0.38073791100032395\n",
            "# HELP vllm:request_decode_time_seconds_created Histogram of time spent in DECODE phase for request.\n",
            "# TYPE vllm:request_decode_time_seconds_created gauge\n",
            "vllm:request_decode_time_seconds_created{engine=\"0\",model_name=\"Qwen/Qwen2.5-7B-Instruct\"} 1.7520584196245966e+09\n",
            "# HELP vllm:cache_config_info Information of the LLMEngine CacheConfig\n",
            "# TYPE vllm:cache_config_info gauge\n",
            "vllm:cache_config_info{block_size=\"16\",cache_dtype=\"auto\",calculate_kv_scales=\"False\",cpu_kvcache_space_bytes=\"None\",cpu_offload_gb=\"0.0\",enable_prefix_caching=\"True\",engine=\"0\",gpu_memory_utilization=\"0.85\",is_attention_free=\"False\",num_cpu_blocks=\"None\",num_gpu_blocks=\"3669\",num_gpu_blocks_override=\"None\",prefix_caching_hash_algo=\"builtin\",sliding_window=\"None\",swap_space=\"4.0\",swap_space_bytes=\"4294967296.0\"} 1.0\n",
            "# HELP http_requests_total Total number of requests by method, status and handler.\n",
            "# TYPE http_requests_total counter\n",
            "http_requests_total{handler=\"/v1/chat/completions\",method=\"POST\",status=\"2xx\"} 2.0\n",
            "# HELP http_requests_created Total number of requests by method, status and handler.\n",
            "# TYPE http_requests_created gauge\n",
            "http_requests_created{handler=\"/v1/chat/completions\",method=\"POST\",status=\"2xx\"} 1.7520587423952892e+09\n",
            "# HELP http_request_size_bytes Content length of incoming requests by handler. Only value of header is respected. Otherwise ignored. No percentile calculated. \n",
            "# TYPE http_request_size_bytes summary\n",
            "http_request_size_bytes_count{handler=\"/v1/chat/completions\"} 2.0\n",
            "http_request_size_bytes_sum{handler=\"/v1/chat/completions\"} 198.0\n",
            "# HELP http_request_size_bytes_created Content length of incoming requests by handler. Only value of header is respected. Otherwise ignored. No percentile calculated. \n",
            "# TYPE http_request_size_bytes_created gauge\n",
            "http_request_size_bytes_created{handler=\"/v1/chat/completions\"} 1.7520587423953228e+09\n",
            "# HELP http_response_size_bytes Content length of outgoing responses by handler. Only value of header is respected. Otherwise ignored. No percentile calculated. \n",
            "# TYPE http_response_size_bytes summary\n",
            "http_response_size_bytes_count{handler=\"/v1/chat/completions\"} 2.0\n",
            "http_response_size_bytes_sum{handler=\"/v1/chat/completions\"} 926.0\n",
            "# HELP http_response_size_bytes_created Content length of outgoing responses by handler. Only value of header is respected. Otherwise ignored. No percentile calculated. \n",
            "# TYPE http_response_size_bytes_created gauge\n",
            "http_response_size_bytes_created{handler=\"/v1/chat/completions\"} 1.7520587423953648e+09\n",
            "# HELP http_request_duration_highr_seconds Latency with many buckets but no API specific labels. Made for more accurate percentile calculations. \n",
            "# TYPE http_request_duration_highr_seconds histogram\n",
            "http_request_duration_highr_seconds_bucket{le=\"0.01\"} 0.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"0.025\"} 0.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"0.05\"} 0.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"0.075\"} 0.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"0.1\"} 0.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"0.25\"} 1.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"0.5\"} 1.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"0.75\"} 1.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"1.0\"} 1.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"1.5\"} 1.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"2.0\"} 1.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"2.5\"} 1.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"3.0\"} 2.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"3.5\"} 2.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"4.0\"} 2.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"4.5\"} 2.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"5.0\"} 2.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"7.5\"} 2.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"10.0\"} 2.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"30.0\"} 2.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"60.0\"} 2.0\n",
            "http_request_duration_highr_seconds_bucket{le=\"+Inf\"} 2.0\n",
            "http_request_duration_highr_seconds_count 2.0\n",
            "http_request_duration_highr_seconds_sum 2.995008057000632\n",
            "# HELP http_request_duration_highr_seconds_created Latency with many buckets but no API specific labels. Made for more accurate percentile calculations. \n",
            "# TYPE http_request_duration_highr_seconds_created gauge\n",
            "http_request_duration_highr_seconds_created 1.752058691854062e+09\n",
            "# HELP http_request_duration_seconds Latency with only few buckets by handler. Made to be only used if aggregation by handler is important. \n",
            "# TYPE http_request_duration_seconds histogram\n",
            "http_request_duration_seconds_bucket{handler=\"/v1/chat/completions\",le=\"0.1\",method=\"POST\"} 0.0\n",
            "http_request_duration_seconds_bucket{handler=\"/v1/chat/completions\",le=\"0.5\",method=\"POST\"} 1.0\n",
            "http_request_duration_seconds_bucket{handler=\"/v1/chat/completions\",le=\"1.0\",method=\"POST\"} 1.0\n",
            "http_request_duration_seconds_bucket{handler=\"/v1/chat/completions\",le=\"+Inf\",method=\"POST\"} 2.0\n",
            "http_request_duration_seconds_count{handler=\"/v1/chat/completions\",method=\"POST\"} 2.0\n",
            "http_request_duration_seconds_sum{handler=\"/v1/chat/completions\",method=\"POST\"} 2.995008057000632\n",
            "# HELP http_request_duration_seconds_created Latency with only few buckets by handler. Made to be only used if aggregation by handler is important. \n",
            "# TYPE http_request_duration_seconds_created gauge\n",
            "http_request_duration_seconds_created{handler=\"/v1/chat/completions\",method=\"POST\"} 1.7520587423954074e+09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or you can use Python format:"
      ],
      "metadata": {
        "id": "fQ45Jo-NtVk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "resp = requests.get(\"http://127.0.0.1:8000/metrics\", timeout=5)\n",
        "metrics_text = resp\n",
        "# print(\"Metrics:\", metrics_text)"
      ],
      "metadata": {
        "id": "O-X-onOK_028"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to consult with [vLLM docs](https://docs.vllm.ai/en/v0.8.2/design/v1/metrics.html) to learn more about the `metrics` API."
      ],
      "metadata": {
        "id": "u1pAf2OOtnyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, you can use the familiar `OpenAI()` client to work with the vLLM server. For that, you'll need to supply:\n",
        "\n",
        "* `base_url` - the url and the port the sever is listening to\n",
        "* `api_key` - this requirement is quite stupid, because we don't actually need an API key, but still you'll have to give it something. Any dummy nonempty string will do.\n",
        "\n",
        "The `chat` interface is the same as we used with Nebius AI Studio."
      ],
      "metadata": {
        "id": "5Y14Y8AS2h0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"DUMMY\", # Any dummy string\n",
        "    base_url=\"http://127.0.0.1:8000/v1\",\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\",   \"content\": \"What time is it now?\"}\n",
        "    ],\n",
        "    max_tokens=64,\n",
        ")\n",
        "\n",
        "completion = response.to_json()\n",
        "print(completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us1Cmtz02lMb",
        "outputId": "78323eef-3d23-44a8-acf4-9d82b093f30d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-6afa0f8690e64269b45101880f7d469b\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"message\": {\n",
            "        \"content\": \"As an AI, I don't have real-time clock access, so I can't provide the current time. You can easily find the current time by checking any device you're using, such as your phone or computer.\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"tool_calls\": [],\n",
            "        \"reasoning_content\": null\n",
            "      },\n",
            "      \"stop_reason\": null\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1752059289,\n",
            "  \"model\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 45,\n",
            "    \"prompt_tokens\": 25,\n",
            "    \"total_tokens\": 70,\n",
            "    \"prompt_tokens_details\": null\n",
            "  },\n",
            "  \"prompt_logprobs\": null,\n",
            "  \"kv_transfer_params\": null\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}
