{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nebius-Academy/LLM-Engineering-Essentials/blob/main/topic5/5.4_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "Course github: [link](https://github.com/Nebius-Academy/LLM-Engineering-Essentials/tree/main)\n",
        "\n",
        "The course is in development now, with more materials coming soon."
      ],
      "metadata": {
        "id": "YC3UvnIu2N6h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "966Q_AlJyz2v"
      },
      "source": [
        "# 5.4. Quantization\n",
        "\n",
        "**By: [Alexey Bukhtiyarov](https://www.linkedin.com/in/leshanbog/)**\n",
        "\n",
        "In this notebook we evaluate both the **speed** and **quality** of LLMs that have been quantized to different precisions.\n",
        "\n",
        "* **Speed** - models are served with **vLLM**\n",
        "* **Quality** - using **lm-eval-harness** we measure downstream accuracy to see how much each quantization level impacts performance.\n",
        "\n",
        "All helper functions that spin up vLLM, drive the benchmarks, and run the evaluations live in `utils.py` - feel free to open that file for implementation details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTIN9sTVyz2x"
      },
      "outputs": [],
      "source": [
        "pip install pandas datasets ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD4riSsIyz2y"
      },
      "source": [
        "## Speed evaluation\n",
        "\n",
        "We will benchmark the inference speed of a single **Qwen-3 8B** model on an NVIDIA L40 using vLLM. The model will be served in four precisions:\n",
        "- BF16\n",
        "- FP8\n",
        "- AWQ INT4 (default kernel)\n",
        "- AWQ INT4 (Marlin kernel)\n",
        "\n",
        "We're going to measure four main metrics:\n",
        "- time-to-first-token (TTFT)\n",
        "- time-per-output-token (TPOT)\n",
        "- full end-to-end latency\n",
        "- token throughput\n",
        "\n",
        "*Note*: The default AWQ INT4 is the original method in vLLM, while the Marlin kernel is a newer, more optimized version designed for even better speed and efficiency on modern GPUs.\n",
        "\n",
        "Both AWQ (default) and AWQ Marlin use INT4 quantization to make the model smaller and faster. The default kernel is the original method for running AWQ quantized models in vLLM, while the Marlin kernel is a newer, more optimized version designed for even better speed and efficiency. Here, a ‚Äúkernel‚Äù just means the low-level code that runs on the GPU to do the computation.\n",
        "\n",
        "For this lesson, you only need to know that Marlin aims to improve performance compared to the default approach. You can find more details about the Marlin kernel [here](https://github.com/IST-DASLab/marlin)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o utils.py https://raw.githubusercontent.com/Nebius-Academy/LLM-Engineering-Essentials/main/topic5/utils.py"
      ],
      "metadata": {
        "id": "tzxuTJU82cAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzhaGUIxyz2y",
        "outputId": "178ba7fd-48f5-4430-f213-ed8058f4d154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 06-22 14:33:10 [__init__.py:244] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from utils import setup_benchmark_environment, bench_single_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcA9CYgjyz2z"
      },
      "outputs": [],
      "source": [
        "ok = setup_benchmark_environment()\n",
        "assert ok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0hiVVeLyz2z",
        "outputId": "3512cde8-2f0d-4a82-ecf4-7c4df280c706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(ok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNie0vyJyz2z"
      },
      "outputs": [],
      "source": [
        "models = [\n",
        "    {\n",
        "        \"name\": \"Qwen-3 8B / BF16\",\n",
        "        \"hf_id\": \"Qwen/Qwen3-8B\",\n",
        "        \"vllm_args\": [\n",
        "            \"--max-model-len\", \"2048\",\n",
        "            \"--max-seq-len-to-capture\", \"2048\",\n",
        "            \"--gpu-memory-utilization\", \"0.96\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Qwen-3 8B / FP8\",\n",
        "        \"hf_id\": \"Qwen/Qwen3-8B-FP8\",\n",
        "        \"vllm_args\": [\n",
        "            \"--max-model-len\", \"2048\",\n",
        "            \"--max-seq-len-to-capture\", \"2048\",\n",
        "            \"--gpu-memory-utilization\", \"0.96\",\n",
        "            \"--quantization\", \"fp8\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Qwen-3 8B / AWQ 4bit\",\n",
        "        \"hf_id\": \"Qwen/Qwen3-8B-AWQ\",\n",
        "        \"vllm_args\": [\n",
        "            \"--max-model-len\", \"2048\",\n",
        "            \"--max-seq-len-to-capture\", \"2048\",\n",
        "            \"--gpu-memory-utilization\", \"0.96\",\n",
        "            \"--quantization\", \"awq\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Qwen-3 8B / AWQ Marlin 4bit \",\n",
        "        \"hf_id\": \"Qwen/Qwen3-8B-AWQ\",\n",
        "        \"vllm_args\": [\n",
        "            \"--max-model-len\", \"2048\",\n",
        "            \"--max-seq-len-to-capture\", \"2048\",\n",
        "            \"--gpu-memory-utilization\", \"0.96\",\n",
        "            \"--dtype\", \"half\",\n",
        "            \"--quantization\", \"awq_marlin\",\n",
        "        ],\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d340dd194a4244a3a995e31502f2b87d",
            "b4a79f5d41bd4bde8615627e7b5034ff",
            "d2f5f9ac0bbd48e389e23bc638d9582b",
            "25d8815c18364c3d87836c32062a7d66"
          ]
        },
        "id": "cELnY72Fyz2z",
        "outputId": "4da7a12d-bec8-4c23-815d-722f4619bab9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d340dd194a4244a3a995e31502f2b87d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4a79f5d41bd4bde8615627e7b5034ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2f5f9ac0bbd48e389e23bc638d9582b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25d8815c18364c3d87836c32062a7d66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "for model in models:\n",
        "    snapshot_download(repo_id=model[\"hf_id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u-qw0dSyz20"
      },
      "outputs": [],
      "source": [
        "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-gcCosgyz20"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for model in models:\n",
        "    res = bench_single_model(\n",
        "        model_name=model[\"hf_id\"],\n",
        "        port=9134,\n",
        "        request_rate=10,\n",
        "        num_prompts=200,\n",
        "        vllm_path=os.path.join(current_dir, \"vllm\"),\n",
        "        vllm_args=model[\"vllm_args\"],\n",
        "        input_len=512,\n",
        "        output_len=128\n",
        "    )\n",
        "    res[\"name\"] = model[\"name\"]\n",
        "    results.append(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvXxeWgpyz20",
        "outputId": "36f10f6a-6cab-4ebf-a841-a3ea0a063391"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>throughput_tokens_per_sec</th>\n",
              "      <th>ttft_mean</th>\n",
              "      <th>ttft_p90</th>\n",
              "      <th>tpot_mean</th>\n",
              "      <th>tpot_p90</th>\n",
              "      <th>e2el_mean</th>\n",
              "      <th>e2el_p90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Qwen-3 8B / BF16</td>\n",
              "      <td>1052.84</td>\n",
              "      <td>144.20</td>\n",
              "      <td>224.63</td>\n",
              "      <td>50.84</td>\n",
              "      <td>56.68</td>\n",
              "      <td>6243.78</td>\n",
              "      <td>7276.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Qwen-3 8B / FP8</td>\n",
              "      <td>1119.78</td>\n",
              "      <td>100.66</td>\n",
              "      <td>147.52</td>\n",
              "      <td>33.47</td>\n",
              "      <td>37.64</td>\n",
              "      <td>4109.30</td>\n",
              "      <td>4872.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Qwen-3 8B / AWQ 4bit</td>\n",
              "      <td>868.82</td>\n",
              "      <td>283.24</td>\n",
              "      <td>461.84</td>\n",
              "      <td>104.87</td>\n",
              "      <td>122.98</td>\n",
              "      <td>12778.73</td>\n",
              "      <td>15857.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Qwen-3 8B / AWQ Marlin 4bit</td>\n",
              "      <td>1170.49</td>\n",
              "      <td>99.31</td>\n",
              "      <td>160.09</td>\n",
              "      <td>24.91</td>\n",
              "      <td>29.16</td>\n",
              "      <td>3032.04</td>\n",
              "      <td>3705.31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           name  throughput_tokens_per_sec  ttft_mean  \\\n",
              "0              Qwen-3 8B / BF16                    1052.84     144.20   \n",
              "1               Qwen-3 8B / FP8                    1119.78     100.66   \n",
              "2          Qwen-3 8B / AWQ 4bit                     868.82     283.24   \n",
              "3  Qwen-3 8B / AWQ Marlin 4bit                     1170.49      99.31   \n",
              "\n",
              "   ttft_p90  tpot_mean  tpot_p90  e2el_mean  e2el_p90  \n",
              "0    224.63      50.84     56.68    6243.78   7276.14  \n",
              "1    147.52      33.47     37.64    4109.30   4872.23  \n",
              "2    461.84     104.87    122.98   12778.73  15857.04  \n",
              "3    160.09      24.91     29.16    3032.04   3705.31  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df[['name', 'throughput_tokens_per_sec', 'ttft_mean', 'ttft_p90', 'tpot_mean', 'tpot_p90', 'e2el_mean', 'e2el_p90']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mEhC-WOyz20"
      },
      "source": [
        "### üìù What These Results Show\n",
        "\n",
        "- FP8 cuts per-token latency by ~35% versus BF16¬†‚Äî Ada/Hopper GPUs (for example L40, H100, H200) include native FP8 tensor‚Äëcores, so decoding cost drops from 50.8‚ÄØms per token (BF16) to 33.5‚ÄØms (FP8) while throughput rises.\n",
        "\n",
        "- AWQ‚ÄëMarlin wins overall¬†‚Äî it uses an optimized kernel code for dramatically less memory movement and a mixed-precision dot-product the GPU can execute in one shot, giving it the best overall throughput (1170‚ÄØtok/s) and the lowest TPOT (24.9‚ÄØms).\n",
        "\n",
        "- Kernel quality beats bit‚Äëwidth.\n",
        "\n",
        "  - The default AWQ path in vLLM dequantises 4‚Äëbit weights to FP16 every layer and every token, and promotes BF16 activations to FP16 before the matmul. The extra memory traffic and frequent dtype conversions wipe out most of the INT4 advantage.\n",
        "\n",
        "  - Marlin avoids this cost by expanding each weight block once per micro‚Äëbatch and re‚Äëusing it, keeping activations in FP16.\n",
        "\n",
        "  - INT4 alone does not guarantee speed¬†‚Äî you need a kernel that is both compute‚Äë and memory‚Äëefficient.\n",
        "\n",
        "- **Take‚Äëaways for practitioners**\n",
        "\n",
        "  - Measure, don‚Äôt assume. Low precision brings speed-ups only when the kernel is memory-bandwidth-bound (i.e. the slow part is transferring data to and from GPU memory, not doing the computations) and when it avoids redundant conversions.\n",
        "\n",
        "  - Use FP8 on Ada/Hopper whenever memory allows ‚Äî it delivers near‚ÄëFP16 quality with roughly one-third to one-half lower latency.\n",
        "\n",
        "  - Prefer AWQ‚ÄëMarlin when you need the smallest memory footprint and high throughput."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWENJVORyz21"
      },
      "source": [
        "## Quality evaluation\n",
        "\n",
        "In this part we will see how quantization affects quality.\n",
        "\n",
        "We will load several models, including a **Llama-3.1-70B** variant compressed with AWQ 4-bit, and measure their performance with `lm-eval`. Thanks to 4-bit quantization, the entire 70B checkpoint fits on a single NVIDIA L40 (46 GB), whereas the 16-bit original needs ‚âà140 GB and an 8-bit version ‚âà70 GB.\n",
        "\n",
        "In addition to AWQ, we will also evaluate the Llama-3.1-70B model in [**GGUF**](https://huggingface.co/docs/hub/en/gguf) format. GGUF is a compact and portable format designed primarily for use with the [`llama.cpp`](https://github.com/ggml-org/llama.cpp) inference stack, often targeting CPU and lightweight GPU setups. While GGUF makes it easy to run models across a wide range of devices and supports very small quantized variants (e.g., Q4_K_M), it is not currently optimized for vLLM, and may show much slower performance characteristics compared to AWQ models. Here, we are primarily interested in evaluating quality rather than speed, demonstrating how quantized formats enable serving larger models, which typically result in better quality.\n",
        "\n",
        "We will also benchmark the **Qwen3-8B**, comparing its full-precision BF16 baseline with more compact FP8 and AWQ-INT4 variants.\n",
        "\n",
        "For Qwen-3 the authors already [report scores](https://huggingface.co/Qwen/Qwen3-8B-AWQ#performance) on tougher leaderboard-style benchmarks (LiveBench, GPQA, etc.), showing that AWQ-INT4 loses only slightly compared with BF16 while cutting memory use by ‚â≥50 %.\n",
        "\n",
        "Below, the `evaluate_model` helper runs evaluation on HellaSwag, a multiple-choice commonsense reasoning benchmark designed to test a model's ability to choose the most plausible continuation of a given context. This allows you to observe the quality-vs-size trade-offs in a reproducible setup focused on everyday reasoning performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAr7XCdHyz21"
      },
      "outputs": [],
      "source": [
        "pip install lm-eval[vllm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "1cdb343f3e2b42a7995cfaf6a525219a",
            "e5336ed4c33a4c12b0ec36976eb29e03",
            "fb58ddaed2f448039e902e819cb7f527"
          ]
        },
        "id": "pJb3hcTAyz21",
        "outputId": "2cd638ef-d9c6-424b-8be9-8f6e2161c046"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1cdb343f3e2b42a7995cfaf6a525219a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5336ed4c33a4c12b0ec36976eb29e03",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb58ddaed2f448039e902e819cb7f527",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Meta-Llama-3.1-70B-Instruct-IQ2_S.gguf:   0%|          | 0.00/22.2G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "local_dir_4bit = snapshot_download(\n",
        "    repo_id=\"bartowski/Meta-Llama-3.1-70B-Instruct-GGUF\",\n",
        "    allow_patterns=[\"Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf\"],\n",
        ")\n",
        "\n",
        "local_dir_2bit = snapshot_download(\n",
        "    repo_id=\"bartowski/Meta-Llama-3.1-70B-Instruct-GGUF\",\n",
        "    allow_patterns=[\"Meta-Llama-3.1-70B-Instruct-IQ2_S.gguf\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIPFTQ4Iyz21",
        "outputId": "11293db2-4751-4fad-ad13-9a51babf654a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-06 14:02:13 [__init__.py:244] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "from utils import evaluate_model\n",
        "\n",
        "models = [\n",
        "    {\n",
        "        \"name\": \"Llama 70B / GGUF 4bit\",\n",
        "        \"hf_id\": f\"{local_dir_4bit}/Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf\",\n",
        "        \"vllm_args\": [\n",
        "            \"--max-model-len\", \"256\",\n",
        "            \"--max-seq-len-to-capture\", \"256\",\n",
        "            \"--max-num-seqs\", \"1\",  # setting to 1 since evaluation requires logits that consume a lot of memory\n",
        "            \"--tokenizer\", \"hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4\",\n",
        "            \"--gpu-memory-utilization\", \"0.96\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Llama 70B / GGUF 2bit\",\n",
        "        \"hf_id\": f\"{local_dir_2bit}/Meta-Llama-3.1-70B-Instruct-IQ2_S.gguf\",\n",
        "        \"vllm_args\": [\n",
        "            \"--max-model-len\", \"256\",\n",
        "            \"--max-seq-len-to-capture\", \"256\",\n",
        "            \"--max-num-seqs\", \"1\",  # setting to 1 since evaluation requires logits that consume a lot of memory\n",
        "            \"--tokenizer\", \"hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4\",\n",
        "            \"--gpu-memory-utilization\", \"0.96\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Llama 70b / AWQ 4bit\",\n",
        "        \"hf_id\": \"hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4\",\n",
        "        \"vllm_args\": [\n",
        "            \"--max-model-len\", \"256\",\n",
        "            \"--max-seq-len-to-capture\", \"256\",\n",
        "            \"--max-num-seqs\", \"1\",  # setting to 1 since evaluation requires logits that consume a lot of memory\n",
        "            \"--quantization\", \"awq_marlin\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Qwen-3 8B / BF16\",\n",
        "        \"hf_id\": \"Qwen/Qwen3-8B\",\n",
        "        \"vllm_args\": [\n",
        "            \"--max-model-len\", \"256\",\n",
        "            \"--max-seq-len-to-capture\", \"256\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Qwen-3 8B / FP8\",\n",
        "        \"hf_id\": \"Qwen/Qwen3-8B-FP8\",\n",
        "        \"vllm_args\": [\n",
        "            \"--max-model-len\", \"256\",\n",
        "            \"--max-seq-len-to-capture\", \"256\",\n",
        "            \"--quantization\", \"fp8\",\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Qwen-3 8B / AWQ 4bit\",\n",
        "        \"hf_id\": \"Qwen/Qwen3-8B-AWQ\",\n",
        "        \"vllm_args\": [\n",
        "            \"--max-model-len\", \"256\",\n",
        "            \"--max-seq-len-to-capture\", \"256\",\n",
        "            \"--quantization\", \"awq_marlin\",\n",
        "        ],\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LnouGioyz21",
        "outputId": "61653503-423f-438b-f53d-3546e4df05fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-06 14:02:46 [config.py:823] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.\n",
            "ERROR 07-06 14:02:46 [config.py:114] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/lex/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-70B-Instruct-GGUF/snapshots/83fb6e83d0a8aada42d499259bc929d922e9a558/Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf'. Use `repo_type` argument if needed., retrying 1 of 2\n",
            "ERROR 07-06 14:02:48 [config.py:112] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/lex/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-70B-Instruct-GGUF/snapshots/83fb6e83d0a8aada42d499259bc929d922e9a558/Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf'. Use `repo_type` argument if needed.\n",
            "INFO 07-06 14:02:48 [config.py:3268] Downcasting torch.float32 to torch.bfloat16.\n",
            "WARNING 07-06 14:02:48 [config.py:931] gguf quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
            "INFO 07-06 14:02:48 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 07-06 14:02:48 [config.py:2232] max_num_batched_tokens (8192) exceeds max_num_seqs* max_model_len (256). This may lead to unexpected behavior.\n",
            "INFO 07-06 14:03:11 [core.py:455] Waiting for init message from front-end.\n",
            "INFO 07-06 14:03:11 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/home/lex/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-70B-Instruct-GGUF/snapshots/83fb6e83d0a8aada42d499259bc929d922e9a558/Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf', speculative_config=None, tokenizer='hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=gguf, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gguf, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=1234, served_model_name=/home/lex/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-70B-Instruct-GGUF/snapshots/83fb6e83d0a8aada42d499259bc929d922e9a558/Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "WARNING 07-06 14:03:11 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f02f247ea40>\n",
            "INFO 07-06 14:03:12 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "WARNING 07-06 14:03:12 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 07-06 14:03:12 [gpu_model_runner.py:1595] Starting to load model /home/lex/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-70B-Instruct-GGUF/snapshots/83fb6e83d0a8aada42d499259bc929d922e9a558/Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf...\n",
            "INFO 07-06 14:03:12 [gpu_model_runner.py:1600] Loading model from scratch...\n",
            "INFO 07-06 14:03:21 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
            "INFO 07-06 14:06:39 [gpu_model_runner.py:1624] Model loading took 40.7626 GiB and 206.114672 seconds\n",
            "INFO 07-06 14:06:59 [backends.py:462] Using cache directory: /home/lex/.cache/vllm/torch_compile_cache/d2fbf9a87f/rank_0_0 for vLLM's torch.compile\n",
            "INFO 07-06 14:06:59 [backends.py:472] Dynamo bytecode transform time: 19.87 s\n",
            "INFO 07-06 14:07:18 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 17.957 s\n",
            "INFO 07-06 14:08:16 [monitor.py:34] torch.compile takes 19.87 s in total\n",
            "INFO 07-06 14:09:22 [gpu_worker.py:227] Available KV cache memory: 0.09 GiB\n",
            "INFO 07-06 14:09:22 [kv_cache_utils.py:715] GPU KV cache size: 304 tokens\n",
            "INFO 07-06 14:09:22 [kv_cache_utils.py:719] Maximum concurrency for 256 tokens per request: 1.19x\n",
            "INFO 07-06 14:13:53 [gpu_model_runner.py:2048] Graph capturing finished in 270 secs, took 1.85 GiB\n",
            "INFO 07-06 14:13:53 [core.py:171] init engine (profile, create kv cache, warmup model) took 434.17 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overwriting default num_fewshot of hellaswag from None to 0\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:00<00:00, 3553.46it/s]\n",
            "Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1600/1600 [18:53<00:00,  1.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-06 14:33:34 [config.py:823] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.\n",
            "ERROR 07-06 14:33:34 [config.py:114] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/lex/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-70B-Instruct-GGUF/snapshots/83fb6e83d0a8aada42d499259bc929d922e9a558/Meta-Llama-3.1-70B-Instruct-IQ2_S.gguf'. Use `repo_type` argument if needed., retrying 1 of 2\n",
            "ERROR 07-06 14:33:36 [config.py:112] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/lex/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-70B-Instruct-GGUF/snapshots/83fb6e83d0a8aada42d499259bc929d922e9a558/Meta-Llama-3.1-70B-Instruct-IQ2_S.gguf'. Use `repo_type` argument if needed.\n",
            "INFO 07-06 14:33:36 [config.py:3268] Downcasting torch.float32 to torch.bfloat16.\n",
            "WARNING 07-06 14:33:36 [config.py:931] gguf quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
            "INFO 07-06 14:33:36 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 07-06 14:33:36 [config.py:2232] max_num_batched_tokens (8192) exceeds max_num_seqs* max_model_len (256). This may lead to unexpected behavior.\n",
            "WARNING 07-06 14:33:58 [utils.py:2597] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
            "WARNING 07-06 14:34:00 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
            "INFO 07-06 14:34:05 [__init__.py:244] Automatically detected platform cuda.\n",
            "INFO 07-06 14:34:09 [core.py:455] Waiting for init message from front-end.\n",
            "INFO 07-06 14:34:09 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/home/lex/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-70B-Instruct-GGUF/snapshots/83fb6e83d0a8aada42d499259bc929d922e9a558/Meta-Llama-3.1-70B-Instruct-IQ2_S.gguf', speculative_config=None, tokenizer='hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=gguf, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gguf, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=1234, served_model_name=/home/lex/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-70B-Instruct-GGUF/snapshots/83fb6e83d0a8aada42d499259bc929d922e9a558/Meta-Llama-3.1-70B-Instruct-IQ2_S.gguf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "WARNING 07-06 14:34:09 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1b468263e0>\n",
            "INFO 07-06 14:34:09 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "WARNING 07-06 14:34:09 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 07-06 14:34:10 [gpu_model_runner.py:1595] Starting to load model /home/lex/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-70B-Instruct-GGUF/snapshots/83fb6e83d0a8aada42d499259bc929d922e9a558/Meta-Llama-3.1-70B-Instruct-IQ2_S.gguf...\n",
            "INFO 07-06 14:34:10 [gpu_model_runner.py:1600] Loading model from scratch...\n",
            "INFO 07-06 14:34:19 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
            "INFO 07-06 14:36:06 [gpu_model_runner.py:1624] Model loading took 22.4291 GiB and 116.203497 seconds\n",
            "INFO 07-06 14:36:26 [backends.py:462] Using cache directory: /home/lex/.cache/vllm/torch_compile_cache/120295a5e6/rank_0_0 for vLLM's torch.compile\n",
            "INFO 07-06 14:36:26 [backends.py:472] Dynamo bytecode transform time: 19.81 s\n",
            "INFO 07-06 14:36:32 [backends.py:161] Cache the graph of shape None for later use\n",
            "INFO 07-06 14:37:42 [backends.py:173] Compiling a graph for general shape takes 75.01 s\n",
            "INFO 07-06 14:39:12 [monitor.py:34] torch.compile takes 94.82 s in total\n",
            "INFO 07-06 14:39:13 [gpu_worker.py:227] Available KV cache memory: 18.31 GiB\n",
            "INFO 07-06 14:39:14 [kv_cache_utils.py:715] GPU KV cache size: 59,984 tokens\n",
            "INFO 07-06 14:39:14 [kv_cache_utils.py:719] Maximum concurrency for 256 tokens per request: 234.31x\n",
            "INFO 07-06 14:40:31 [gpu_model_runner.py:2048] Graph capturing finished in 77 secs, took 2.36 GiB\n",
            "INFO 07-06 14:40:31 [core.py:171] init engine (profile, create kv cache, warmup model) took 264.26 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overwriting default num_fewshot of hellaswag from None to 0\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:00<00:00, 3508.51it/s]\n",
            "Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1600/1600 [12:48<00:00,  2.08it/s]\n",
            "[rank0]:[W706 14:53:45.809929113 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-06 14:53:48 [config.py:823] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.\n",
            "INFO 07-06 14:53:48 [awq_marlin.py:116] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
            "INFO 07-06 14:53:48 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 07-06 14:53:48 [config.py:2232] max_num_batched_tokens (8192) exceeds max_num_seqs* max_model_len (256). This may lead to unexpected behavior.\n",
            "WARNING 07-06 14:53:52 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
            "INFO 07-06 14:53:56 [__init__.py:244] Automatically detected platform cuda.\n",
            "INFO 07-06 14:53:59 [core.py:455] Waiting for init message from front-end.\n",
            "INFO 07-06 14:53:59 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4', speculative_config=None, tokenizer='hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=256, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=1234, served_model_name=hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "WARNING 07-06 14:54:00 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f440885d0f0>\n",
            "INFO 07-06 14:54:00 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "WARNING 07-06 14:54:00 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 07-06 14:54:00 [gpu_model_runner.py:1595] Starting to load model hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4...\n",
            "INFO 07-06 14:54:00 [gpu_model_runner.py:1600] Loading model from scratch...\n",
            "INFO 07-06 14:54:00 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
            "INFO 07-06 14:54:01 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:19<02:34, 19.29s/it]\n",
            "Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:33<01:52, 16.13s/it]\n",
            "Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:53<01:48, 18.02s/it]\n",
            "Loading safetensors checkpoint shards:  44% Completed | 4/9 [01:13<01:34, 18.91s/it]\n",
            "Loading safetensors checkpoint shards:  56% Completed | 5/9 [01:34<01:17, 19.40s/it]\n",
            "Loading safetensors checkpoint shards:  67% Completed | 6/9 [01:54<00:59, 19.69s/it]\n",
            "Loading safetensors checkpoint shards:  78% Completed | 7/9 [02:14<00:39, 19.99s/it]\n",
            "Loading safetensors checkpoint shards:  89% Completed | 8/9 [02:23<00:16, 16.40s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 9/9 [02:43<00:00, 17.61s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 9/9 [02:43<00:00, 18.20s/it]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-06 14:56:45 [default_loader.py:272] Loading weights took 163.93 seconds\n",
            "INFO 07-06 14:56:50 [gpu_model_runner.py:1624] Model loading took 37.0909 GiB and 169.567562 seconds\n",
            "INFO 07-06 14:57:11 [backends.py:462] Using cache directory: /home/lex/.cache/vllm/torch_compile_cache/0a7455f96e/rank_0_0 for vLLM's torch.compile\n",
            "INFO 07-06 14:57:11 [backends.py:472] Dynamo bytecode transform time: 21.10 s\n",
            "INFO 07-06 14:57:15 [backends.py:161] Cache the graph of shape None for later use\n",
            "INFO 07-06 14:58:20 [backends.py:173] Compiling a graph for general shape takes 66.48 s\n",
            "INFO 07-06 15:00:30 [monitor.py:34] torch.compile takes 87.58 s in total\n",
            "INFO 07-06 15:00:32 [gpu_worker.py:227] Available KV cache memory: 0.99 GiB\n",
            "INFO 07-06 15:00:32 [kv_cache_utils.py:715] GPU KV cache size: 3,216 tokens\n",
            "INFO 07-06 15:00:32 [kv_cache_utils.py:719] Maximum concurrency for 256 tokens per request: 12.56x\n",
            "INFO 07-06 15:01:24 [gpu_model_runner.py:2048] Graph capturing finished in 52 secs, took 1.25 GiB\n",
            "INFO 07-06 15:01:25 [core.py:171] init engine (profile, create kv cache, warmup model) took 274.41 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overwriting default num_fewshot of hellaswag from None to 0\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:00<00:00, 3580.91it/s]\n",
            "Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1600/1600 [01:52<00:00, 14.24it/s]\n",
            "[rank0]:[W706 15:03:42.976037516 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-06 15:03:56 [config.py:823] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.\n",
            "INFO 07-06 15:03:56 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 07-06 15:03:58 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
            "INFO 07-06 15:04:00 [__init__.py:244] Automatically detected platform cuda.\n",
            "INFO 07-06 15:04:03 [core.py:455] Waiting for init message from front-end.\n",
            "INFO 07-06 15:04:03 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='Qwen/Qwen3-8B', speculative_config=None, tokenizer='Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=1234, served_model_name=Qwen/Qwen3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "WARNING 07-06 15:04:04 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ff9921f2e90>\n",
            "INFO 07-06 15:04:04 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "WARNING 07-06 15:04:04 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 07-06 15:04:04 [gpu_model_runner.py:1595] Starting to load model Qwen/Qwen3-8B...\n",
            "INFO 07-06 15:04:04 [gpu_model_runner.py:1600] Loading model from scratch...\n",
            "INFO 07-06 15:04:04 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
            "INFO 07-06 15:04:05 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:15<01:02, 15.57s/it]\n",
            "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:20<00:28,  9.45s/it]\n",
            "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:37<00:25, 12.69s/it]\n",
            "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:53<00:14, 14.16s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:06<00:00, 13.82s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:06<00:00, 13.38s/it]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-06 15:05:12 [default_loader.py:272] Loading weights took 67.00 seconds\n",
            "INFO 07-06 15:05:13 [gpu_model_runner.py:1624] Model loading took 15.2683 GiB and 67.840160 seconds\n",
            "INFO 07-06 15:05:22 [backends.py:462] Using cache directory: /home/lex/.cache/vllm/torch_compile_cache/8e186832ae/rank_0_0 for vLLM's torch.compile\n",
            "INFO 07-06 15:05:22 [backends.py:472] Dynamo bytecode transform time: 9.22 s\n",
            "INFO 07-06 15:05:30 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 7.903 s\n",
            "INFO 07-06 15:05:32 [monitor.py:34] torch.compile takes 9.22 s in total\n",
            "INFO 07-06 15:05:33 [gpu_worker.py:227] Available KV cache memory: 23.33 GiB\n",
            "INFO 07-06 15:05:33 [kv_cache_utils.py:715] GPU KV cache size: 169,856 tokens\n",
            "INFO 07-06 15:05:33 [kv_cache_utils.py:719] Maximum concurrency for 256 tokens per request: 663.50x\n",
            "INFO 07-06 15:05:58 [gpu_model_runner.py:2048] Graph capturing finished in 24 secs, took 0.60 GiB\n",
            "INFO 07-06 15:05:58 [core.py:171] init engine (profile, create kv cache, warmup model) took 45.31 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overwriting default num_fewshot of hellaswag from None to 0\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:00<00:00, 3590.18it/s]\n",
            "Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1600/1600 [00:16<00:00, 94.59it/s] \n",
            "[rank0]:[W706 15:06:37.115833554 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-06 15:06:38 [config.py:823] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.\n",
            "INFO 07-06 15:06:38 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 07-06 15:06:40 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
            "INFO 07-06 15:06:43 [__init__.py:244] Automatically detected platform cuda.\n",
            "INFO 07-06 15:06:46 [core.py:455] Waiting for init message from front-end.\n",
            "INFO 07-06 15:06:46 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='Qwen/Qwen3-8B-FP8', speculative_config=None, tokenizer='Qwen/Qwen3-8B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=1234, served_model_name=Qwen/Qwen3-8B-FP8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "WARNING 07-06 15:06:46 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f71171acca0>\n",
            "INFO 07-06 15:06:47 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "WARNING 07-06 15:06:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 07-06 15:06:47 [gpu_model_runner.py:1595] Starting to load model Qwen/Qwen3-8B-FP8...\n",
            "INFO 07-06 15:06:47 [gpu_model_runner.py:1600] Loading model from scratch...\n",
            "INFO 07-06 15:06:47 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
            "INFO 07-06 15:06:48 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:17<00:17, 17.59s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:38<00:00, 19.32s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:38<00:00, 19.06s/it]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-06 15:07:26 [default_loader.py:272] Loading weights took 38.22 seconds\n",
            "INFO 07-06 15:07:26 [gpu_model_runner.py:1624] Model loading took 8.8011 GiB and 39.210030 seconds\n",
            "INFO 07-06 15:07:36 [backends.py:462] Using cache directory: /home/lex/.cache/vllm/torch_compile_cache/54c66b6b61/rank_0_0 for vLLM's torch.compile\n",
            "INFO 07-06 15:07:36 [backends.py:472] Dynamo bytecode transform time: 9.26 s\n",
            "INFO 07-06 15:07:44 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 7.999 s\n",
            "WARNING 07-06 15:07:46 [fp8_utils.py:526] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/lex/.local/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=6144,K=4096,device_name=NVIDIA_L40S,dtype=fp8_w8a8,block_shape=[128,128].json\n",
            "WARNING 07-06 15:07:46 [fp8_utils.py:526] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/lex/.local/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=4096,K=4096,device_name=NVIDIA_L40S,dtype=fp8_w8a8,block_shape=[128,128].json\n",
            "WARNING 07-06 15:07:46 [fp8_utils.py:526] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/lex/.local/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=24576,K=4096,device_name=NVIDIA_L40S,dtype=fp8_w8a8,block_shape=[128,128].json\n",
            "WARNING 07-06 15:07:46 [fp8_utils.py:526] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/lex/.local/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=4096,K=12288,device_name=NVIDIA_L40S,dtype=fp8_w8a8,block_shape=[128,128].json\n",
            "INFO 07-06 15:07:46 [monitor.py:34] torch.compile takes 9.26 s in total\n",
            "INFO 07-06 15:07:47 [gpu_worker.py:227] Available KV cache memory: 29.79 GiB\n",
            "INFO 07-06 15:07:47 [kv_cache_utils.py:715] GPU KV cache size: 216,944 tokens\n",
            "INFO 07-06 15:07:47 [kv_cache_utils.py:719] Maximum concurrency for 256 tokens per request: 847.44x\n",
            "INFO 07-06 15:08:17 [gpu_model_runner.py:2048] Graph capturing finished in 30 secs, took 0.65 GiB\n",
            "INFO 07-06 15:08:17 [core.py:171] init engine (profile, create kv cache, warmup model) took 50.85 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overwriting default num_fewshot of hellaswag from None to 0\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:00<00:00, 3536.33it/s]\n",
            "Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1600/1600 [00:13<00:00, 119.53it/s]\n",
            "[rank0]:[W706 15:08:52.623436345 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-06 15:08:54 [config.py:823] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.\n",
            "INFO 07-06 15:08:54 [awq_marlin.py:116] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
            "INFO 07-06 15:08:54 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 07-06 15:08:56 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
            "INFO 07-06 15:08:59 [__init__.py:244] Automatically detected platform cuda.\n",
            "INFO 07-06 15:09:02 [core.py:455] Waiting for init message from front-end.\n",
            "INFO 07-06 15:09:02 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=256, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=1234, served_model_name=Qwen/Qwen3-8B-AWQ, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "WARNING 07-06 15:09:03 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f8fec58d120>\n",
            "INFO 07-06 15:09:03 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "WARNING 07-06 15:09:03 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 07-06 15:09:03 [gpu_model_runner.py:1595] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
            "INFO 07-06 15:09:03 [gpu_model_runner.py:1600] Loading model from scratch...\n",
            "INFO 07-06 15:09:03 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
            "INFO 07-06 15:09:04 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:04<00:04,  4.33s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:24<00:00, 13.53s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:24<00:00, 12.15s/it]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 07-06 15:09:29 [default_loader.py:272] Loading weights took 24.40 seconds\n",
            "INFO 07-06 15:09:30 [gpu_model_runner.py:1624] Model loading took 5.7073 GiB and 26.045704 seconds\n",
            "INFO 07-06 15:09:41 [backends.py:462] Using cache directory: /home/lex/.cache/vllm/torch_compile_cache/61e64b146d/rank_0_0 for vLLM's torch.compile\n",
            "INFO 07-06 15:09:41 [backends.py:472] Dynamo bytecode transform time: 11.21 s\n",
            "INFO 07-06 15:09:46 [backends.py:161] Cache the graph of shape None for later use\n",
            "INFO 07-06 15:10:22 [backends.py:173] Compiling a graph for general shape takes 40.25 s\n",
            "INFO 07-06 15:11:16 [monitor.py:34] torch.compile takes 51.46 s in total\n",
            "INFO 07-06 15:11:17 [gpu_worker.py:227] Available KV cache memory: 32.89 GiB\n",
            "INFO 07-06 15:11:17 [kv_cache_utils.py:715] GPU KV cache size: 239,456 tokens\n",
            "INFO 07-06 15:11:17 [kv_cache_utils.py:719] Maximum concurrency for 256 tokens per request: 935.38x\n",
            "INFO 07-06 15:11:48 [gpu_model_runner.py:2048] Graph capturing finished in 30 secs, took 0.62 GiB\n",
            "INFO 07-06 15:11:48 [core.py:171] init engine (profile, create kv cache, warmup model) took 137.94 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overwriting default num_fewshot of hellaswag from None to 0\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:00<00:00, 3612.30it/s]\n",
            "Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1600/1600 [00:12<00:00, 130.23it/s]\n",
            "[rank0]:[W706 15:12:21.949231873 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 27s, sys: 0 ns, total: 3min 27s\n",
            "Wall time: 1h 10min 7s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "results = []\n",
        "\n",
        "for model in models:\n",
        "    hellaswag_acc = evaluate_model(\n",
        "        model_name=model[\"hf_id\"],\n",
        "        vllm_args=model[\"vllm_args\"],\n",
        "        limit=400,\n",
        "    )\n",
        "    results.append({\n",
        "        \"name\": model[\"name\"],\n",
        "        \"acc\": hellaswag_acc\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQgbIZ97yz21",
        "outputId": "58994367-c7ea-4214-c7a4-1b2fedf17530"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Llama 70B / GGUF 4bit</td>\n",
              "      <td>0.7075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Llama 70B / GGUF 2bit</td>\n",
              "      <td>0.2675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Llama 70b / AWQ 4bit</td>\n",
              "      <td>0.7150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Qwen-3 8B / BF16</td>\n",
              "      <td>0.6375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Qwen-3 8B / FP8</td>\n",
              "      <td>0.6275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Qwen-3 8B / AWQ 4bit</td>\n",
              "      <td>0.6250</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    name     acc\n",
              "0  Llama 70B / GGUF 4bit  0.7075\n",
              "1  Llama 70B / GGUF 2bit  0.2675\n",
              "2   Llama 70b / AWQ 4bit  0.7150\n",
              "3       Qwen-3 8B / BF16  0.6375\n",
              "4        Qwen-3 8B / FP8  0.6275\n",
              "5   Qwen-3 8B / AWQ 4bit  0.6250"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r6rk0H_yz22"
      },
      "source": [
        "The results show that quantization introduces only a modest drop in accuracy while offering substantial memory savings.\n",
        "\n",
        "Both the Llama 3.1 70B GGUF 4-bit (70.8%) and AWQ 4-bit (71.5%) variants achieve the highest accuracy on HellaSwag, significantly outperforming all Qwen-3 8B variants. This performance gap is primarily due to the much larger model size of Llama 70B, showcasing how quantization makes it feasible to serve high-capacity models on limited hardware while retaining their quality advantages.\n",
        "\n",
        "For Qwen-3 8B, the accuracy shows a gradual decline from BF16 (63.8%) to FP8 (62.8%) and AWQ 4-bit (62.5%), reflecting the expected trade-off between efficiency and quality. Notably, extreme quantization like GGUF 2-bit on Llama 70B results in a significant performance drop (26.8%), indicating that aggressive compression can severely degrade model quality.\n",
        "\n",
        "**Overall, these findings confirm that low-bit quantization, particularly 4-bit and FP8 formats, is an effective strategy for deploying language models in resource-constrained environments. Depending on the deployment goal, quantization can be used either to speed up inference and improve throughput for a given model, or to enable serving much larger models (like Llama 70B) that deliver higher quality outputs, giving the flexibility to balance efficiency and performance.**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
